[
    {
        "citation": "Mejia and Kajikawa, 1991, p. 1511",
        "content": "Vol.:(0123456789)Scientometrics\nhttps://doi.org/10.1007/s11192-023-04806-2\n1 3\nEmbedding models for supervised automatic extraction \nand classification of named entities in scientific \nacknowledgements\nNina Smirnova1  · Philipp Mayr1 \nReceived: 2 February 2023 / Accepted: 26 July 2023 \n© The Author(s) 2023\nAbstract\nAcknowledgments in scientific papers may give an insight into aspects of the scientific \ncommunity, such as reward systems, collaboration patterns, and hidden research trends. \nThe aim of the paper is to evaluate the performance of different embedding models for the \ntask of automatic extraction and classification of acknowledged entities from the acknowl-\nedgment text in scientific papers. We trained and implemented a named entity recogni-\ntion (NER) task using the flair NLP framework. The training was conducted using three \ndefault Flair NER models with four differently-sized corpora and different versions of the \nflair NLP framework. The Flair Embeddings model trained on the medium corpus with the \nlatest FLAIR version showed the best accuracy of 0.79. Expanding the size of a training \ncorpus from very small to medium size massively increased the accuracy of all training \nalgorithms, but further expansion of the training corpus did not bring further improvement. \nMoreover, the performance of the model slightly deteriorated. Our model is able to recog-\nnize six entity types: funding agency, grant number, individuals, university, corporation, \nand miscellaneous. The model works more precisely for some entity types than for others; \nthus, individuals and grant numbers showed a very good F1-Score over 0.9. Most of the \nprevious works on acknowledgment analysis were limited by the manual evaluation of data \nand therefore by the amount of processed data. This model can be applied for the compre-\nhensive analysis of acknowledgment texts and may potentially make a great contribution to \nthe field of automated acknowledgment analysis.\nKeywords Natural language processing · Named entity recognition · Web of science · \nAcknowledgement · Text mining · Flair NLP-framework\n * Nina Smirnova \n nina.smirnova@gesis.org\n Philipp Mayr \n philipp.mayr@gesis.org\n1 GESIS – Leibniz Institute for the Social Sciences, Unter Sachsenhausen 6-8, 50667 Cologne, \nGermany Scientometrics\n1 3\nIntroduction\nAcknowledgments in scientific papers are short texts where the author(s) “identify those \nwho made special intellectual or technical contribution to a study that are not sufficient \nto qualify them for authorship” . Cronin and Weaver \n(1995) ascribe an acknowledgment alongside authorship and citedness to measures of a \nresearcher’s scholarly performance: a feature that reflects the researcher’s productivity and \nimpact. Giles and Councill (2004) argue that acknowledgments to individuals, in the same \nway as citations, may be used as a metric to measure an individual’s intellectual contribu-\ntion to scientific work. Acknowledgments of financial support are interesting in terms of \nevaluating the influence of funding agencies on academic research. Acknowledgments of \ntechnical and instrumental support may reveal “indirect contributions of research labora-\ntories and universities to research activities” (Giles & Councill, 2004, p. 17599).\nThe analysis of acknowledgments is particularly interesting as acknowledgments may \ngive an insight into aspects of the scientific community, such as reward systems (Dzieżyc \n& Kazienko, 2022), collaboration patterns, and hidden research trends (Giles & Councill, \n2004; Diaz-Faes & Bordons, 2017). From the linguistic point of view, acknowledgments \nare unstructured text data, which through automatic analysis poses research and methodo-\nlogical problems like data cleaning, choosing the proper tokenization method, and whether \nand how word embeddings may enhance their automatic analysis.\nTo our knowledge, previous works on automatic acknowledgment analysis were mostly \nconcerned with the extraction of funding organizations and grant numbers (Alexandera & \nVries, 2021; Kayal et  al., 2017; Borst et  al., 2022) or classification of acknowledgment \ntexts (Song et al., 2020; Hubbard et al., 2022). Furthermore, large bibliographic databases \nsuch as Web of Science (WoS)1 and Scopus selectively index only funding information, \ni.e., names of funding organizations and grant identification numbers. Consequently, we \nwant to extend that to other types of acknowledged entities: individuals, universities, cor -\nporations, and other miscellaneous information. Analysis of the acknowledged individuals \nprovides insight into informal scientific collaboration (Rose & Georg, 2021; Kusumegi & \nSano, 2022). Acknowledged universities and corporations reveal interactions and knowl-\nedge exchange between industry and universities (Chen et  al., 2022). Entities from the \nmiscellaneous category include other information like project names, which could uncover \ninternational scientific collaborations.\nThe state-of-the-art named entity recognition (NER) models showed a great perfor -\nmance on the CoNLL-2003 dataset (Akbik et al., 2018; Devlin et al., 2018; Yamada et al., \n2020; Yu et  al., 2020). CoNLL-2003 corpus (Sang et  al., 2003) is a benchmark dataset \nfor language-independent named entity recognition, i.e., designed to train and evaluate \nNER models. English data for the corpus were taken from the Reuters corpus. The dataset \ncomprises four types of named entities: person, location, organisation, and miscellaneous. \nHowever, specific domains require specifically labelled training data. The development of a \ntraining dataset for the specific domain is an expensive and time-consuming process since \nNER usually requires a quite large training corpus. Therefore, the objective of this paper is \nto evaluate the performance of existing embedding models for the task of automatic extrac-\ntion and classification of acknowledged entities from the acknowledgment text in scientific \npapers using small training datasets or without training data (zero-short approach).\n1 http:// wokin fo. com/ produ cts_ tools/ multi disci plina ry/ webof  scien ce/ fundi ngsea rch/.Scientometrics \n1 3\nThe present paper is an extended version of the article (Smirnova & Mayr, 2022)2 pre-\nsented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Sci-\nentific Documents (EEKE2022).3 Flair, an open-source natural language processing (NLP) \nframework (Akbik et al., 2019) is used in our study to create a tool for the extraction of \nacknowledged entities because this library is easily customizable. It offers the possibility \nof creating a customized Named Entity Recognition (NER) tagger, which can be used for \nprocessing and analyzing acknowledgment texts. Furthermore, Flair has shown better accu-\nracy for NER tasks using pre-trained datasets in comparison with many other open source \nNLP tools.4\nIn the first experiment (Sect.  4.1) we trained and implemented a NER task using three \ndefault Flair NER models with two differently-sized corpora.5 All the descriptions of the \nFlair framework features refer to the releases 0.9 and 0.11. The models were trained to rec-\nognize six types of acknowledged entities: funding agency, grant number, individuals, uni-\nversity, corporation, and miscellaneous. The model with the best accuracy can be applied \nfor the comprehensive analysis of the acknowledgment texts. In Experiments 2 and 3 we \nperformed additional training with altered training parameters or altered training corpora \n(Sects  4.2 and 4.3). Most of the previous works on acknowledgment analysis were limited \nby the manual evaluation of data and therefore by the amount of processed data (Giles \n& Councill, 2004; Paul-Hus et al., 2017; Paul-Hus & Desrochers, 2019; Mccain, 2017). \nFurthermore, Thomer and Weber (2014) argues that using named entities can benefit the \nprocess of manual document classification and evaluation of the data. Therefore, a model \nthat is capable of extracting and classification of different types of entities may potentially \nmake a significant contribution to the field of automated acknowledgment analysis.\nResearch questions\nIn this paper, we address the following research questions:\n• RQ1: Is the few-shot or zero-shot approach able to identify predefined acknowledged \nentity classes?\n• RQ2: Which of the Flair default NER models is more suitable for the defined task of \nextraction and classification of acknowledged entities from scientific acknowledgments \nusing a small training dataset?\n• RQ3: How does the size of the training corpus affect the training accuracy for different \nNER models?\nCreating a training dataset for supervised learning is a time-consuming and expensive task, \nsince as a rule, such a model requires a reasonably large amount of training data. Annota-\ntion is a crucial moment, as wrongly annotated data will deteriorate training results. There-\nfore, more than one annotator is usually required to provide credible results. That is why \n2 In this paper we conducted an additional experiment (Experiment 3) with 2 new corpora (corpus Nos. 3 \nand 4).\n3 https:// eeke- works hop. github. io/ 2022/.\n4 https:// github. com/ flair NLP/ flair .\n5 The release 0.9 (https:// github. com/ flair NLP/ flair/ relea ses/ tag/ v0.9) was used in the experiments 1 and 2. \nExperiment 3 was performed using release 0.11 (https:// github. com/ flair NLP/ flair/ relea ses/ tag/ v0. 11). Scientometrics\n1 3\nit is of interest to test if the existing NER models can provide reasonable accuracy while \nusing small or no training data.\nBackground and related work\nResearch in the field of acknowledgments analysis has been carried out since the 1970s. \nThe first typology of acknowledgments was proposed by Mackintosh (1972) (as cited \nin Cronin, 1995) and comprised three categories: facilities, access to data, and help of indi-\nviduals. McCain (1991) distinguished five types of acknowledgements: research-related \ninformation, secondary access to research-related information, specific research-related \ncommunication, general peer communication, and technical or clerical support. Cronin and \nWeaver (1995) defined three broad categories: resource-, procedure- and concept-related. \nMejia and Kajikawa (2018) developed a four-level classification based on sponsored \nresearch field: change maker, incremental, breakthrough, and matured.\nDoehne and Herfeld (2023) distinguished acknowledgements from the perspective of \nappreciation of influential scholars and defined two axes: scientific influence and insti-\ntutional influence. Scientific influence refers to the productiveness and creativity of the \nresearcher, while institutional influence is associated with the scholar’s administrative posi-\ntion in the scientific community.\nWang and Shapira (2011) investigated the connection between research funding and \nthe development of science and technology using acknowledgments from articles from the \nfield of nanotechnology. Rose and Georg (2021) studied informal cooperation in academic \nresearch. The analysis revealed generational and gender differences in informal collabora-\ntion. The authors claim that information from informal collaboration networks makes bet-\nter predictions of the academic impact of researchers and articles than information from \nco-author networks. Mejia and Kajikawa (2018) argued that the classification of funders \ncould be useful in developing funding strategies for policymakers and funders.\nDoehne and Herfeld (2023) manually investigated acknowledgement sections of papers, \nwhich were published or preprinted in association with the Cowles Foundation between \nearly 1940 and 1970 to trace the influence of the informal social structure and academic \nleaders on the early acceptance of scientific innovations. Blockmodelling was applied to \nthe acknowledgement data. Their analysis showed that the adoption of scientific innova-\ntions was partly influenced by the social structure and by the scientific leaders at Cowles.\nRecent advances in NER\nNamed Entity Recognition (NER) is a form of NLP that aims to extract named entities \nfrom unstructured text and classify them into predefined categories. A named entity is a \nreal-world object that is important for understanding the text. Current approaches in NER \ncan be distinguished into supervised and unsupervised tasks. In a supervised NER a model \nis trained using a labelled dataset. This training dataset or corpus is usually split into sev -\neral datasets: training set, test set, and validation set. NER models require corpora with \nsemantic annotation, i.e., metadata about concepts attached to unstructured text data. The \nannotation process is crucial as insufficient or redundant metadata can slow down and bias \na learning process (Pustejovsky & Stubbs, 2012, Chapter 1).\nSupervised NER mainly relays on machine learning or deep learning methods. The \nstate-of-the-art models are based on deep recurrent models, convolution-based, or Scientometrics \n1 3\npre-trained transformer architectures (Iovine et  al., 2022). Thus, Akbik et  al. (2018) \nproposed a new character-based contextual string embeddings method. This approach \npasses a sequence of characters through the character-level language model to gener -\nate word-level embeddings. The model was pre-trained on large unlabeled corpora. The \ntraining was carried out using a character-based neural language model together with a \nBidirectional LSTM (BiLSTM) sequence-labelling model. This approach generates dif-\nferent embeddings for the same word depending on its context and showed good results \non downstream tasks such as NER. Devlin et al. (2018) presented BERT (Bidirectional \nEncoder Representations Transformers), a transformer-based language representa-\ntion model that models the representation of contextualized word embeddings. BERT \nshowed superior results on downstream tasks using different benchmarking datasets. \nLater, Liu et al. (2019) performed an optimization of the BERT model and introduced \nRoBERTa (Robustly Optimized BERT Pretraining Approach). RoBERTa was evaluated \non three benchmarks and demonstrated massive improvements over the reported BERT \nperformance.\nCurrently, several domain-specific models have been developed. Thus, Beltagy et  al.. \n(2019) released SciBERT a BERT-based language model pre-trained on a large number \nof unlabeled scientific articles from the computer science and biomedical domains. SciB-\nERT showed improvements over BERT on several downstream NLP tasks, including NER. \nRecently, Shen et al. (2022) introduced the SsciBERT, a language model based on BERT \nand pre-trained on abstracts published in the Social Science Citation Index (SSCI) journals. \nThe model showed good results in discipline classification and abstract structure-function \nrecognition in articles from the social sciences domain.\nUnsupervised methods are often based on lexicons or predefined rules. Thus, Etzioni \net al. (2005) uses lists of patterns and domain-specific rules to extract named entities. Efti-\nmov et al. (2017) developed a rule-based NER model to extract dietary information from \nscientific publications. Evaluation of the model performance showed good results. Opposed \nto previous unsupervised NER approaches, Iovine et al. (2022) proposed a cycle-consist-\nency approach for NER (CycleNER). CycleNER is unsupervised and does not require par -\nallel training data. The method showed 73% of supervised performance on CoNLL03.\nNER in scientometrics analysis\nNamed entities are widely used in scientometrics analysis. Thus, Kenekayoro (2018) devel-\noped a supervised method for the automatic extraction of named entities from academic \nbibliographies. The aim of the study was to create a database containing unified academic \ninformation about individuals to help in expert finding. A labeled training dataset was \ndeveloped using biographies extracted from ORCID.6 The authors tested several models \nfor NER. The Support Vector Machine classification algorithm (SVM) showed the best \nperformance.\nJiang et al. (2022) proposed a strategy for the identification of software in scientific bio-\ninformatics publications using the combination of SVM and CRF (Conditional Random \nField). Application of the method to the sample of articles from bioinformatics domains \nallowed them to observe interesting patterns in using software in scientific research.\nKusumegi and Sano (2022) analysed scholarly relationships by analysing acknowl-\nedged individuals from the acknowledgments statements from eight open-access journals. \n6 https:// orcid. org/. Scientometrics\n1 3\nIndividuals were extracted using the Stanford CoreNLP NER tagger. In the next steps, \nscholars were identified among the extracted individuals by mapping them to the Microsoft \nAcademic Graph (MAG).\nWe are aware of several works on automated information extraction from acknowledg-\nments. Giles and Councill (2004) developed an automated method for the extraction and \nanalysis of acknowledgment texts using regular expressions and SVM. Computer science \nresearch papers from the CiteSeer digital library were used as a data source. Extracted enti-\nties were analysed and manually assigned to the following four categories: funding agen-\ncies, corporations, universities, and individuals.\nThomer and Weber (2014) used the 4-class Stanford Entity Recognizer (Finkel et al., \n2005) to extract persons, locations, organizations, and miscellaneous entities from the col-\nlection of bioinformatics texts from PubMed Central’s Open Access corpus. The aim of \nthe study was to determine an approach to \"increase the speed of ... classification without \nsacrificing accuracy, nor reliability\" (Thomer & Weber, 2014, p. 1134).\nKayal et  al. (2017) introduced a method for extraction of funding organizations and \ngrants from acknowledgment texts using a combination of sequential learning models: con-\nditional random fields (CRF), hidden markov models (HMM), and maximum entropy mod-\nels (MaxEnt). The final model contained pooled outputs from the models used.\nAlexandera and Vries (2021) proposed AckNER, a tool for extracting financial informa-\ntion from the funding or acknowledgment section of a research article. AckNER works \nwith the use of dependency parse trees and regular expressions and is able to extract names \nof the organisations, projects, programs, and funds, as also numbers of contracts and \ngrants7.\nFollowing, Borst et  al. (2022) applied a question-answering (QA) based approach to \nidentify funding information in acknowledgments texts. This approach performs similarly \nto AckNER and requires a smaller set of training and test data.\nTable  1 shows an overview of works on NER in scientometrics. Overall, previous works \non the extraction of named entities from acknowledgements texts were mostly concerned \nwith the extraction of funding information, i.e., only names of funding bodies and grant \nnumbers, or extraction and linking of individuals. The special issue by Zhang et al. (2023) \nprovided a recent overview of current works in the extraction of knowledge entities.\nTo the best of our knowledge the work of Giles and Councill (2004) is the only \nattempt to extract and categorise multiple acknowledged entities. Nevertheless, entities \nwere extracted using the SVM algorithm but the classification of entities themselves \nwas produced manually, which limited the number of acknowledgement texts to be ana-\nlysed. Furthermore, as far as we know, there was no research done concerning the eval-\nuation of embedding models for extraction of information from acknowledgement texts \nand no tool for automatic extraction of different kinds of acknowledged entities was \ndeveloped.\n7 AckNER showed better performance as Flair, but is specifically designed to recognize two types of \nacknowledged entities (Alexandera & Vries, 2021), which was insufficient for the present project.Scientometrics \n1 3\nTable 1  Overview of works on NER in scientometrics\nPaper Area of application and aim of \nthe studyCorpus Entities Methods and tools\nGiles and Councill (2004) Extraction of acknowledged enti-\nties form acknowledgementsCiteSeer Funding agencies, Companies, \nEducational Institutions, \nIndividualsSVM for extracting entities and \ntheir manual classification\nThomer and Weber (2014) Using NER to improve classifica-\ntion of acknowledgementsPubMed Central’s Open Access Persons, locations, organizations, \nand miscellaneous4-class Stanford Entity Recognizer\nKayal et al. (2017) Extraction of funding information \nfrom acknowledgementsPubMed Central’s Open Access Funding bodies, grants CRF, HMM, MaxEnt\nKenekayoro (2018) Extraction of biography informa-\ntion from academic biographiesORCID Award, Location, Organization, \nPerson, Position, Specializa-\ntion, OthersSVM\nAlexandera and Vries (2021) Extraction of funding information \nfrom acknowledgementsTU Delft’s institutional repository Funding bodies, grants SpaCy dependency parser + regu-\nlar expressions\nJiang et al. (2022) Extraction of scientific software \nfrom scientific articles (full \ntexts) in bioinformaticsbioinformatics journals EnsembleSVMs-CRF\nBorst et al. (2022) Extraction of funding information \nfrom acknowledgementsEconStor Funding bodies, grants Haystack\nKusumegi and Sano (2022) Extraction and linking of \nacknowledged individuals from \nacknowledgementsPLOS Individuals Stanford CoreNLP NER tagger + \nMAG Scientometrics\n1 3\nMethod\nIn the present paper, different models for extraction and classification of acknowledged \nentities supported by the Flair NLP framework were evaluated. The choice of classification \nwas inspired by Giles and Councill’s (2004) classification: funding agencies (FUND), cor -\nporations (COR), universities (UNI), and individuals (IND). For our project, this classifica-\ntion was enhanced with the miscellaneous (MISC) and grant numbers (GRNB) categories. \nThe GRNB category was adopted from WoS funding information indexing. The entities \nin the miscellaneous category could provide useful information, but cannot be ascribed to \nother categories, e.g., names of projects and names of conferences. Figure  1 demonstrates \nan example of acknowledged entities of different types. To the best of our knowledge, Giles \nand Councill’s classification is the only existing classification of acknowledged entities and \ntherefore can be applied to the NER task. Other works on acknowledgment analysis were \nfocused on the classification of acknowledgment texts.\nThe Flair NLP framework\nFlair is an open-sourced NLP framework built on PyTorch (Paszke et  al., 2019), which \nis an open-source machine learning library. “The core idea of the framework is to pre-\nsent a simple, unified interface for conceptually very different types of word and document \nembeddings” (Akbik et  al., 2019, p.  54). Flair has three default training algorithms for \nNER which were used for the first experiment in the present research: a) NER Model with \nFlair Embeddings (later on Flair Embeddings) (Akbik et al., 2018), b) NER Model with \nTransformers (later on Transformers) (Schweter & Akbik, 2020), and c) Zero-shot NER \nwith TARS (later on TARS) (Halder et al., 2020) 8.\nThe Flair Embeddings model uses stacked embeddings, i.e., a combination of contex-\ntual string embeddings (Akbik et al., 2018) with a static embeddings model. This approach \nwill generate different embeddings for the same word depending on its context. Stacked \nembedding is an important Flair feature, as a combination of different embeddings might \nbring better results than their separate uses (Akbik et al., 2019).\nThe Transformers model or FLERT-extension (document-level features for NER) is a \nset of settings to perform a NER on the document level using fine-tuning and feature-based \nFig. 1  An example of acknowledged entities. Each entity type is marked with a distinct color\n8 New transformer models as SciBERT or SsciBERT were not evaluated in this study, as the objective of \nthe study is to evaluate the performance of the Flair default models.Scientometrics \n1 3\nLSTM-CRF with the multilingual XML-RoBERTa transformer model (Schweter & Akbik, \n2020).\nThe TARS (task-aware representation of sentences) is a transformer-based model, \nwhich allows performing training without any training data (zero-shot learning) or with a \nsmall dataset (few-short learning) (Halder et al., 2020). The TARS approach differs from \nthe traditional transfer learning approach in the way that the TARS model also considers \nsemantic information captured in the class labels themselves. For example, for analyzing \nacknowledgments, class labels like funding organization or university already carry seman-\ntic information.\nTraining data\nThe Web of Science (WoS) database was used to harvest the training data (funding \nacknowledgments).9 From 2008 on, WoS started indexing information about funders and \ngrants. WoS uses information from different funding reporting systems such as Research-\nfish,10 Medline11 and others. As WoS contains millions of metadata records (Singh et al., \n2021), the data chosen for the present study was restricted by year and scientific domain \n(for the corpora Nos. 1, 2, and 3) or additionally by the affiliation country (for corpus No.4). \nTo construct corpora Nos. 1-3 records from four different scientific domains published \nfrom 2014 to 2019 were considered: two domains from the social sciences (sociology and \neconomics) and oceanography and computer science. Different scientific domains were Table 2  Number of sentences/texts in the training corpora\nCorpus No. Training set (train) Test set (test) Validation set (dev) Total\n1 29/27 10/10 10/10 49/47\n2 339/282 165/150 150/136 654/441\n3 784/657 165/150 150/136 1099/816\n4 1148/885 165/150 150/136 1463/1044\nTable 3  Number of sentences/texts from each scientific domain in the training corpora\nCorpus No. Oceanography Economics Social Sciences Computer Science\n1 13/13 3/3 20/20 16/14\n2 127/75 92/58 351/234 173/129\n3 175/112 128/89 590/434 333/269\n9 The present research was conducted in scopes of two projects: MinAck (https:// kalaw inka. github. io/ \nminack/) and SEASON (https:// github. com/ kalaw inka/ season). Corpora Nos.1, 2, and 3 were created for the \nMinAck project and serve the purpose of a general evaluation of the impact of the size of the training cor -\npus on the model performance. Corpus No.4 was designed specifically for the SEASON project in the hope \nof improving the recognition of Indian funding information. The project SEASON aims to get insight into \nGerman-Indian scientific collaboration. Our other corpora mainly contain papers published by European \ninstitutions. That is why we enhance Corpus 4 with the papers published by Indian institutions.\n10 https:// resea rchfi sh. com/.\n11 https:// www. nlm. nih. gov/ bsd/ fundi ng_ suppo rt. html. Scientometrics\n1 3\nchosen since previous work on acknowledgment analysis revealed the relations between \nthe scientific domain and the types of acknowledged entities, i.e., acknowledged individu-\nals are more characteristic of theoretical and social-oriented domains. At the same time, \ninformation on technical and instrumental support is more common for the natural and \nlife sciences domains (Diaz-Faes & Bordons, 2017). Only the WoS record types “article” \nand “review” published in a scientific journal in English were selected; then 1000 distinct \nacknowledgments texts were randomly gathered from this sample for the training dataset. \nFurther different amounts of sentences containing acknowledged entities were distributed \ninto the differently-sized training corpora. Table  2 demonstrates the number of sentences \nin each set in the four corpora. We selected only sentences that contain an acknowledged \nentity, regardless of the scientific domain. Table  3 contains the number of sentences and \ntexts from each scientific domain in the training corpora.12 The same article can belong to \nseveral scientific domains, therefore, the number of sentences and texts in Tables  2 and 3 \ndoes not match. Corpus No.4 was designed in such a way that all the training data from the \nCorpus No.3 was enhanced with acknowledgments texts from the articles that have Indian \naffiliations regardless of scientific domain or publication date.\nPreliminary analysis of the WoS data showed that the indexing of WoS funding infor -\nmation has several issues. The WoS includes only acknowledgments containing funding \ninformation; therefore, not every WoS entry has an acknowledgment, individuals are not \nincluded, and indexed funding organizations are not divided into different entity types like \nuniversities, corporations, etc. Therefore, the existing indexing of funding organizations \nis incomplete. Furthermore, there is a disproportion between the occurrences of acknowl-\nedged entities of different types. Thus, the most frequent entity types in the dataset with \nthe training data are IND, FUND and GRNB, followed by UNI and MISC. COR is the \nFig. 2  The distribution of acknowledged entities in the training corpora\n12 Corpus No.4 is not in Table  2, because the corpus contains additional acknowledgment texts from arti-\ncles with Indian affiliations regardless of the scientific domain and therefore contains different scientific \ndomains.Scientometrics \n1 3\ncategory most underrepresented in the data set. Consequently, there are different amounts \nof entities of different types in the training corpora (as Fig.  2 demonstrates), which might \nhave influenced the training results. Training with the corpora Nos. 2, 3, and 4 was evalu-\nated on the same training and validation datasets to ensure plausible accuracy (Fig.  3-B). \nFig. 3  The distribution of acknowledged entities in the test and validation corpora\nFig. 4  Annotation flowchart Scientometrics\n1 3\nHowever, training with corpus No.1 was evaluated with the smaller test and validation sets, \nas corpus No.1 contains a smaller number of sentences (Fig. 3-A).\nData annotation\nThe training corpus was annotated with six types of entities. As WoS already contains \nsome indexed funding information, it was decided to develop a semi-automated approach \nfor data annotation (as Fig.  4 demonstrates) and use indexed information provided by WoS, \ntherefore, grant numbers were adopted from the WoS indexing unaltered.\nFlair has a pre-trained 4-class NER Flair model (CoNLL-03).13 The model can pre-\ndict four tags: PER (person name), LOC (location), ORG (organization name), and MISC \n(other names). As Flair showed adequate results in the extraction of names of individu-\nals, it was decided to apply the pre-trained 4-class CoNLL-03 Flair model to the training \ndataset. Entities that fell into the PER category were added as the IND annotation to the \ntraining corpus. Furthermore, we noticed that some funding information was partially cor -\nrectly extracted into the ORG and MISC categories. Therefore, WoS funding organization \nindexing and entities from the ORG and MISC categories were adopted and distinguished \nbetween three categories (FUND, COR, and UNI) using regular expressions. In addition, \nthe automatic classification of entities was manually examined and reviewed. Mismatched \ncategories, partially extracted entities, and not extracted entities were corrected. Acknowl-\nedged entities, which fall into the MISC category, were manually annotated by one annota-\ntor. In the miscellaneous category entities referring to names of the conferences and pro-\njects were included.\nExperiments\nIn the present paper, we evaluated three default Flair NER models with four differently-\nsized corpora. In total, we performed three experiments. In the first experiment, mod-\nels with the default parameter were evaluated using corpora Nos. 1 and 2. In the second \nexperiment, we evaluated Flair Embeddings and Transformers model with altered training \nparameters and corpus No.2. In the third experiment, the first experiment was replicated \nwith corpora Nos. 3 and 4.\nExperiment 1\nIn the first experiment, we tested the TARS model zero-shot and few-shot scenarios (with \ncorpus No. 1), as well as the performance of two default FLAIR models (Flair Embeddings \nand Transformers) with corpus No.2. Additionally, the performance of Flair Embeddings \nand Transformers models was tested with the corpus No.1 The training was conducted with \nthe recommended parameters for all algorithms, as Flair developers specifically ran vari-\nous tests to find the best hyperparameters for the default models. For the few-shot TARS, \nthe training was conducted with the small dataset (corpus No.1), and for Transformers and \nFlair Embeddings with a larger dataset (corpus No.2).\n13 https:// github. com/ flair NLP/ flairScientometrics \n1 3\nThe Flair Embeddings model was initiated as a combination of static and contextual \nstring embeddings. We applied GloVe (Pennington et  al., 2014) as a static word-level \nembedding model. Thus, in our case, stacked embeddings comprise GloVe embeddings, \nforward contextual string embeddings, and backward contextual string embeddings. The \nmodel was trained with the recommended parameters: the size of mini-batches was set to \n32 and the maximum number of epochs was set to 150.\nFor Transformers, training was initiated with the RoBERTa model (Liu et al., 2019). For \nthe present paper, a fine-tuning approach was used. The fine-tuning procedure consisted of \nadding a linear layer to a transformer and retraining the entire network with a small learn-\ning rate. We used a standard approach, where only a linear classifier layer was added on the \ntop of the transformer, as adding the additional CRF decoder between the transformer and \nlinear classifier did not increase accuracy compared with this standard approach (Schweter \n& Akbik, 2020). The chosen transformer model uses subword tokenization. We used the \nmean of embeddings of all subtokens and concatenation of all transformer layers to pro-\nduce embeddings. The context around the sentence was considered. The training was initi-\nated with a small learning rate using the Adam Optimisation Algorithm (Kingma & Ba, \n2014).\nThe TARS model requires labels to be defined in a natural language. Therefore, we \ntransformed our original coded labels into the natural language: FUND - “Funding \nAgency”, IND - “Person”, COR - “Corporation”, GRNB - “Grant Number\", UNI - “Uni-\nversity”, and MISC - “Miscellaneous”. The training for the few-shot approach was initiated \nwith the TARS NER model (Halder et al., 2020).\nResults\nOverall, the training demonstrated mixed results. Table  4 shows training results with cor -\npus No.1 and the TARS zero-shot approach. GRNB showed adequate results by training \nwith Flair Embeddings and TARSfew-shot models. IND was the best-recognized entity by \ntraining with Flair Embeddings and TARS (both zero- and few-shot) with an F1-score of \n0.8 (Flair Embeddings) and 0.86 (TARS) respectively. Training with Transformers was not \nsuccessful for IND with an F1-score of 0. In general, transformers were a less efficient \nalgorithm for training with a small dataset with an overall accuracy of 0.35. FUND dem-\nonstrated not satisfactory results with F1-score of less than 0.5 for all models. Entity types \nFig. 5  The training results with the training corpus No.2. A Comprises diagrams with the F1-scores of the \ntraining with three algorithms for each label class. B depicts the total accuracy of training algorithms Scientometrics\n1 3\nMISC, UNI, and COR showed the worst results with the F1-score equal to zero for all algo-\nrithms. The low accuracy for MISC, UNI, and COR resulted in low overall accuracy for \nall algorithms. Overall, training with corpus No.1 showed insufficient results for all algo-\nrithms. Flair Embeddings and TARS showed better accuracy compared to Transformers.\nFigure  5 shows the training results with corpus No.2. Similar to the training with corpus \nNo.1, IND and GRNB are the best-recognized categories. The best results for IND and \nGRNB demonstrated Flair embeddings with an F1-score of 0.98 (IND) and 0.96 (GRNB). \nTARS achieved the best results for FUND with an F1-score of 0.77 against 0.71 for Flair \nEmbeddings and 0.68 for Transformers. Miscellaneous demonstrated the worst accuracy \nfor Flair Embeddings (0.64) and Transformers (0.49), while for TARS the worst accuracy \nlies in the COR category with an F1-score of 0.54. The best result for UNI showed Flair \nEmbeddings with an F1-score over 0.7. The COR category showed a decent precision \nof 0.88 with Flair Embeddings but a low recall of 0.58 which resulted in a low F1-Score \n(0.7)14.\nTraining with corpus No.2 showed a significant improvement in training accuracy \n(Fig.  5B). Overall, Flair Embeddings was more accurate than other training algorithms, \nalthough training with TARS showed better results for the FUND category. The Trans-\nformers showed the worst results during training.\nAdditionally, a zero-shot approach was tested for the TARS model on corpus no.1. The \nmodel was able to successfully recognize individuals, but struggled with other categories, \nas Table 4 demonstrates. The total accuracy of the model comprises 0.23.\nExperiment 2\nOur first hypothesis to explain the pure model performance for the FUND, COR, MISC, \nand UNI categories is their semantic proximity that prevents successful recognition. \nEntities of these categories are often used in the same context. To examine this hypoth-\nesis, we conducted an experiment using Flair Embeddings with the dataset contain-\ning three types of entities: IND, GRNB, and ORG. The MISC category was excluded \nfrom the training, as one of the aims of the present research is to extract information \nabout acknowledged entities, and the MISC category contains only additional informa-\ntion. The new ORG category was established, which includes a combination of entities \nfrom the FUND, COR, and UNI categories. The training was performed with exactly \nthe same parameters as training with the Flair Embeddings model in Experiment 1 \n(Sect. 4.1).Table 4  F1-scores of the training with three algorithms for each label class with Corpus No. 1\nAlgorithm FUND GRNB IND UNI COR MISC accuracy\nTARS (zero-shot) 0.23 0.33 0.86 0 0 0 0.23\nTARS (few-shot) 0.32 0.76 0.86 0 0 0 0.35\nFlair embeddings 0.42 0.61 0.80 0 0 0 0.35\nTransformers 0.30 0.40 0 0 0 0 0.15\n14 Accuracy metrics by type of entity and total accuracy for all experiments can be found in Appendixes   A \nand BScientometrics \n1 3\nThe UNI and COR categories, though, have distinct patterns. In this case, the low \nperformance of the models for the COR and UNI categories could be explained by the \nsmall size of the training sample that contains these categories (see Fig.  2). Thus, the \nmodel was not able to identify patterns because of the lack of data.\nSecondly, low results for FUND, COR, MISC, and UNI categories might also lie in \nthe nature of the miscellaneous category, as some entities that fall into this category are \nsemantically very close to the FUND and COR categories. As a result, training without \na MISC category might potentially show better performance. To examine this hypoth-\nesis, we conducted training with Flair Embeddings with a dataset excluding the MISC \ncategory, i.e., with five entity types. Training results are shown in Fig. 6 A.\nAdditionally, the problem might lie in the nature of the training algorithms that \nwere used. On the one hand, Flair developers claimed Transformers to be the most effi-\ncient algorithm (Schweter & Akbik, 2020). On the other, the stacked embeddings are \nan important feature of the Flair tool, as a combination of different embeddings might \nbring better results than their separate uses (Akbik et  al., 2019). Thus, the combina-\ntion of the Transformer embeddings model with the contextual string embeddings might \nimprove the model performance. Thus, for the third additional training, we combined \ncontextual string embeddings with FLERT parameters.\nFig. 6  The results of Experiment 2. A–C comprise diagrams with the F1-scores of the training with three \nalgorithms for each label class. D Represents the total accuracy of the training algorithms Scientometrics\n1 3\nResults\nResults of the training are represented in Fig.  6. During the training with three types of \nentities (Fig.  6B) IND and GRNB still achieved high F1-scores of 0.96 (IND) and 0.95 \n(GRNB). Nevertheless, ORG gained only an F1-score of 0.64, which is worse than the \nprevious results with six entity types. The results of the training with five types of entities \nwere quite similar to those achieved during the training with six types of entities. FUND \nand UNI categories showed a small improvement in precision, recall, and F1 score com-\npared to training with 6 types of entities with Flair Embeddings. At the same time, the \nperformance of the COR category deteriorated noticeably (0.6 vs. the previous 0.7). The \nimprovement in overall accuracy (Fig.  6D) (0.80 vs. the previous 0.77) could be explained \nby the fact that the MISC category was not present in this training and could not affect \noverall accuracy with its low F1-score.\nAs Fig.  6C demonstrates, training with Flair Embeddings and RoBERTa showed no \nimprovements compared to the results of the primary training with Transformers and worse \nperformance compared with Flair Embeddings. As in Experiment 1, the COR category \nachieved high precision but low recall, resulting in a low F1-score (0.67). For some catego-\nries (COR and GRNB) Flair Embeddings combined with RoBERTa performed better than \nTransformers but still worse than Flair Embeddings.\nExperiment 3\nThe results of experiment 2 showed that altering the training parameters and decreasing the \nnumber of entity classes does not improve the model accuracy. We assume that increasing \nthe size of the training corpus would improve the performance of entities with low recogni-\ntion accuracy. Therefore, for this experiment, we designed two corpora with an increased \nnumber of acknowledged entities.\nAs the Flair Embeddings algorithm trained with Corpus No.2 showed the best perfor -\nmance, it was of interest if the increased training data will outperform its accuracy score. \nTraining in Experiments 1 and 2 was carried out using Flair version 0.9. As Flair recently \nupdated to version 0.11, we used this newest version for the following training. The training \nwas carried out with exactly the same parameters as the training with the Flair Embeddings \nFig. 7  The results of Experiment 3. A Comprises diagrams with the F1-scores of training with three cor -\npora for each label class. B Represents the total accuracy of the trainingScientometrics \n1 3\nmodel in Experiment 1 (Sect. 3.1). To achieve comparable results we also retrained, for \nnow, the best model (Flair Embeddings with Corpus No.2) with the Flair 0.11.\nResults\nResults of the training are represented in Fig.  7. Retraining of the original model with the \nFlair 0.11 Fig.  7-B showed slightly better performance (0.79 vs. 0.77) than training with \nversion 0.9. In general, no huge differences in accuracy were found during training with \nextended corpora.\nOverall, the best F1-Score for the FUND category (0.77) was reached with the TARS \nalgorithm and corpus No.2. COR gained the best accuracy (0.7) with Flair Embeddings \nand corpus No.2 using Flair version 0.9. The GRNB category showed the best perfor -\nmance (0.96) with Flair Embeddings trained on the corpus with five types of entities \n(Flair Embeddings 5 Ent). The best F1-Score of the IND category was achieved with \nFlair Embeddings trained on corpus No.2 with Flair version 0.11. MISC performed the \nbest (0.66) with Flair Embeddings trained on Corpus No.4 with Flair version 0.11. The \nbest accuracy of the UNI category was achieved with Flair Embeddings trained on corpus \nNo.3 with Flair version 0.11. In general, the best overall accuracy of 0.79 (for six entity \ntypes) had the Flair Embeddings model trained on corpus No.2 with Flair version 0.11.\nDiscussion\nAs expected, Experiment 1 showed a large improvement in accuracy for all algorithms \nwhen the size of a training corpus was increased from 49 to 654 sentences. However, fur -\nther enlargement of the corpus (in Experiment 3) did not make any progress. Some types \nof entity, such as IND and GRNB, showed great performance (GRNB with an F1-Score of \n0.96 or IND with 0.98) with the small training samples, i.e., 354 entities from the GRNB \ncategory or 439 entities from the IND category. At the same time, training with a sample of \n1322 labelled funding organisations achieved an F1-Score of only 0.75.\nThe TARS model is designed to perform NER with small or no training data. In \nexperiment 1, TARS without training data was able to extract individuals with quite high \naccuracy (F-1 score of 0.86). TARS trained with the small corpus (No. 1) did not show \nimprovement in the F-1 score of individuals, but greatly improved the F-1 score of the \nGRNB category. For other entity types, this model showed extremely weak results. It was \nexpected that training with Flair Embeddings and Transformers will not bring high recog-\nnition accuracy with corpus No.1, however, interesting results can be observed. Thus, Flair \nEmbeddings showed decent accuracy of 0.8 for individuals with the small training dataset.\nThe imbalance in the performance of different types of entities can be explained by the \nnature of the data, on which the original models were trained. Thus, Flair Embeddings were \ntrained on the 1-billion words English corpus (Chelba et al., 2013). RoBERTa was pre-trained \non the combination of five datasets containing news articles, blog entries, books, and Wikipe-\ndia articles. TARS was mainly pre-trained on datasets for text classification. Thus, the models \nused were not trained on domain-specific data. This can also explain the pure Transformers \nand TARS performance. The higher accuracy for the individuals category in the training with \nTARS can be explained by the fact, that the word ’person’ is semantically more straightforward \nthan other categories. The same could be applied to grant numbers. Furthermore, grant num-\nbers generally have similar patterns, which can be applied to all entities of this type, that can  Scientometrics\n1 3\nexplain a rapid improvement in F-1 score between zero-shot and few-shot models. Moreover, \nIND and GRNB categories showed better performance for other algorithms too, which could \nlie in the structure of these entities: names of individuals and grant numbers usually have undi-\nversified patterns and in acknowledgement texts are used in a small variety of contexts. At the \nsame time, other entity types, such as funding organisations and universities could have similar \npatterns and could be used in the same context. In some cases, even for human annotators, it \nis impossible to distinguish between university, funding body and corporation without back -\nground knowledge about the entity.\nPrevious works showed improvements in downstream tasks using embedding models \nfine-tuned for the domain used (Shen et al., 2022; Beltagy et al.., 2019). Therefore, fine-\ntuning the general language model on the sample of acknowledgment texts could improve \nthe performance of the NER model for acknowledgment texts. We are planning to fine-tune \nBERT and Flair Embeddings (contextual string embeddings) on a sample of approx. 5 mil-\nlion acknowledgment texts from WoS and evaluate the performance of the NER models.\nThe results of Experiment 2 generally did not show an improvement in accuracy. On the \ncontrary, training with the three entity types deteriorated the model performance. Train-\ning without the MISC category did not show significant performance progress either. \nMoreover, further analysis of acknowledged entities showed that the miscellaneous cate-\ngory contained very inhomogeneous and partly irrelevant data, making the analysis more \ncomplicated (Smirnova & Mayr, 2023). Therefore, we assume that the model would make \nbetter predictions if the number of entity types is expanded and miscellaneous categories \nexcluded, i.e., the MISC category could be split into the following categories: names of \nprojects, names of conferences, names of software and dataset. Different subcategories \ncould also be distinguished in the FUND category.\nCorpora No.2 and No.3 contain the same number of MISC and COR entities15, while \nin corpus 4 number of occurrences of MISC and COR entities is higher. For MISC and \nCOR, accuracy slightly increased with corpus 4, therefore we assume that the extraction \naccuracy for these entities will increase with the increase of the training data. The situa-\ntion is different for funding organizations and universities. The number of UNI and FUND \nentities increased evenly from corpus No.1 to corpus No.4. Nevertheless, the best result for \nthe UNI category was achieved with corpus No.3. The poor performance of corpus No.4 \ncould be explained by the inclusion of Indian funders. Thus, the names of many Indian \nfunders are very similar to the entities which usually fall into the UNI category, e.g., the \nDepartment of Science and Technology or the Department of Biotechnology. This pattern \nis more common to the entities which fall into the UNI category. Therefore, that might \nmake the exact extraction of UNI and FUND entities more confusing. Moreover, many \nIndian Universities contain the name of individuals, e.g., Rajiv Gandhi University, which \ncan cause confusion of the UNI category with the IND category. Generally, no improve-\nment in increasing the size of the corpus for the FUND category can be explained by the \nambiguous nature of the entities which fall into the FUND category and their semantical \nproximity with other types of entities. Analysis of the extracted entities showed that many \nentities were extracted correctly, but were assigned to the wrong category (Smirnova & \nMayr, 2023). Therefore, an additional classification algorithm applied to extracted entities \ncould improve the model’s performance.\n15 These differences in entity distribution are caused by the peculiarities of acknowledgement information \nstored in WoS. As only acknowledgements with indexed funding information are stored in the database, it \nwas difficult to find an adequate number of acknowledged entities of other typesScientometrics \n1 3\nConclusion\nIn this paper, we evaluated different embedding models for the task of automatic extraction \nand classification of acknowledged entities from acknowledgment texts16. The annotation \nof the training corpora was the most challenging and time-consuming task of all data prep-\naration procedures. Therefore, a semi-automated approach was used to help significantly \naccelerate the procedure.\nThe study’s main limitations were its small size and just one annotator of the training \ncorpora. Additionally, we used acknowledgments texts collected in WoS. WoS only stores \nacknowledgments containing funding information, therefore there was a lack of other types \nof entities, such as corporations or universities in the training data.\nIn the present paper, we aimed to answer three questions. Thus, regarding research ques-\ntion 1, the few-shot and zero-shot models showed very low total recognition accuracy. At \nthe same time, it was observed that some entities performed better than others with all \nalgorithms and training corpora. Thus, individuals gained a good F1-score over 0.8 with \nzero-shot and few-shot models, as also with Flair embeddings trained with the smallest \ncorpus. With the enlargement of the training corpora, the performance of the IND category \nalso increased and achieved an F1-score over 0.9. The GRNB category showed an adequate \nF-1 score of 0.76 with the few-shot algorithm trained with the smallest corpus, following \ntraining with corpus No.2 boosts the F-1 score to over 0.9. Therefore, few-shot and zero-\nshot approaches were not able to identify all the defined acknowledged entity classes.\nWith respect to research question 2, Flair Embeddings showed the best accuracy in \ntraining with corpus No.2 (and version 0.11) and the fastest training time compared to the \nother models; thus, it is recommended to further use the Flair Embeddings model for the \nrecognition of acknowledged entities.\nExploring research question 3 we observed, that the expansion of the size of a training \ncorpus from very small (corpus No.1) to medium size (corpus No.2) massively increased the \naccuracy of all training algorithms. The best-performing model (Flair Embedding) was further \nretrained with the two bigger corpora, but the following expansion of the training corpus did \nnot bring further improvement. Moreover, the performance of the model slightly deteriorated.\nAcknowledgements The original work was funded by the German Center for Higher Education Research \nand Science Studies (DZHW) via the project “Mining Acknowledgement Texts in Web of Science \n(MinAck)”17. Access to the WoS data was granted via the Competence Centre for Bibliometrics18. Data \naccess was funded by BMBF (Federal Ministry of Education and Research, Germany) under grant number \n01PQ17001. Nina Smirnova received funding from the German Research Foundation (DFG) via the project \n“POLLUX”19. The present paper is an extended version of the paper “Evaluation of Embedding Models for \nAutomatic Extraction and Classification of Acknowledged Entities in Scientific Documents” (Smirnova & \nMayr, 2022) presented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Sci-\nentific Documents (EEKE2022).\nAppendix A: Accuracy metrics by type of entity (label) for all \nexperiments\nSee Table 5.\n16 The best model can be tested  at https:// huggi ngface. co/ kalaw inka/ flair- ner- ackno  wledg ments\n17 https:// kalaw inka. github. io/ minack/.\n18 https:// www. bibli ometr  ie. info/ en/ index. php? id= home.\n19 https:// www. pollux- fid. de/ about. Scientometrics\n1 3\nTable 5  Accuracy metrics by type of entity (label) for all experiments\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings No.1 9 IND 0.7692 0.8333 0.8000 12 1\nFlair embeddings No.1 9 GRNB 0.5385 0.7000 0.6087 10 1\nFlair embeddings No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nFlair embeddings No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nFlair embeddings No.1 9 COR 0.0000 0.0000 0.0000 1 1\nFlair embeddings No.1 9 FUND 0.4000 0.4444 0.4211 18 1\nFlair embeddings No.2 9 FUND 0.6524 0.7771 0.7093 157 1\nFlair embeddings No.2 9 IND 0.9764 0.9831 0.9797 295 1\nFlair embeddings No.2 9 GRNB 0.9398 0.9750 0.9571 160 1\nFlair embeddings No.2 9 UNI 0.7527 0.7071 0.7292 99 1\nFlair embeddings No.2 9 MISC 0.6420 0.6341 0.6380 82 1\nFlair embeddings No.2 9 COR 0.8750 0.5833 0.7000 12 1\nTARS (pretrained) No.1 9 IND 1.0000 0.7500 0.8571 12 1\nTARS (pretrained) No.1 9 GRNB 0.7273 0.8000 0.7619 10 1\nTARS (pretrained) No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTARS (pretrained) No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTARS (pretrained) No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTARS (pretrained) No.1 9 FUND 0.3158 0.3333 0.3243 18 1\nTARS (pretrained) No.2 9 FUND 0.7257 0.8089 0.7651 157 1\nTARS (pretrained) No.2 9 IND 0.9281 0.8746 0.9005 295 1\nTARS (pretrained) No.2 9 GRNB 0.8895 0.9563 0.9217 160 1\nTARS (pretrained) No.2 9 UNI 0.7407 0.6061 0.6667 99 1\nTARS (pretrained) No.2 9 MISC 0.6719 0.5244 0.5890 82 1\nTARS (pretrained) No.2 9 COR 0.5000 0.5833 0.5385 12 1\nTransformers No.1 9 GRNB 0.3000 0.6000 0.4000 10 1\nTransformers No.1 9 IND 0.0000 0.0000 0.0000 12 1\nTransformers No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTransformers No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTransformers No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTransformers No.1 9 FUND 0.2414 0.3889 0.2979 18 1\nTransformers No.2 9 FUND 0.6211 0.7516 0.6801 157 1\nTransformers No.2 9 IND 0.9346 0.9695 0.9517 295 1\nTransformers No.2 9 GRNB 0.8704 0.8812 0.8758 160 1\nTransformers No.2 9 UNI 0.6476 0.6869 0.6667 99 1\nTransformers No.2 9 MISC 0.4767 0.5000 0.4881 82 1\nTransformers No.2 9 COR 0.7500 0.5000 0.6000 12 1\nFlair embeddings (3 Ent) No.2 9 IND 0.9577 0.9703 0.9639 303 2\nFlair embeddings (3 Ent) No.2 9 ORG 0.6400 0.6154 0.6275 208 2\nFlair embeddings (3 Ent) No.2 9 GRNB 0.9286 0.9750 0.9512 160 2\nFlair embeddings (5 Ent) No.2 9 IND 0.9764 0.9797 0.9780 295 2\nFlair embeddings (5 Ent) No.2 9 GRNB 0.9345 0.9812 0.9573 160 2\nFlair embeddings (5 Ent) No.2 9 UNI 0.7802 0.7172 0.7474 99 2\nFlair embeddings (5 Ent) No.2 9 COR 0.7500 0.5000 0.6000 12 2\nFlair embeddings (5 Ent) No.2 9 FUND 0.6722 0.7707 0.7181 157 2Scientometrics \n1 3\nAppendix B: Overall accuracy for all experiments\nSee Table 6.Rows are sorted by experiment number and algorithmTable 5  (continued)\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings (RoB-\nERTa)No.2 9 IND 0.9206 0.9831 0.9508 295 2\nFlair embeddings (RoB-\nERTa)No.2 9 GRNB 0.8896 0.9062 0.8978 160 2\nFlair embeddings (RoB-\nERTa)No.2 9 UNI 0.5963 0.6566 0.6250 99 2\nFlair embeddings (RoB-\nERTa)No.2 9 MISC 0.4135 0.5244 0.4624 82 2\nFlair embeddings (RoB-\nERTa)No.2 9 COR 1.0000 0.5000 0.6667 12 2\nFlair embeddings (RoB-\nERTa)No.2 9 FUND 0.6096 0.7261 0.6628 157 2\nFlair embeddings No.2 11 GRNB 0.9345 0.9812 0.9573 160 3\nFlair embeddings No.2 11 IND 0.9797 0.9831 0.9814 295 3\nFlair embeddings No.2 11 FUND 0.7027 0.8280 0.7602 157 3\nFlair embeddings No.2 11 UNI 0.7684 0.7374 0.7526 99 3\nFlair embeddings No.2 11 MISC 0.6543 0.6463 0.6503 82 3\nFlair embeddings No.2 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 UNI 0.8000 0.7273 0.7619 99 3\nFlair embeddings No.3 11 IND 0.9731 0.9797 0.9764 295 3\nFlair embeddings No.3 11 GRNB 0.9281 0.9688 0.9480 160 3\nFlair embeddings No.3 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 MISC 0.6571 0.5610 0.6053 82 3\nFlair embeddings No.3 11 FUND 0.6757 0.7962 0.7310 157 3\nFlair embeddings No.4 11 MISC 0.7424 0.5976 0.6622 82 3\nFlair embeddings No.4 11 COR 0.8571 0.5000 0.6316 12 3\nFlair embeddings No.4 11 UNI 0.7753 0.6970 0.7340 99 3\nFlair embeddings No.4 11 IND 0.9698 0.9797 0.9747 295 3\nFlair embeddings No.4 11 FUND 0.6823 0.8344 0.7507 157 3\nFlair embeddings No.4 11 GRNB 0.9162 0.9563 0.9358 160 3 Scientometrics\n1 3\nFunding Open access funding enabled and organized by Projekt DEAL.\nDeclarations \n Conflict of interest Philipp Mayr, the co-author of this paper, has a conflict of interest because he serves on \nthe editorial board of the journal Scientometrics. In addition, he is a co-guest editor of the special issue on \n\"Extraction and Evaluation of Knowledge Entities from Scientific Documents\". He declares that he has noth-\ning to do with the decision about this paper submission.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAkbik, A., Bergmann, T., Blythe, D., Rasul, K., Schweter, S., & Vollgraf, R. 2019. FLAIR: An Easy-to-Use \nFramework for State-of-the-Art NLP. Minneapolis, Minnesota (pp. 54–59). Association for Computa-\ntional Linguistics.\nAkbik, A., Blythe, D., & Vollgraf, R. (2018). Contextual string embeddings for sequence labeling. In 2018, \n27th International Conference on Computational Linguistics (pp. 1638–1649).\nAlexandera, D. & Vries, A. P. (2021). This research is funded by...”: Named Entity Recognition of financial \ninformation in research papers. In BIR 2021: 11th International Workshop on Bibliometric-enhanced \nInformation Retrieval at ECIR (pp. 102–110).\nBeltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. \nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing \nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)  \n(pp. 3613–3618). Association for Computational Linguistics.Table 6  Overall accuracy for all experiments\nRows are sorted by experiment number and algorithmAlgorithm Corpus Version Accuracy Experiment\nFlair embeddings No.2 9 0.7702 1\nFlair embeddings No.1 9 0.3472 1\nTARS (pretrained) No.2 9 0.7113 1\nTARS (pretrained) No.1 9 0.3485 1\nTransformers No.2 9 0.6783 1\nTransformers No.1 9 0.1477 1\nFlair Embeddings (3 Entity Types) No.2 9 0.7536 2\nFlair embeddings (5 Entity Types) No.2 9 0.7990 2\nFlair embeddings + RoBERTa No.2 9 0.6697 2\nFlair Embeddings No.2 11 0.7869 3\nFlair embeddings No.4 11 0.7814 3\nFlair embeddings No.3 11 0.7691 3Scientometrics \n1 3\nBorst, T., Mielck, J., Nannt, M., & Riese, W. (2022). Extracting funder information from scien-\ntific papers—Experiences with question answering. In Silvello, G., O.  Corcho, P.  Manghi, G.M. \nDi Nunzio, K. Golub, N. Ferro, and A. Poggi (Eds.),Linking theory and practice of digital libraries  \n(Vol. 13541, pp. 289–296). Springer International Publishing. Series Title: Lecture Notes in Com-\nputer Science. https:// doi. org/ 10. 1007/ 978-3- 031- 16802-4_ 24.\nChelba, C., T.  Mikolov, M.  Schuster, Q.  Ge, T.  Brants, P.  Koehn, & Robinson, T. (2013). One Bil-\nlion Word Benchmark for Measuring Progress in Statistical Language Modeling. 10.48550/\nARXIV.1312.3005 .\nChen, H., Song, X., Jin, Q., & Wang, X. (2022). Network dynamics in university-industry collaboration: \nA collaboration-knowledge dual-layer network perspective. Scientometrics, 127(11), 6637–6660. \nhttps:// doi. org/ 10. 1007/ s11192- 022- 04330-9\nCronin, B. (1995). The Scholar’s courtesy: The role of acknowledgement in the primary communication \nprocess. Taylor Graham.\nCronin, B., & Weaver, S. (1995). The praxis of acknowledgement: From bibliometrics to influmetrics. \nRevista Española de Documentación Científica, 18(2), 172.\nDevlin, J., Chang, M.W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional \ntransformers for language understanding. 10.48550/ARXIV.1810.04805 .\nDiaz-Faes, A. A., & Bordons, M. (2017). Making visible the invisible through the analysis of acknowl-\nedgements in the humanities. Aslib Journal of Information Management, 69(5), 576–590. https://  \ndoi. org/ 10. 1108/ AJIM- 01- 2017- 0008\nDoehne, M., & Herfeld, C. (2023). How academic opinion leaders shape scientific ideas: an acknowl-\nedgment analysis., 128(4), 2507–2533. https:// doi. org/ 10. 1007/ s11192- 022- 04623-z\nDzieżyc, M., & Kazienko, P. (2022). Effectiveness of research grants funded by European research coun-\ncil and polish national science centre. Journal of Informetrics, 16(1), 101243. https:// doi. org/ 10.  \n1016/j. joi. 2021. 101243\nEftimov, T., Koroušić Seljak, B., & Korošec, P. (2017). A rule-based named-entity recognition \nmethod for knowledge extraction of evidence-based dietary recommendations. PLoS ONE, 12(6), \ne0179488. https:// doi. org/ 10. 1371/ journ al. pone. 01794 88\nEtzioni, O., Cafarella, M., Downey, D., Popescu, A. M., Shaked, T., Soderland, S., Weld, D. S., & Yates, \nA. (2005). Unsupervised named-entity extraction from the web: An experimental study. Artificial \nIntelligence, 165(1), 91–134. https:// doi. org/ 10. 1016/j. artint. 2005. 03. 001\nFinkel, J.R., Grenager, T., & Manning, C. (2005). Incorporating non-local information into information \nextraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL’05), Ann Arbor, Michigan (pp. 363–370). Association for \nComputational Linguistics.\nGiles, C. L., & Councill, I. G. (2004). Who gets acknowledged: Measuring scientific contributions \nthrough automatic acknowledgment indexing. Proceedings of the National Academy of Sciences,  \n101(51), 17599–17604. https:// doi. org/ 10. 1073/ pnas. 04077 43101\nHalder, K., Akbik, A., Krapac, J., & Vollgraf, R. (2020). Task-Aware Representation of Sentences for \nGeneric Text Classification. In Proceedings of the 28th International Conference on Computational \nLinguistics, Barcelona, Spain (Online) (pp. 3202–3213). International Committee on Computa-\ntional Linguistics.\nHubbard, D., Laddusaw, S., Tan, Q., & Hu, X. (2022). Analysis of acknowledgments of libraries in the \njournal literature using machine learning. Proceedings of the Association for Information Science \nand Technology, 59(1), 709–711. https:// doi. org/ 10. 1002/ pra2. 698\nIovine, A., Fang, A., Fetahu, B., Rokhlenko, O., & Malmasi, S. (2022). CycleNER: An unsupervised \ntraining approach for named entity recognition. In Proceedings of the ACM Web Conference 2022  \n(pp. 2916–2924). ACM.\nJiang, L., Kang, X., Huang, S., & Yang, B. (2022). A refinement strategy for identification of scientific \nsoftware from bioinformatics publications. Scientometrics, 127(6), 3293–3316. https:// doi. org/ 10.  \n1007/ s11192- 022- 04381-y\nKassirer, J. P., & Angell, M. (1991). On authorship and acknowledgments. The New England Journal of \nMedicine, 325(21), 1510–1512. https:// doi. org/ 10. 1056/ NEJM1 99111 21325 2112\nKayal, S., Afzal, Z., Tsatsaronis, G., Katrenko, S., Coupet, P., Doornenbal, M., & Gregory, M. (2017). \nTagging funding agencies and grants in scientific articles using sequential learning models. In \nBioNLP 2017, Vancouver, Canada (pp. 216–221). Association for Computational Linguistics.\nKenekayoro, P. (2018). Identifying named entities in academic biographies with supervised learning. \nScientometrics, 116(2), 751–765. https:// doi. org/ 10. 1007/ s11192- 018- 2797-4\nKingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. 10.48550/\nARXIV.1412.6980 . Scientometrics\n1 3\nKusumegi, K., & Sano, Y. (2022). Dataset of identified scholars mentioned in acknowledgement state-\nments. Scientific Data, 9(1), 461. https:// doi. org/ 10. 1038/ s41597- 022- 01585-y\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoy -\nanov, V. (2019). RoBERTa: A robustly optimized BERT pretraining approach. arXiv: 1907. 11692  \n[cs] .\nMackintosh, K. (1972). Acknowledgements patterns in sociology. Ph. D. thesis, University of Oregon.\nMccain, K. (2017). Beyond Garfield’s citation index: An assessment of some issues in building a per -\nsonal name acknowledgments index. Scientometrics. https:// doi. org/ 10. 1007/ s11192- 017- 2598-1\nMcCain, K. W. (1991). Communication, competition, and secrecy: The production and dissemination of \nresearch-related information in genetics. Science, Technology, & Human Values, 16(4), 491–516. \nhttps:// doi. org/ 10. 1177/ 01622 43991 01600 404\nMejia, C., & Kajikawa, Y. (2018). Using acknowledgement data to characterize funding organizations \nby the types of research sponsored: the case of robotics research. Scientometrics, 114(3), 883–904. \nhttps:// doi. org/ 10. 1007/ s11192- 017- 2617-2\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, \nN., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkur -\nthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: An imperative style, high-\nperformance deep learning library. arXiv: 1912. 01703 [cs, stat].\nPaul-Hus, A., & Desrochers, N. (2019). Acknowledgements are not just thank you notes: A qualitative \nanalysis of acknowledgements content in scientific articles and reviews published in 2015. PLoS \nONE, 14, e0226727. https:// doi. org/ 10. 1371/ journ al. pone. 02267 27\nPaul-Hus, A., Díaz-Faes, A., Sainte-Marie, M., Desrochers, N., Costas, R., & Larivière, V. (2017). \nBeyond funding: Acknowledgement patterns in biomedical, natural and social sciences. PLoS ONE,  \n12, e0185578. https:// doi. org/ 10. 1371/ journ al. pone. 01855 78\nPennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors for Word Representation. \nIn Empirical Methods in Natural Language Processing (EMNLP) (pp. 1532–1543).\nPustejovsky, J., & Stubbs, A. (2012). Natural language annotation for machine learning. O’Reilly \nMedia Inc.\nRose, M., & Georg, C. P. (2021). What 5,000 acknowledgements tell us about informal collaboration \nin financial economics. Research Policy, 50, 104236. https:// doi. org/ 10. 1016/j. respol. 2021. 104236\nSang, T. K., & E. F., & De Meulder, F. (2003). Introduction to the CoNLL-2003 shared task: Language-\nindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Lan-\nguage Learning at HLT-NAACL (pp. 142–147).\nSchweter, S., & Akbik, A. (2020). FLERT: Document-level features for named entity recognition. ArXiv. \n10.48550/arXiv.2011.06993 .\nShen, S., Liu, J., Lin, L., Huang, Y., Zhang, L., Liu, C., Feng, Y., & Wang, D. (2022). SsciBERT: A \npre-trained language model for social science texts. Scientometrics. https:// doi. org/ 10. 1007/  \ns11192- 022- 04602-4\nSingh, V. K., Singh, P., Karmakar, M., Leta, J., & Mayr, P. (2021). The journal coverage of web of sci-\nence, scopus and dimensions: A comparative analysis. Scientometrics, 126(6), 5113–5142. https://  \ndoi. org/ 10. 1007/ s11192- 021- 03948-5\nSmirnova, N., & Mayr, P. (2022). Evaluation of embedding models for automatic extraction and classifi-\ncation of acknowledged entities in scientific documents. In 3rd Workshop on Extraction and Evalu-\nation of Knowledge Entities from Scientific Documents 2022 (EEKE 2022) (pp. 48–55). CEUR-WS.\norg.\nSmirnova, N., & Mayr, P. (2023). A comprehensive analysis of acknowledgement texts in web of sci-\nence: A case study on four scientific domains. Scientometrics, 1(128), 709–734. https:// doi. org/ 10.  \n1007/ s11192- 022- 04554-9\nSong, M., Kang, K. Y., Timakum, T., & Zhang, X. (2020). Examining influential factors for acknowl-\nedgements classification using supervised learning. PLoS ONE. https:// doi. org/ 10. 1371/ journ al.  \npone. 02289 28\nThomer, A. K., & Weber, N. M. (2014). Using named entity recognition as a classification heuristic. In \niConference 2014 Proceedings (pp. 1133 – 1138). iSchools.\nWang, J., & Shapira, P. (2011). Funding acknowledgement analysis: An enhanced tool to investigate \nresearch sponsorship impacts: The case of nanotechnology. Scientometrics, 87(3), 563–586. https://  \ndoi. org/ 10. 1007/ s11192- 011- 0362-5\nYamada, I., Asai, A., Shindo, H., Takeda, H., & Matsumoto, Y. (2020). LUKE: Deep contextualized \nentity representations with entity-aware self-attention. In Proceedings of the 2020 Conference on \nEmpirical Methods in Natural Language Processing (EMNLP) (pp. 6442–6454). Association for \nComputational Linguistics.Scientometrics \n1 3\nYu, J., Bohnet, B., & Poesio, M. (2020). Named entity recognition as dependency parsing. 10.48550/\nARXIV.2005.07150.\nZhang, C., Mayr, P., Lu, W., & Zhang, Y. (2023). Guest editorial: Extraction and evaluation of knowledge \nentities in the age of artificial intelligence. Aslib Journal of Information Management, 75, 433–437. \nhttps:// doi. org/ 10. 1108/ AJIM- 05- 2023- 507",
        "type": "direct"
    },
    {
        "citation": " al 1995",
        "content": ". Cronin and Weaver",
        "type": "regular"
    },
    {
        "citation": " al 2004",
        "content": "ascribe an acknowledgment alongside authorship and citedness to measures of a \nresearcher’s scholarly performance: a feature that reflects the researcher’s productivity and \nimpact. Giles and Councill",
        "type": "regular"
    },
    {
        "citation": "Kusumegi and Sano, 2004, p. 17599",
        "content": "argue that acknowledgments to individuals, in the same \nway as citations, may be used as a metric to measure an individual’s intellectual contribu-\ntion to scientific work. Acknowledgments of financial support are interesting in terms of \nevaluating the influence of funding agencies on academic research. Acknowledgments of \ntechnical and instrumental support may reveal “indirect contributions of research labora-\ntories and universities to research activities” .\nThe analysis of acknowledgments is particularly interesting as acknowledgments may \ngive an insight into aspects of the scientific community, such as reward systems (Dzieżyc \n& Kazienko, 2022), collaboration patterns, and hidden research trends (Giles & Councill, \n2004; Diaz-Faes & Bordons, 2017). From the linguistic point of view, acknowledgments \nare unstructured text data, which through automatic analysis poses research and methodo-\nlogical problems like data cleaning, choosing the proper tokenization method, and whether \nand how word embeddings may enhance their automatic analysis.\nTo our knowledge, previous works on automatic acknowledgment analysis were mostly \nconcerned with the extraction of funding organizations and grant numbers (Alexandera & \nVries, 2021; Kayal et  al., 2017; Borst et  al., 2022) or classification of acknowledgment \ntexts (Song et al., 2020; Hubbard et al., 2022). Furthermore, large bibliographic databases \nsuch as Web of Science (WoS)1 and Scopus selectively index only funding information, \ni.e., names of funding organizations and grant identification numbers. Consequently, we \nwant to extend that to other types of acknowledged entities: individuals, universities, cor -\nporations, and other miscellaneous information. Analysis of the acknowledged individuals \nprovides insight into informal scientific collaboration (Rose & Georg, 2021; Kusumegi & \nSano, 2022). Acknowledged universities and corporations reveal interactions and knowl-\nedge exchange between industry and universities (Chen et  al., 2022). Entities from the \nmiscellaneous category include other information like project names, which could uncover \ninternational scientific collaborations.\nThe state-of-the-art named entity recognition (NER) models showed a great perfor -\nmance on the CoNLL-2003 dataset (Akbik et al., 2018; Devlin et al., 2018; Yamada et al., \n2020; Yu et  al., 2020). CoNLL-2003 corpus (Sang et  al., 2003) is a benchmark dataset \nfor language-independent named entity recognition, i.e., designed to train and evaluate \nNER models. English data for the corpus were taken from the Reuters corpus. The dataset \ncomprises four types of named entities: person, location, organisation, and miscellaneous. \nHowever, specific domains require specifically labelled training data. The development of a \ntraining dataset for the specific domain is an expensive and time-consuming process since \nNER usually requires a quite large training corpus. Therefore, the objective of this paper is \nto evaluate the performance of existing embedding models for the task of automatic extrac-\ntion and classification of acknowledged entities from the acknowledgment text in scientific \npapers using small training datasets or without training data (zero-short approach).\n1 http:// wokin fo. com/ produ cts_ tools/ multi disci plina ry/ webof  scien ce/ fundi ngsea rch/.Scientometrics \n1 3\nThe present paper is an extended version of the article (Smirnova & Mayr, 2022)2 pre-\nsented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Sci-\nentific Documents (EEKE2022).3 Flair, an open-source natural language processing (NLP) \nframework (Akbik et al., 2019) is used in our study to create a tool for the extraction of \nacknowledged entities because this library is easily customizable. It offers the possibility \nof creating a customized Named Entity Recognition (NER) tagger, which can be used for \nprocessing and analyzing acknowledgment texts. Furthermore, Flair has shown better accu-\nracy for NER tasks using pre-trained datasets in comparison with many other open source \nNLP tools.4\nIn the first experiment (Sect.  4.1) we trained and implemented a NER task using three \ndefault Flair NER models with two differently-sized corpora.5 All the descriptions of the \nFlair framework features refer to the releases 0.9 and 0.11. The models were trained to rec-\nognize six types of acknowledged entities: funding agency, grant number, individuals, uni-\nversity, corporation, and miscellaneous. The model with the best accuracy can be applied \nfor the comprehensive analysis of the acknowledgment texts. In Experiments 2 and 3 we \nperformed additional training with altered training parameters or altered training corpora \n(Sects  4.2 and 4.3). Most of the previous works on acknowledgment analysis were limited \nby the manual evaluation of data and therefore by the amount of processed data (Giles \n& Councill, 2004; Paul-Hus et al., 2017; Paul-Hus & Desrochers, 2019; Mccain, 2017). \nFurthermore, Thomer and Weber (2014) argues that using named entities can benefit the \nprocess of manual document classification and evaluation of the data. Therefore, a model \nthat is capable of extracting and classification of different types of entities may potentially \nmake a significant contribution to the field of automated acknowledgment analysis.\nResearch questions\nIn this paper, we address the following research questions:\n• RQ1: Is the few-shot or zero-shot approach able to identify predefined acknowledged \nentity classes?\n• RQ2: Which of the Flair default NER models is more suitable for the defined task of \nextraction and classification of acknowledged entities from scientific acknowledgments \nusing a small training dataset?\n• RQ3: How does the size of the training corpus affect the training accuracy for different \nNER models?\nCreating a training dataset for supervised learning is a time-consuming and expensive task, \nsince as a rule, such a model requires a reasonably large amount of training data. Annota-\ntion is a crucial moment, as wrongly annotated data will deteriorate training results. There-\nfore, more than one annotator is usually required to provide credible results. That is why \n2 In this paper we conducted an additional experiment (Experiment 3) with 2 new corpora (corpus Nos. 3 \nand 4).\n3 https:// eeke- works hop. github. io/ 2022/.\n4 https:// github. com/ flair NLP/ flair .\n5 The release 0.9 (https:// github. com/ flair NLP/ flair/ relea ses/ tag/ v0.9) was used in the experiments 1 and 2. \nExperiment 3 was performed using release 0.11 (https:// github. com/ flair NLP/ flair/ relea ses/ tag/ v0. 11). Scientometrics\n1 3\nit is of interest to test if the existing NER models can provide reasonable accuracy while \nusing small or no training data.\nBackground and related work\nResearch in the field of acknowledgments analysis has been carried out since the 1970s. \nThe first typology of acknowledgments was proposed by Mackintosh (1972) (as cited \nin Cronin, 1995) and comprised three categories: facilities, access to data, and help of indi-\nviduals. McCain (1991) distinguished five types of acknowledgements: research-related \ninformation, secondary access to research-related information, specific research-related \ncommunication, general peer communication, and technical or clerical support. Cronin and \nWeaver (1995) defined three broad categories: resource-, procedure- and concept-related. \nMejia and Kajikawa (2018) developed a four-level classification based on sponsored \nresearch field: change maker, incremental, breakthrough, and matured.\nDoehne and Herfeld (2023) distinguished acknowledgements from the perspective of \nappreciation of influential scholars and defined two axes: scientific influence and insti-\ntutional influence. Scientific influence refers to the productiveness and creativity of the \nresearcher, while institutional influence is associated with the scholar’s administrative posi-\ntion in the scientific community.\nWang and Shapira (2011) investigated the connection between research funding and \nthe development of science and technology using acknowledgments from articles from the \nfield of nanotechnology. Rose and Georg (2021) studied informal cooperation in academic \nresearch. The analysis revealed generational and gender differences in informal collabora-\ntion. The authors claim that information from informal collaboration networks makes bet-\nter predictions of the academic impact of researchers and articles than information from \nco-author networks. Mejia and Kajikawa (2018) argued that the classification of funders \ncould be useful in developing funding strategies for policymakers and funders.\nDoehne and Herfeld (2023) manually investigated acknowledgement sections of papers, \nwhich were published or preprinted in association with the Cowles Foundation between \nearly 1940 and 1970 to trace the influence of the informal social structure and academic \nleaders on the early acceptance of scientific innovations. Blockmodelling was applied to \nthe acknowledgement data. Their analysis showed that the adoption of scientific innova-\ntions was partly influenced by the social structure and by the scientific leaders at Cowles.\nRecent advances in NER\nNamed Entity Recognition (NER) is a form of NLP that aims to extract named entities \nfrom unstructured text and classify them into predefined categories. A named entity is a \nreal-world object that is important for understanding the text. Current approaches in NER \ncan be distinguished into supervised and unsupervised tasks. In a supervised NER a model \nis trained using a labelled dataset. This training dataset or corpus is usually split into sev -\neral datasets: training set, test set, and validation set. NER models require corpora with \nsemantic annotation, i.e., metadata about concepts attached to unstructured text data. The \nannotation process is crucial as insufficient or redundant metadata can slow down and bias \na learning process (Pustejovsky & Stubbs, 2012, Chapter 1).\nSupervised NER mainly relays on machine learning or deep learning methods. The \nstate-of-the-art models are based on deep recurrent models, convolution-based, or Scientometrics \n1 3\npre-trained transformer architectures (Iovine et  al., 2022). Thus, Akbik et  al. (2018) \nproposed a new character-based contextual string embeddings method. This approach \npasses a sequence of characters through the character-level language model to gener -\nate word-level embeddings. The model was pre-trained on large unlabeled corpora. The \ntraining was carried out using a character-based neural language model together with a \nBidirectional LSTM (BiLSTM) sequence-labelling model. This approach generates dif-\nferent embeddings for the same word depending on its context and showed good results \non downstream tasks such as NER. Devlin et al. (2018) presented BERT (Bidirectional \nEncoder Representations Transformers), a transformer-based language representa-\ntion model that models the representation of contextualized word embeddings. BERT \nshowed superior results on downstream tasks using different benchmarking datasets. \nLater, Liu et al. (2019) performed an optimization of the BERT model and introduced \nRoBERTa (Robustly Optimized BERT Pretraining Approach). RoBERTa was evaluated \non three benchmarks and demonstrated massive improvements over the reported BERT \nperformance.\nCurrently, several domain-specific models have been developed. Thus, Beltagy et  al.. \n(2019) released SciBERT a BERT-based language model pre-trained on a large number \nof unlabeled scientific articles from the computer science and biomedical domains. SciB-\nERT showed improvements over BERT on several downstream NLP tasks, including NER. \nRecently, Shen et al. (2022) introduced the SsciBERT, a language model based on BERT \nand pre-trained on abstracts published in the Social Science Citation Index (SSCI) journals. \nThe model showed good results in discipline classification and abstract structure-function \nrecognition in articles from the social sciences domain.\nUnsupervised methods are often based on lexicons or predefined rules. Thus, Etzioni \net al. (2005) uses lists of patterns and domain-specific rules to extract named entities. Efti-\nmov et al. (2017) developed a rule-based NER model to extract dietary information from \nscientific publications. Evaluation of the model performance showed good results. Opposed \nto previous unsupervised NER approaches, Iovine et al. (2022) proposed a cycle-consist-\nency approach for NER (CycleNER). CycleNER is unsupervised and does not require par -\nallel training data. The method showed 73% of supervised performance on CoNLL03.\nNER in scientometrics analysis\nNamed entities are widely used in scientometrics analysis. Thus, Kenekayoro (2018) devel-\noped a supervised method for the automatic extraction of named entities from academic \nbibliographies. The aim of the study was to create a database containing unified academic \ninformation about individuals to help in expert finding. A labeled training dataset was \ndeveloped using biographies extracted from ORCID.6 The authors tested several models \nfor NER. The Support Vector Machine classification algorithm (SVM) showed the best \nperformance.\nJiang et al. (2022) proposed a strategy for the identification of software in scientific bio-\ninformatics publications using the combination of SVM and CRF (Conditional Random \nField). Application of the method to the sample of articles from bioinformatics domains \nallowed them to observe interesting patterns in using software in scientific research.\nKusumegi and Sano (2022) analysed scholarly relationships by analysing acknowl-\nedged individuals from the acknowledgments statements from eight open-access journals. \n6 https:// orcid. org/. Scientometrics\n1 3\nIndividuals were extracted using the Stanford CoreNLP NER tagger. In the next steps, \nscholars were identified among the extracted individuals by mapping them to the Microsoft \nAcademic Graph (MAG).\nWe are aware of several works on automated information extraction from acknowledg-\nments. Giles and Councill (2004) developed an automated method for the extraction and \nanalysis of acknowledgment texts using regular expressions and SVM. Computer science \nresearch papers from the CiteSeer digital library were used as a data source. Extracted enti-\nties were analysed and manually assigned to the following four categories: funding agen-\ncies, corporations, universities, and individuals.\nThomer and Weber (2014) used the 4-class Stanford Entity Recognizer (Finkel et al., \n2005) to extract persons, locations, organizations, and miscellaneous entities from the col-\nlection of bioinformatics texts from PubMed Central’s Open Access corpus. The aim of \nthe study was to determine an approach to \"increase the speed of ... classification without \nsacrificing accuracy, nor reliability\" (Thomer & Weber, 2014, p. 1134).\nKayal et  al. (2017) introduced a method for extraction of funding organizations and \ngrants from acknowledgment texts using a combination of sequential learning models: con-\nditional random fields (CRF), hidden markov models (HMM), and maximum entropy mod-\nels (MaxEnt). The final model contained pooled outputs from the models used.\nAlexandera and Vries (2021) proposed AckNER, a tool for extracting financial informa-\ntion from the funding or acknowledgment section of a research article. AckNER works \nwith the use of dependency parse trees and regular expressions and is able to extract names \nof the organisations, projects, programs, and funds, as also numbers of contracts and \ngrants7.\nFollowing, Borst et  al. (2022) applied a question-answering (QA) based approach to \nidentify funding information in acknowledgments texts. This approach performs similarly \nto AckNER and requires a smaller set of training and test data.\nTable  1 shows an overview of works on NER in scientometrics. Overall, previous works \non the extraction of named entities from acknowledgements texts were mostly concerned \nwith the extraction of funding information, i.e., only names of funding bodies and grant \nnumbers, or extraction and linking of individuals. The special issue by Zhang et al. (2023) \nprovided a recent overview of current works in the extraction of knowledge entities.\nTo the best of our knowledge the work of Giles and Councill (2004) is the only \nattempt to extract and categorise multiple acknowledged entities. Nevertheless, entities \nwere extracted using the SVM algorithm but the classification of entities themselves \nwas produced manually, which limited the number of acknowledgement texts to be ana-\nlysed. Furthermore, as far as we know, there was no research done concerning the eval-\nuation of embedding models for extraction of information from acknowledgement texts \nand no tool for automatic extraction of different kinds of acknowledged entities was \ndeveloped.\n7 AckNER showed better performance as Flair, but is specifically designed to recognize two types of \nacknowledged entities (Alexandera & Vries, 2021), which was insufficient for the present project.Scientometrics \n1 3\nTable 1  Overview of works on NER in scientometrics\nPaper Area of application and aim of \nthe studyCorpus Entities Methods and tools\nGiles and Councill (2004) Extraction of acknowledged enti-\nties form acknowledgementsCiteSeer Funding agencies, Companies, \nEducational Institutions, \nIndividualsSVM for extracting entities and \ntheir manual classification\nThomer and Weber (2014) Using NER to improve classifica-\ntion of acknowledgementsPubMed Central’s Open Access Persons, locations, organizations, \nand miscellaneous4-class Stanford Entity Recognizer\nKayal et al. (2017) Extraction of funding information \nfrom acknowledgementsPubMed Central’s Open Access Funding bodies, grants CRF, HMM, MaxEnt\nKenekayoro (2018) Extraction of biography informa-\ntion from academic biographiesORCID Award, Location, Organization, \nPerson, Position, Specializa-\ntion, OthersSVM\nAlexandera and Vries (2021) Extraction of funding information \nfrom acknowledgementsTU Delft’s institutional repository Funding bodies, grants SpaCy dependency parser + regu-\nlar expressions\nJiang et al. (2022) Extraction of scientific software \nfrom scientific articles (full \ntexts) in bioinformaticsbioinformatics journals EnsembleSVMs-CRF\nBorst et al. (2022) Extraction of funding information \nfrom acknowledgementsEconStor Funding bodies, grants Haystack\nKusumegi and Sano (2022) Extraction and linking of \nacknowledged individuals from \nacknowledgementsPLOS Individuals Stanford CoreNLP NER tagger + \nMAG Scientometrics\n1 3\nMethod\nIn the present paper, different models for extraction and classification of acknowledged \nentities supported by the Flair NLP framework were evaluated. The choice of classification \nwas inspired by Giles and Councill’s (2004) classification: funding agencies (FUND), cor -\nporations (COR), universities (UNI), and individuals (IND). For our project, this classifica-\ntion was enhanced with the miscellaneous (MISC) and grant numbers (GRNB) categories. \nThe GRNB category was adopted from WoS funding information indexing. The entities \nin the miscellaneous category could provide useful information, but cannot be ascribed to \nother categories, e.g., names of projects and names of conferences. Figure  1 demonstrates \nan example of acknowledged entities of different types. To the best of our knowledge, Giles \nand Councill’s classification is the only existing classification of acknowledged entities and \ntherefore can be applied to the NER task. Other works on acknowledgment analysis were \nfocused on the classification of acknowledgment texts.\nThe Flair NLP framework\nFlair is an open-sourced NLP framework built on PyTorch (Paszke et  al., 2019), which \nis an open-source machine learning library. “The core idea of the framework is to pre-\nsent a simple, unified interface for conceptually very different types of word and document \nembeddings” (Akbik et  al., 2019, p.  54). Flair has three default training algorithms for \nNER which were used for the first experiment in the present research: a) NER Model with \nFlair Embeddings (later on Flair Embeddings) (Akbik et al., 2018), b) NER Model with \nTransformers (later on Transformers) (Schweter & Akbik, 2020), and c) Zero-shot NER \nwith TARS (later on TARS) (Halder et al., 2020) 8.\nThe Flair Embeddings model uses stacked embeddings, i.e., a combination of contex-\ntual string embeddings (Akbik et al., 2018) with a static embeddings model. This approach \nwill generate different embeddings for the same word depending on its context. Stacked \nembedding is an important Flair feature, as a combination of different embeddings might \nbring better results than their separate uses (Akbik et al., 2019).\nThe Transformers model or FLERT-extension (document-level features for NER) is a \nset of settings to perform a NER on the document level using fine-tuning and feature-based \nFig. 1  An example of acknowledged entities. Each entity type is marked with a distinct color\n8 New transformer models as SciBERT or SsciBERT were not evaluated in this study, as the objective of \nthe study is to evaluate the performance of the Flair default models.Scientometrics \n1 3\nLSTM-CRF with the multilingual XML-RoBERTa transformer model (Schweter & Akbik, \n2020).\nThe TARS (task-aware representation of sentences) is a transformer-based model, \nwhich allows performing training without any training data (zero-shot learning) or with a \nsmall dataset (few-short learning) (Halder et al., 2020). The TARS approach differs from \nthe traditional transfer learning approach in the way that the TARS model also considers \nsemantic information captured in the class labels themselves. For example, for analyzing \nacknowledgments, class labels like funding organization or university already carry seman-\ntic information.\nTraining data\nThe Web of Science (WoS) database was used to harvest the training data (funding \nacknowledgments).9 From 2008 on, WoS started indexing information about funders and \ngrants. WoS uses information from different funding reporting systems such as Research-\nfish,10 Medline11 and others. As WoS contains millions of metadata records (Singh et al., \n2021), the data chosen for the present study was restricted by year and scientific domain \n(for the corpora Nos. 1, 2, and 3) or additionally by the affiliation country (for corpus No.4). \nTo construct corpora Nos. 1-3 records from four different scientific domains published \nfrom 2014 to 2019 were considered: two domains from the social sciences (sociology and \neconomics) and oceanography and computer science. Different scientific domains were Table 2  Number of sentences/texts in the training corpora\nCorpus No. Training set (train) Test set (test) Validation set (dev) Total\n1 29/27 10/10 10/10 49/47\n2 339/282 165/150 150/136 654/441\n3 784/657 165/150 150/136 1099/816\n4 1148/885 165/150 150/136 1463/1044\nTable 3  Number of sentences/texts from each scientific domain in the training corpora\nCorpus No. Oceanography Economics Social Sciences Computer Science\n1 13/13 3/3 20/20 16/14\n2 127/75 92/58 351/234 173/129\n3 175/112 128/89 590/434 333/269\n9 The present research was conducted in scopes of two projects: MinAck (https:// kalaw inka. github. io/ \nminack/) and SEASON (https:// github. com/ kalaw inka/ season). Corpora Nos.1, 2, and 3 were created for the \nMinAck project and serve the purpose of a general evaluation of the impact of the size of the training cor -\npus on the model performance. Corpus No.4 was designed specifically for the SEASON project in the hope \nof improving the recognition of Indian funding information. The project SEASON aims to get insight into \nGerman-Indian scientific collaboration. Our other corpora mainly contain papers published by European \ninstitutions. That is why we enhance Corpus 4 with the papers published by Indian institutions.\n10 https:// resea rchfi sh. com/.\n11 https:// www. nlm. nih. gov/ bsd/ fundi ng_ suppo rt. html. Scientometrics\n1 3\nchosen since previous work on acknowledgment analysis revealed the relations between \nthe scientific domain and the types of acknowledged entities, i.e., acknowledged individu-\nals are more characteristic of theoretical and social-oriented domains. At the same time, \ninformation on technical and instrumental support is more common for the natural and \nlife sciences domains (Diaz-Faes & Bordons, 2017). Only the WoS record types “article” \nand “review” published in a scientific journal in English were selected; then 1000 distinct \nacknowledgments texts were randomly gathered from this sample for the training dataset. \nFurther different amounts of sentences containing acknowledged entities were distributed \ninto the differently-sized training corpora. Table  2 demonstrates the number of sentences \nin each set in the four corpora. We selected only sentences that contain an acknowledged \nentity, regardless of the scientific domain. Table  3 contains the number of sentences and \ntexts from each scientific domain in the training corpora.12 The same article can belong to \nseveral scientific domains, therefore, the number of sentences and texts in Tables  2 and 3 \ndoes not match. Corpus No.4 was designed in such a way that all the training data from the \nCorpus No.3 was enhanced with acknowledgments texts from the articles that have Indian \naffiliations regardless of scientific domain or publication date.\nPreliminary analysis of the WoS data showed that the indexing of WoS funding infor -\nmation has several issues. The WoS includes only acknowledgments containing funding \ninformation; therefore, not every WoS entry has an acknowledgment, individuals are not \nincluded, and indexed funding organizations are not divided into different entity types like \nuniversities, corporations, etc. Therefore, the existing indexing of funding organizations \nis incomplete. Furthermore, there is a disproportion between the occurrences of acknowl-\nedged entities of different types. Thus, the most frequent entity types in the dataset with \nthe training data are IND, FUND and GRNB, followed by UNI and MISC. COR is the \nFig. 2  The distribution of acknowledged entities in the training corpora\n12 Corpus No.4 is not in Table  2, because the corpus contains additional acknowledgment texts from arti-\ncles with Indian affiliations regardless of the scientific domain and therefore contains different scientific \ndomains.Scientometrics \n1 3\ncategory most underrepresented in the data set. Consequently, there are different amounts \nof entities of different types in the training corpora (as Fig.  2 demonstrates), which might \nhave influenced the training results. Training with the corpora Nos. 2, 3, and 4 was evalu-\nated on the same training and validation datasets to ensure plausible accuracy (Fig.  3-B). \nFig. 3  The distribution of acknowledged entities in the test and validation corpora\nFig. 4  Annotation flowchart Scientometrics\n1 3\nHowever, training with corpus No.1 was evaluated with the smaller test and validation sets, \nas corpus No.1 contains a smaller number of sentences (Fig. 3-A).\nData annotation\nThe training corpus was annotated with six types of entities. As WoS already contains \nsome indexed funding information, it was decided to develop a semi-automated approach \nfor data annotation (as Fig.  4 demonstrates) and use indexed information provided by WoS, \ntherefore, grant numbers were adopted from the WoS indexing unaltered.\nFlair has a pre-trained 4-class NER Flair model (CoNLL-03).13 The model can pre-\ndict four tags: PER (person name), LOC (location), ORG (organization name), and MISC \n(other names). As Flair showed adequate results in the extraction of names of individu-\nals, it was decided to apply the pre-trained 4-class CoNLL-03 Flair model to the training \ndataset. Entities that fell into the PER category were added as the IND annotation to the \ntraining corpus. Furthermore, we noticed that some funding information was partially cor -\nrectly extracted into the ORG and MISC categories. Therefore, WoS funding organization \nindexing and entities from the ORG and MISC categories were adopted and distinguished \nbetween three categories (FUND, COR, and UNI) using regular expressions. In addition, \nthe automatic classification of entities was manually examined and reviewed. Mismatched \ncategories, partially extracted entities, and not extracted entities were corrected. Acknowl-\nedged entities, which fall into the MISC category, were manually annotated by one annota-\ntor. In the miscellaneous category entities referring to names of the conferences and pro-\njects were included.\nExperiments\nIn the present paper, we evaluated three default Flair NER models with four differently-\nsized corpora. In total, we performed three experiments. In the first experiment, mod-\nels with the default parameter were evaluated using corpora Nos. 1 and 2. In the second \nexperiment, we evaluated Flair Embeddings and Transformers model with altered training \nparameters and corpus No.2. In the third experiment, the first experiment was replicated \nwith corpora Nos. 3 and 4.\nExperiment 1\nIn the first experiment, we tested the TARS model zero-shot and few-shot scenarios (with \ncorpus No. 1), as well as the performance of two default FLAIR models (Flair Embeddings \nand Transformers) with corpus No.2. Additionally, the performance of Flair Embeddings \nand Transformers models was tested with the corpus No.1 The training was conducted with \nthe recommended parameters for all algorithms, as Flair developers specifically ran vari-\nous tests to find the best hyperparameters for the default models. For the few-shot TARS, \nthe training was conducted with the small dataset (corpus No.1), and for Transformers and \nFlair Embeddings with a larger dataset (corpus No.2).\n13 https:// github. com/ flair NLP/ flairScientometrics \n1 3\nThe Flair Embeddings model was initiated as a combination of static and contextual \nstring embeddings. We applied GloVe (Pennington et  al., 2014) as a static word-level \nembedding model. Thus, in our case, stacked embeddings comprise GloVe embeddings, \nforward contextual string embeddings, and backward contextual string embeddings. The \nmodel was trained with the recommended parameters: the size of mini-batches was set to \n32 and the maximum number of epochs was set to 150.\nFor Transformers, training was initiated with the RoBERTa model (Liu et al., 2019). For \nthe present paper, a fine-tuning approach was used. The fine-tuning procedure consisted of \nadding a linear layer to a transformer and retraining the entire network with a small learn-\ning rate. We used a standard approach, where only a linear classifier layer was added on the \ntop of the transformer, as adding the additional CRF decoder between the transformer and \nlinear classifier did not increase accuracy compared with this standard approach (Schweter \n& Akbik, 2020). The chosen transformer model uses subword tokenization. We used the \nmean of embeddings of all subtokens and concatenation of all transformer layers to pro-\nduce embeddings. The context around the sentence was considered. The training was initi-\nated with a small learning rate using the Adam Optimisation Algorithm (Kingma & Ba, \n2014).\nThe TARS model requires labels to be defined in a natural language. Therefore, we \ntransformed our original coded labels into the natural language: FUND - “Funding \nAgency”, IND - “Person”, COR - “Corporation”, GRNB - “Grant Number\", UNI - “Uni-\nversity”, and MISC - “Miscellaneous”. The training for the few-shot approach was initiated \nwith the TARS NER model (Halder et al., 2020).\nResults\nOverall, the training demonstrated mixed results. Table  4 shows training results with cor -\npus No.1 and the TARS zero-shot approach. GRNB showed adequate results by training \nwith Flair Embeddings and TARSfew-shot models. IND was the best-recognized entity by \ntraining with Flair Embeddings and TARS (both zero- and few-shot) with an F1-score of \n0.8 (Flair Embeddings) and 0.86 (TARS) respectively. Training with Transformers was not \nsuccessful for IND with an F1-score of 0. In general, transformers were a less efficient \nalgorithm for training with a small dataset with an overall accuracy of 0.35. FUND dem-\nonstrated not satisfactory results with F1-score of less than 0.5 for all models. Entity types \nFig. 5  The training results with the training corpus No.2. A Comprises diagrams with the F1-scores of the \ntraining with three algorithms for each label class. B depicts the total accuracy of training algorithms Scientometrics\n1 3\nMISC, UNI, and COR showed the worst results with the F1-score equal to zero for all algo-\nrithms. The low accuracy for MISC, UNI, and COR resulted in low overall accuracy for \nall algorithms. Overall, training with corpus No.1 showed insufficient results for all algo-\nrithms. Flair Embeddings and TARS showed better accuracy compared to Transformers.\nFigure  5 shows the training results with corpus No.2. Similar to the training with corpus \nNo.1, IND and GRNB are the best-recognized categories. The best results for IND and \nGRNB demonstrated Flair embeddings with an F1-score of 0.98 (IND) and 0.96 (GRNB). \nTARS achieved the best results for FUND with an F1-score of 0.77 against 0.71 for Flair \nEmbeddings and 0.68 for Transformers. Miscellaneous demonstrated the worst accuracy \nfor Flair Embeddings (0.64) and Transformers (0.49), while for TARS the worst accuracy \nlies in the COR category with an F1-score of 0.54. The best result for UNI showed Flair \nEmbeddings with an F1-score over 0.7. The COR category showed a decent precision \nof 0.88 with Flair Embeddings but a low recall of 0.58 which resulted in a low F1-Score \n(0.7)14.\nTraining with corpus No.2 showed a significant improvement in training accuracy \n(Fig.  5B). Overall, Flair Embeddings was more accurate than other training algorithms, \nalthough training with TARS showed better results for the FUND category. The Trans-\nformers showed the worst results during training.\nAdditionally, a zero-shot approach was tested for the TARS model on corpus no.1. The \nmodel was able to successfully recognize individuals, but struggled with other categories, \nas Table 4 demonstrates. The total accuracy of the model comprises 0.23.\nExperiment 2\nOur first hypothesis to explain the pure model performance for the FUND, COR, MISC, \nand UNI categories is their semantic proximity that prevents successful recognition. \nEntities of these categories are often used in the same context. To examine this hypoth-\nesis, we conducted an experiment using Flair Embeddings with the dataset contain-\ning three types of entities: IND, GRNB, and ORG. The MISC category was excluded \nfrom the training, as one of the aims of the present research is to extract information \nabout acknowledged entities, and the MISC category contains only additional informa-\ntion. The new ORG category was established, which includes a combination of entities \nfrom the FUND, COR, and UNI categories. The training was performed with exactly \nthe same parameters as training with the Flair Embeddings model in Experiment 1 \n(Sect. 4.1).Table 4  F1-scores of the training with three algorithms for each label class with Corpus No. 1\nAlgorithm FUND GRNB IND UNI COR MISC accuracy\nTARS (zero-shot) 0.23 0.33 0.86 0 0 0 0.23\nTARS (few-shot) 0.32 0.76 0.86 0 0 0 0.35\nFlair embeddings 0.42 0.61 0.80 0 0 0 0.35\nTransformers 0.30 0.40 0 0 0 0 0.15\n14 Accuracy metrics by type of entity and total accuracy for all experiments can be found in Appendixes   A \nand BScientometrics \n1 3\nThe UNI and COR categories, though, have distinct patterns. In this case, the low \nperformance of the models for the COR and UNI categories could be explained by the \nsmall size of the training sample that contains these categories (see Fig.  2). Thus, the \nmodel was not able to identify patterns because of the lack of data.\nSecondly, low results for FUND, COR, MISC, and UNI categories might also lie in \nthe nature of the miscellaneous category, as some entities that fall into this category are \nsemantically very close to the FUND and COR categories. As a result, training without \na MISC category might potentially show better performance. To examine this hypoth-\nesis, we conducted training with Flair Embeddings with a dataset excluding the MISC \ncategory, i.e., with five entity types. Training results are shown in Fig. 6 A.\nAdditionally, the problem might lie in the nature of the training algorithms that \nwere used. On the one hand, Flair developers claimed Transformers to be the most effi-\ncient algorithm (Schweter & Akbik, 2020). On the other, the stacked embeddings are \nan important feature of the Flair tool, as a combination of different embeddings might \nbring better results than their separate uses (Akbik et  al., 2019). Thus, the combina-\ntion of the Transformer embeddings model with the contextual string embeddings might \nimprove the model performance. Thus, for the third additional training, we combined \ncontextual string embeddings with FLERT parameters.\nFig. 6  The results of Experiment 2. A–C comprise diagrams with the F1-scores of the training with three \nalgorithms for each label class. D Represents the total accuracy of the training algorithms Scientometrics\n1 3\nResults\nResults of the training are represented in Fig.  6. During the training with three types of \nentities (Fig.  6B) IND and GRNB still achieved high F1-scores of 0.96 (IND) and 0.95 \n(GRNB). Nevertheless, ORG gained only an F1-score of 0.64, which is worse than the \nprevious results with six entity types. The results of the training with five types of entities \nwere quite similar to those achieved during the training with six types of entities. FUND \nand UNI categories showed a small improvement in precision, recall, and F1 score com-\npared to training with 6 types of entities with Flair Embeddings. At the same time, the \nperformance of the COR category deteriorated noticeably (0.6 vs. the previous 0.7). The \nimprovement in overall accuracy (Fig.  6D) (0.80 vs. the previous 0.77) could be explained \nby the fact that the MISC category was not present in this training and could not affect \noverall accuracy with its low F1-score.\nAs Fig.  6C demonstrates, training with Flair Embeddings and RoBERTa showed no \nimprovements compared to the results of the primary training with Transformers and worse \nperformance compared with Flair Embeddings. As in Experiment 1, the COR category \nachieved high precision but low recall, resulting in a low F1-score (0.67). For some catego-\nries (COR and GRNB) Flair Embeddings combined with RoBERTa performed better than \nTransformers but still worse than Flair Embeddings.\nExperiment 3\nThe results of experiment 2 showed that altering the training parameters and decreasing the \nnumber of entity classes does not improve the model accuracy. We assume that increasing \nthe size of the training corpus would improve the performance of entities with low recogni-\ntion accuracy. Therefore, for this experiment, we designed two corpora with an increased \nnumber of acknowledged entities.\nAs the Flair Embeddings algorithm trained with Corpus No.2 showed the best perfor -\nmance, it was of interest if the increased training data will outperform its accuracy score. \nTraining in Experiments 1 and 2 was carried out using Flair version 0.9. As Flair recently \nupdated to version 0.11, we used this newest version for the following training. The training \nwas carried out with exactly the same parameters as the training with the Flair Embeddings \nFig. 7  The results of Experiment 3. A Comprises diagrams with the F1-scores of training with three cor -\npora for each label class. B Represents the total accuracy of the trainingScientometrics \n1 3\nmodel in Experiment 1 (Sect. 3.1). To achieve comparable results we also retrained, for \nnow, the best model (Flair Embeddings with Corpus No.2) with the Flair 0.11.\nResults\nResults of the training are represented in Fig.  7. Retraining of the original model with the \nFlair 0.11 Fig.  7-B showed slightly better performance (0.79 vs. 0.77) than training with \nversion 0.9. In general, no huge differences in accuracy were found during training with \nextended corpora.\nOverall, the best F1-Score for the FUND category (0.77) was reached with the TARS \nalgorithm and corpus No.2. COR gained the best accuracy (0.7) with Flair Embeddings \nand corpus No.2 using Flair version 0.9. The GRNB category showed the best perfor -\nmance (0.96) with Flair Embeddings trained on the corpus with five types of entities \n(Flair Embeddings 5 Ent). The best F1-Score of the IND category was achieved with \nFlair Embeddings trained on corpus No.2 with Flair version 0.11. MISC performed the \nbest (0.66) with Flair Embeddings trained on Corpus No.4 with Flair version 0.11. The \nbest accuracy of the UNI category was achieved with Flair Embeddings trained on corpus \nNo.3 with Flair version 0.11. In general, the best overall accuracy of 0.79 (for six entity \ntypes) had the Flair Embeddings model trained on corpus No.2 with Flair version 0.11.\nDiscussion\nAs expected, Experiment 1 showed a large improvement in accuracy for all algorithms \nwhen the size of a training corpus was increased from 49 to 654 sentences. However, fur -\nther enlargement of the corpus (in Experiment 3) did not make any progress. Some types \nof entity, such as IND and GRNB, showed great performance (GRNB with an F1-Score of \n0.96 or IND with 0.98) with the small training samples, i.e., 354 entities from the GRNB \ncategory or 439 entities from the IND category. At the same time, training with a sample of \n1322 labelled funding organisations achieved an F1-Score of only 0.75.\nThe TARS model is designed to perform NER with small or no training data. In \nexperiment 1, TARS without training data was able to extract individuals with quite high \naccuracy (F-1 score of 0.86). TARS trained with the small corpus (No. 1) did not show \nimprovement in the F-1 score of individuals, but greatly improved the F-1 score of the \nGRNB category. For other entity types, this model showed extremely weak results. It was \nexpected that training with Flair Embeddings and Transformers will not bring high recog-\nnition accuracy with corpus No.1, however, interesting results can be observed. Thus, Flair \nEmbeddings showed decent accuracy of 0.8 for individuals with the small training dataset.\nThe imbalance in the performance of different types of entities can be explained by the \nnature of the data, on which the original models were trained. Thus, Flair Embeddings were \ntrained on the 1-billion words English corpus (Chelba et al., 2013). RoBERTa was pre-trained \non the combination of five datasets containing news articles, blog entries, books, and Wikipe-\ndia articles. TARS was mainly pre-trained on datasets for text classification. Thus, the models \nused were not trained on domain-specific data. This can also explain the pure Transformers \nand TARS performance. The higher accuracy for the individuals category in the training with \nTARS can be explained by the fact, that the word ’person’ is semantically more straightforward \nthan other categories. The same could be applied to grant numbers. Furthermore, grant num-\nbers generally have similar patterns, which can be applied to all entities of this type, that can  Scientometrics\n1 3\nexplain a rapid improvement in F-1 score between zero-shot and few-shot models. Moreover, \nIND and GRNB categories showed better performance for other algorithms too, which could \nlie in the structure of these entities: names of individuals and grant numbers usually have undi-\nversified patterns and in acknowledgement texts are used in a small variety of contexts. At the \nsame time, other entity types, such as funding organisations and universities could have similar \npatterns and could be used in the same context. In some cases, even for human annotators, it \nis impossible to distinguish between university, funding body and corporation without back -\nground knowledge about the entity.\nPrevious works showed improvements in downstream tasks using embedding models \nfine-tuned for the domain used (Shen et al., 2022; Beltagy et al.., 2019). Therefore, fine-\ntuning the general language model on the sample of acknowledgment texts could improve \nthe performance of the NER model for acknowledgment texts. We are planning to fine-tune \nBERT and Flair Embeddings (contextual string embeddings) on a sample of approx. 5 mil-\nlion acknowledgment texts from WoS and evaluate the performance of the NER models.\nThe results of Experiment 2 generally did not show an improvement in accuracy. On the \ncontrary, training with the three entity types deteriorated the model performance. Train-\ning without the MISC category did not show significant performance progress either. \nMoreover, further analysis of acknowledged entities showed that the miscellaneous cate-\ngory contained very inhomogeneous and partly irrelevant data, making the analysis more \ncomplicated (Smirnova & Mayr, 2023). Therefore, we assume that the model would make \nbetter predictions if the number of entity types is expanded and miscellaneous categories \nexcluded, i.e., the MISC category could be split into the following categories: names of \nprojects, names of conferences, names of software and dataset. Different subcategories \ncould also be distinguished in the FUND category.\nCorpora No.2 and No.3 contain the same number of MISC and COR entities15, while \nin corpus 4 number of occurrences of MISC and COR entities is higher. For MISC and \nCOR, accuracy slightly increased with corpus 4, therefore we assume that the extraction \naccuracy for these entities will increase with the increase of the training data. The situa-\ntion is different for funding organizations and universities. The number of UNI and FUND \nentities increased evenly from corpus No.1 to corpus No.4. Nevertheless, the best result for \nthe UNI category was achieved with corpus No.3. The poor performance of corpus No.4 \ncould be explained by the inclusion of Indian funders. Thus, the names of many Indian \nfunders are very similar to the entities which usually fall into the UNI category, e.g., the \nDepartment of Science and Technology or the Department of Biotechnology. This pattern \nis more common to the entities which fall into the UNI category. Therefore, that might \nmake the exact extraction of UNI and FUND entities more confusing. Moreover, many \nIndian Universities contain the name of individuals, e.g., Rajiv Gandhi University, which \ncan cause confusion of the UNI category with the IND category. Generally, no improve-\nment in increasing the size of the corpus for the FUND category can be explained by the \nambiguous nature of the entities which fall into the FUND category and their semantical \nproximity with other types of entities. Analysis of the extracted entities showed that many \nentities were extracted correctly, but were assigned to the wrong category (Smirnova & \nMayr, 2023). Therefore, an additional classification algorithm applied to extracted entities \ncould improve the model’s performance.\n15 These differences in entity distribution are caused by the peculiarities of acknowledgement information \nstored in WoS. As only acknowledgements with indexed funding information are stored in the database, it \nwas difficult to find an adequate number of acknowledged entities of other typesScientometrics \n1 3\nConclusion\nIn this paper, we evaluated different embedding models for the task of automatic extraction \nand classification of acknowledged entities from acknowledgment texts16. The annotation \nof the training corpora was the most challenging and time-consuming task of all data prep-\naration procedures. Therefore, a semi-automated approach was used to help significantly \naccelerate the procedure.\nThe study’s main limitations were its small size and just one annotator of the training \ncorpora. Additionally, we used acknowledgments texts collected in WoS. WoS only stores \nacknowledgments containing funding information, therefore there was a lack of other types \nof entities, such as corporations or universities in the training data.\nIn the present paper, we aimed to answer three questions. Thus, regarding research ques-\ntion 1, the few-shot and zero-shot models showed very low total recognition accuracy. At \nthe same time, it was observed that some entities performed better than others with all \nalgorithms and training corpora. Thus, individuals gained a good F1-score over 0.8 with \nzero-shot and few-shot models, as also with Flair embeddings trained with the smallest \ncorpus. With the enlargement of the training corpora, the performance of the IND category \nalso increased and achieved an F1-score over 0.9. The GRNB category showed an adequate \nF-1 score of 0.76 with the few-shot algorithm trained with the smallest corpus, following \ntraining with corpus No.2 boosts the F-1 score to over 0.9. Therefore, few-shot and zero-\nshot approaches were not able to identify all the defined acknowledged entity classes.\nWith respect to research question 2, Flair Embeddings showed the best accuracy in \ntraining with corpus No.2 (and version 0.11) and the fastest training time compared to the \nother models; thus, it is recommended to further use the Flair Embeddings model for the \nrecognition of acknowledged entities.\nExploring research question 3 we observed, that the expansion of the size of a training \ncorpus from very small (corpus No.1) to medium size (corpus No.2) massively increased the \naccuracy of all training algorithms. The best-performing model (Flair Embedding) was further \nretrained with the two bigger corpora, but the following expansion of the training corpus did \nnot bring further improvement. Moreover, the performance of the model slightly deteriorated.\nAcknowledgements The original work was funded by the German Center for Higher Education Research \nand Science Studies (DZHW) via the project “Mining Acknowledgement Texts in Web of Science \n(MinAck)”17. Access to the WoS data was granted via the Competence Centre for Bibliometrics18. Data \naccess was funded by BMBF (Federal Ministry of Education and Research, Germany) under grant number \n01PQ17001. Nina Smirnova received funding from the German Research Foundation (DFG) via the project \n“POLLUX”19. The present paper is an extended version of the paper “Evaluation of Embedding Models for \nAutomatic Extraction and Classification of Acknowledged Entities in Scientific Documents” (Smirnova & \nMayr, 2022) presented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Sci-\nentific Documents (EEKE2022).\nAppendix A: Accuracy metrics by type of entity (label) for all \nexperiments\nSee Table 5.\n16 The best model can be tested  at https:// huggi ngface. co/ kalaw inka/ flair- ner- ackno  wledg ments\n17 https:// kalaw inka. github. io/ minack/.\n18 https:// www. bibli ometr  ie. info/ en/ index. php? id= home.\n19 https:// www. pollux- fid. de/ about. Scientometrics\n1 3\nTable 5  Accuracy metrics by type of entity (label) for all experiments\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings No.1 9 IND 0.7692 0.8333 0.8000 12 1\nFlair embeddings No.1 9 GRNB 0.5385 0.7000 0.6087 10 1\nFlair embeddings No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nFlair embeddings No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nFlair embeddings No.1 9 COR 0.0000 0.0000 0.0000 1 1\nFlair embeddings No.1 9 FUND 0.4000 0.4444 0.4211 18 1\nFlair embeddings No.2 9 FUND 0.6524 0.7771 0.7093 157 1\nFlair embeddings No.2 9 IND 0.9764 0.9831 0.9797 295 1\nFlair embeddings No.2 9 GRNB 0.9398 0.9750 0.9571 160 1\nFlair embeddings No.2 9 UNI 0.7527 0.7071 0.7292 99 1\nFlair embeddings No.2 9 MISC 0.6420 0.6341 0.6380 82 1\nFlair embeddings No.2 9 COR 0.8750 0.5833 0.7000 12 1\nTARS (pretrained) No.1 9 IND 1.0000 0.7500 0.8571 12 1\nTARS (pretrained) No.1 9 GRNB 0.7273 0.8000 0.7619 10 1\nTARS (pretrained) No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTARS (pretrained) No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTARS (pretrained) No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTARS (pretrained) No.1 9 FUND 0.3158 0.3333 0.3243 18 1\nTARS (pretrained) No.2 9 FUND 0.7257 0.8089 0.7651 157 1\nTARS (pretrained) No.2 9 IND 0.9281 0.8746 0.9005 295 1\nTARS (pretrained) No.2 9 GRNB 0.8895 0.9563 0.9217 160 1\nTARS (pretrained) No.2 9 UNI 0.7407 0.6061 0.6667 99 1\nTARS (pretrained) No.2 9 MISC 0.6719 0.5244 0.5890 82 1\nTARS (pretrained) No.2 9 COR 0.5000 0.5833 0.5385 12 1\nTransformers No.1 9 GRNB 0.3000 0.6000 0.4000 10 1\nTransformers No.1 9 IND 0.0000 0.0000 0.0000 12 1\nTransformers No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTransformers No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTransformers No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTransformers No.1 9 FUND 0.2414 0.3889 0.2979 18 1\nTransformers No.2 9 FUND 0.6211 0.7516 0.6801 157 1\nTransformers No.2 9 IND 0.9346 0.9695 0.9517 295 1\nTransformers No.2 9 GRNB 0.8704 0.8812 0.8758 160 1\nTransformers No.2 9 UNI 0.6476 0.6869 0.6667 99 1\nTransformers No.2 9 MISC 0.4767 0.5000 0.4881 82 1\nTransformers No.2 9 COR 0.7500 0.5000 0.6000 12 1\nFlair embeddings (3 Ent) No.2 9 IND 0.9577 0.9703 0.9639 303 2\nFlair embeddings (3 Ent) No.2 9 ORG 0.6400 0.6154 0.6275 208 2\nFlair embeddings (3 Ent) No.2 9 GRNB 0.9286 0.9750 0.9512 160 2\nFlair embeddings (5 Ent) No.2 9 IND 0.9764 0.9797 0.9780 295 2\nFlair embeddings (5 Ent) No.2 9 GRNB 0.9345 0.9812 0.9573 160 2\nFlair embeddings (5 Ent) No.2 9 UNI 0.7802 0.7172 0.7474 99 2\nFlair embeddings (5 Ent) No.2 9 COR 0.7500 0.5000 0.6000 12 2\nFlair embeddings (5 Ent) No.2 9 FUND 0.6722 0.7707 0.7181 157 2Scientometrics \n1 3\nAppendix B: Overall accuracy for all experiments\nSee Table 6.Rows are sorted by experiment number and algorithmTable 5  (continued)\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings (RoB-\nERTa)No.2 9 IND 0.9206 0.9831 0.9508 295 2\nFlair embeddings (RoB-\nERTa)No.2 9 GRNB 0.8896 0.9062 0.8978 160 2\nFlair embeddings (RoB-\nERTa)No.2 9 UNI 0.5963 0.6566 0.6250 99 2\nFlair embeddings (RoB-\nERTa)No.2 9 MISC 0.4135 0.5244 0.4624 82 2\nFlair embeddings (RoB-\nERTa)No.2 9 COR 1.0000 0.5000 0.6667 12 2\nFlair embeddings (RoB-\nERTa)No.2 9 FUND 0.6096 0.7261 0.6628 157 2\nFlair embeddings No.2 11 GRNB 0.9345 0.9812 0.9573 160 3\nFlair embeddings No.2 11 IND 0.9797 0.9831 0.9814 295 3\nFlair embeddings No.2 11 FUND 0.7027 0.8280 0.7602 157 3\nFlair embeddings No.2 11 UNI 0.7684 0.7374 0.7526 99 3\nFlair embeddings No.2 11 MISC 0.6543 0.6463 0.6503 82 3\nFlair embeddings No.2 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 UNI 0.8000 0.7273 0.7619 99 3\nFlair embeddings No.3 11 IND 0.9731 0.9797 0.9764 295 3\nFlair embeddings No.3 11 GRNB 0.9281 0.9688 0.9480 160 3\nFlair embeddings No.3 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 MISC 0.6571 0.5610 0.6053 82 3\nFlair embeddings No.3 11 FUND 0.6757 0.7962 0.7310 157 3\nFlair embeddings No.4 11 MISC 0.7424 0.5976 0.6622 82 3\nFlair embeddings No.4 11 COR 0.8571 0.5000 0.6316 12 3\nFlair embeddings No.4 11 UNI 0.7753 0.6970 0.7340 99 3\nFlair embeddings No.4 11 IND 0.9698 0.9797 0.9747 295 3\nFlair embeddings No.4 11 FUND 0.6823 0.8344 0.7507 157 3\nFlair embeddings No.4 11 GRNB 0.9162 0.9563 0.9358 160 3 Scientometrics\n1 3\nFunding Open access funding enabled and organized by Projekt DEAL.\nDeclarations \n Conflict of interest Philipp Mayr, the co-author of this paper, has a conflict of interest because he serves on \nthe editorial board of the journal Scientometrics. In addition, he is a co-guest editor of the special issue on \n\"Extraction and Evaluation of Knowledge Entities from Scientific Documents\". He declares that he has noth-\ning to do with the decision about this paper submission.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAkbik, A., Bergmann, T., Blythe, D., Rasul, K., Schweter, S., & Vollgraf, R. 2019. FLAIR: An Easy-to-Use \nFramework for State-of-the-Art NLP. Minneapolis, Minnesota (pp. 54–59). Association for Computa-\ntional Linguistics.\nAkbik, A., Blythe, D., & Vollgraf, R. (2018). Contextual string embeddings for sequence labeling. In 2018, \n27th International Conference on Computational Linguistics (pp. 1638–1649).\nAlexandera, D. & Vries, A. P. (2021). This research is funded by...”: Named Entity Recognition of financial \ninformation in research papers. In BIR 2021: 11th International Workshop on Bibliometric-enhanced \nInformation Retrieval at ECIR (pp. 102–110).\nBeltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. \nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing \nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)  \n(pp. 3613–3618). Association for Computational Linguistics.Table 6  Overall accuracy for all experiments\nRows are sorted by experiment number and algorithmAlgorithm Corpus Version Accuracy Experiment\nFlair embeddings No.2 9 0.7702 1\nFlair embeddings No.1 9 0.3472 1\nTARS (pretrained) No.2 9 0.7113 1\nTARS (pretrained) No.1 9 0.3485 1\nTransformers No.2 9 0.6783 1\nTransformers No.1 9 0.1477 1\nFlair Embeddings (3 Entity Types) No.2 9 0.7536 2\nFlair embeddings (5 Entity Types) No.2 9 0.7990 2\nFlair embeddings + RoBERTa No.2 9 0.6697 2\nFlair Embeddings No.2 11 0.7869 3\nFlair embeddings No.4 11 0.7814 3\nFlair embeddings No.3 11 0.7691 3Scientometrics \n1 3\nBorst, T., Mielck, J., Nannt, M., & Riese, W. (2022). Extracting funder information from scien-\ntific papers—Experiences with question answering. In Silvello, G., O.  Corcho, P.  Manghi, G.M. \nDi Nunzio, K. Golub, N. Ferro, and A. Poggi (Eds.),Linking theory and practice of digital libraries  \n(Vol. 13541, pp. 289–296). Springer International Publishing. Series Title: Lecture Notes in Com-\nputer Science. https:// doi. org/ 10. 1007/ 978-3- 031- 16802-4_ 24.\nChelba, C., T.  Mikolov, M.  Schuster, Q.  Ge, T.  Brants, P.  Koehn, & Robinson, T. (2013). One Bil-\nlion Word Benchmark for Measuring Progress in Statistical Language Modeling. 10.48550/\nARXIV.1312.3005 .\nChen, H., Song, X., Jin, Q., & Wang, X. (2022). Network dynamics in university-industry collaboration: \nA collaboration-knowledge dual-layer network perspective. Scientometrics, 127(11), 6637–6660. \nhttps:// doi. org/ 10. 1007/ s11192- 022- 04330-9\nCronin, B. (1995). The Scholar’s courtesy: The role of acknowledgement in the primary communication \nprocess. Taylor Graham.\nCronin, B., & Weaver, S. (1995). The praxis of acknowledgement: From bibliometrics to influmetrics. \nRevista Española de Documentación Científica, 18(2), 172.\nDevlin, J., Chang, M.W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional \ntransformers for language understanding. 10.48550/ARXIV.1810.04805 .\nDiaz-Faes, A. A., & Bordons, M. (2017). Making visible the invisible through the analysis of acknowl-\nedgements in the humanities. Aslib Journal of Information Management, 69(5), 576–590. https://  \ndoi. org/ 10. 1108/ AJIM- 01- 2017- 0008\nDoehne, M., & Herfeld, C. (2023). How academic opinion leaders shape scientific ideas: an acknowl-\nedgment analysis., 128(4), 2507–2533. https:// doi. org/ 10. 1007/ s11192- 022- 04623-z\nDzieżyc, M., & Kazienko, P. (2022). Effectiveness of research grants funded by European research coun-\ncil and polish national science centre. Journal of Informetrics, 16(1), 101243. https:// doi. org/ 10.  \n1016/j. joi. 2021. 101243\nEftimov, T., Koroušić Seljak, B., & Korošec, P. (2017). A rule-based named-entity recognition \nmethod for knowledge extraction of evidence-based dietary recommendations. PLoS ONE, 12(6), \ne0179488. https:// doi. org/ 10. 1371/ journ al. pone. 01794 88\nEtzioni, O., Cafarella, M., Downey, D., Popescu, A. M., Shaked, T., Soderland, S., Weld, D. S., & Yates, \nA. (2005). Unsupervised named-entity extraction from the web: An experimental study. Artificial \nIntelligence, 165(1), 91–134. https:// doi. org/ 10. 1016/j. artint. 2005. 03. 001\nFinkel, J.R., Grenager, T., & Manning, C. (2005). Incorporating non-local information into information \nextraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL’05), Ann Arbor, Michigan (pp. 363–370). Association for \nComputational Linguistics.\nGiles, C. L., & Councill, I. G. (2004). Who gets acknowledged: Measuring scientific contributions \nthrough automatic acknowledgment indexing. Proceedings of the National Academy of Sciences,  \n101(51), 17599–17604. https:// doi. org/ 10. 1073/ pnas. 04077 43101\nHalder, K., Akbik, A., Krapac, J., & Vollgraf, R. (2020). Task-Aware Representation of Sentences for \nGeneric Text Classification. In Proceedings of the 28th International Conference on Computational \nLinguistics, Barcelona, Spain (Online) (pp. 3202–3213). International Committee on Computa-\ntional Linguistics.\nHubbard, D., Laddusaw, S., Tan, Q., & Hu, X. (2022). Analysis of acknowledgments of libraries in the \njournal literature using machine learning. Proceedings of the Association for Information Science \nand Technology, 59(1), 709–711. https:// doi. org/ 10. 1002/ pra2. 698\nIovine, A., Fang, A., Fetahu, B., Rokhlenko, O., & Malmasi, S. (2022). CycleNER: An unsupervised \ntraining approach for named entity recognition. In Proceedings of the ACM Web Conference 2022  \n(pp. 2916–2924). ACM.\nJiang, L., Kang, X., Huang, S., & Yang, B. (2022). A refinement strategy for identification of scientific \nsoftware from bioinformatics publications. Scientometrics, 127(6), 3293–3316. https:// doi. org/ 10.  \n1007/ s11192- 022- 04381-y\nKassirer, J. P., & Angell, M. (1991). On authorship and acknowledgments. The New England Journal of \nMedicine, 325(21), 1510–1512. https:// doi. org/ 10. 1056/ NEJM1 99111 21325 2112\nKayal, S., Afzal, Z., Tsatsaronis, G., Katrenko, S., Coupet, P., Doornenbal, M., & Gregory, M. (2017). \nTagging funding agencies and grants in scientific articles using sequential learning models. In \nBioNLP 2017, Vancouver, Canada (pp. 216–221). Association for Computational Linguistics.\nKenekayoro, P. (2018). Identifying named entities in academic biographies with supervised learning. \nScientometrics, 116(2), 751–765. https:// doi. org/ 10. 1007/ s11192- 018- 2797-4\nKingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. 10.48550/\nARXIV.1412.6980 . Scientometrics\n1 3\nKusumegi, K., & Sano, Y. (2022). Dataset of identified scholars mentioned in acknowledgement state-\nments. Scientific Data, 9(1), 461. https:// doi. org/ 10. 1038/ s41597- 022- 01585-y\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoy -\nanov, V. (2019). RoBERTa: A robustly optimized BERT pretraining approach. arXiv: 1907. 11692  \n[cs] .\nMackintosh, K. (1972). Acknowledgements patterns in sociology. Ph. D. thesis, University of Oregon.\nMccain, K. (2017). Beyond Garfield’s citation index: An assessment of some issues in building a per -\nsonal name acknowledgments index. Scientometrics. https:// doi. org/ 10. 1007/ s11192- 017- 2598-1\nMcCain, K. W. (1991). Communication, competition, and secrecy: The production and dissemination of \nresearch-related information in genetics. Science, Technology, & Human Values, 16(4), 491–516. \nhttps:// doi. org/ 10. 1177/ 01622 43991 01600 404\nMejia, C., & Kajikawa, Y. (2018). Using acknowledgement data to characterize funding organizations \nby the types of research sponsored: the case of robotics research. Scientometrics, 114(3), 883–904. \nhttps:// doi. org/ 10. 1007/ s11192- 017- 2617-2\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, \nN., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkur -\nthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: An imperative style, high-\nperformance deep learning library. arXiv: 1912. 01703 [cs, stat].\nPaul-Hus, A., & Desrochers, N. (2019). Acknowledgements are not just thank you notes: A qualitative \nanalysis of acknowledgements content in scientific articles and reviews published in 2015. PLoS \nONE, 14, e0226727. https:// doi. org/ 10. 1371/ journ al. pone. 02267 27\nPaul-Hus, A., Díaz-Faes, A., Sainte-Marie, M., Desrochers, N., Costas, R., & Larivière, V. (2017). \nBeyond funding: Acknowledgement patterns in biomedical, natural and social sciences. PLoS ONE,  \n12, e0185578. https:// doi. org/ 10. 1371/ journ al. pone. 01855 78\nPennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors for Word Representation. \nIn Empirical Methods in Natural Language Processing (EMNLP) (pp. 1532–1543).\nPustejovsky, J., & Stubbs, A. (2012). Natural language annotation for machine learning. O’Reilly \nMedia Inc.\nRose, M., & Georg, C. P. (2021). What 5,000 acknowledgements tell us about informal collaboration \nin financial economics. Research Policy, 50, 104236. https:// doi. org/ 10. 1016/j. respol. 2021. 104236\nSang, T. K., & E. F., & De Meulder, F. (2003). Introduction to the CoNLL-2003 shared task: Language-\nindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Lan-\nguage Learning at HLT-NAACL (pp. 142–147).\nSchweter, S., & Akbik, A. (2020). FLERT: Document-level features for named entity recognition. ArXiv. \n10.48550/arXiv.2011.06993 .\nShen, S., Liu, J., Lin, L., Huang, Y., Zhang, L., Liu, C., Feng, Y., & Wang, D. (2022). SsciBERT: A \npre-trained language model for social science texts. Scientometrics. https:// doi. org/ 10. 1007/  \ns11192- 022- 04602-4\nSingh, V. K., Singh, P., Karmakar, M., Leta, J., & Mayr, P. (2021). The journal coverage of web of sci-\nence, scopus and dimensions: A comparative analysis. Scientometrics, 126(6), 5113–5142. https://  \ndoi. org/ 10. 1007/ s11192- 021- 03948-5\nSmirnova, N., & Mayr, P. (2022). Evaluation of embedding models for automatic extraction and classifi-\ncation of acknowledged entities in scientific documents. In 3rd Workshop on Extraction and Evalu-\nation of Knowledge Entities from Scientific Documents 2022 (EEKE 2022) (pp. 48–55). CEUR-WS.\norg.\nSmirnova, N., & Mayr, P. (2023). A comprehensive analysis of acknowledgement texts in web of sci-\nence: A case study on four scientific domains. Scientometrics, 1(128), 709–734. https:// doi. org/ 10.  \n1007/ s11192- 022- 04554-9\nSong, M., Kang, K. Y., Timakum, T., & Zhang, X. (2020). Examining influential factors for acknowl-\nedgements classification using supervised learning. PLoS ONE. https:// doi. org/ 10. 1371/ journ al.  \npone. 02289 28\nThomer, A. K., & Weber, N. M. (2014). Using named entity recognition as a classification heuristic. In \niConference 2014 Proceedings (pp. 1133 – 1138). iSchools.\nWang, J., & Shapira, P. (2011). Funding acknowledgement analysis: An enhanced tool to investigate \nresearch sponsorship impacts: The case of nanotechnology. Scientometrics, 87(3), 563–586. https://  \ndoi. org/ 10. 1007/ s11192- 011- 0362-5\nYamada, I., Asai, A., Shindo, H., Takeda, H., & Matsumoto, Y. (2020). LUKE: Deep contextualized \nentity representations with entity-aware self-attention. In Proceedings of the 2020 Conference on \nEmpirical Methods in Natural Language Processing (EMNLP) (pp. 6442–6454). Association for \nComputational Linguistics.Scientometrics \n1 3\nYu, J., Bohnet, B., & Poesio, M. (2020). Named entity recognition as dependency parsing. 10.48550/\nARXIV.2005.07150.\nZhang, C., Mayr, P., Lu, W., & Zhang, Y. (2023). Guest editorial: Extraction and evaluation of knowledge \nentities in the age of artificial intelligence. Aslib Journal of Information Management, 75, 433–437. \nhttps:// doi. org/ 10. 1108/ AJIM- 05- 2023- 507",
        "type": "direct"
    },
    {
        "citation": "Kusumegi and Sano, 2020; Hubbard et al., 2022",
        "content": ".\nThe analysis of acknowledgments is particularly interesting as acknowledgments may \ngive an insight into aspects of the scientific community, such as reward systems (Dzieżyc \n& Kazienko, 2022), collaboration patterns, and hidden research trends (Giles & Councill, \n2004; Diaz-Faes & Bordons, 2017). From the linguistic point of view, acknowledgments \nare unstructured text data, which through automatic analysis poses research and methodo-\nlogical problems like data cleaning, choosing the proper tokenization method, and whether \nand how word embeddings may enhance their automatic analysis.\nTo our knowledge, previous works on automatic acknowledgment analysis were mostly \nconcerned with the extraction of funding organizations and grant numbers (Alexandera & \nVries, 2021; Kayal et  al., 2017; Borst et  al., 2022) or classification of acknowledgment \ntexts . Furthermore, large bibliographic databases \nsuch as Web of Science (WoS)1 and Scopus selectively index only funding information, \ni.e., names of funding organizations and grant identification numbers. Consequently, we \nwant to extend that to other types of acknowledged entities: individuals, universities, cor -\nporations, and other miscellaneous information. Analysis of the acknowledged individuals \nprovides insight into informal scientific collaboration (Rose & Georg, 2021; Kusumegi & \nSano, 2022). Acknowledged universities and corporations reveal interactions and knowl-\nedge exchange between industry and universities (Chen et  al., 2022). Entities from the \nmiscellaneous category include other information like project names, which could uncover \ninternational scientific collaborations.\nThe state-of-the-art named entity recognition (NER) models showed a great perfor -\nmance on the CoNLL-2003 dataset (Akbik et al., 2018; Devlin et al., 2018; Yamada et al., \n2020; Yu et  al., 2020). CoNLL-2003 corpus (Sang et  al., 2003) is a benchmark dataset \nfor language-independent named entity recognition, i.e., designed to train and evaluate \nNER models. English data for the corpus were taken from the Reuters corpus. The dataset \ncomprises four types of named entities: person, location, organisation, and miscellaneous. \nHowever, specific domains require specifically labelled training data. The development of a \ntraining dataset for the specific domain is an expensive and time-consuming process since \nNER usually requires a quite large training corpus. Therefore, the objective of this paper is \nto evaluate the performance of existing embedding models for the task of automatic extrac-\ntion and classification of acknowledged entities from the acknowledgment text in scientific \npapers using small training datasets or without training data (zero-short approach).\n1 http:// wokin fo. com/ produ cts_ tools/ multi disci plina ry/ webof  scien ce/ fundi ngsea rch/.Scientometrics \n1 3\nThe present paper is an extended version of the article (Smirnova & Mayr, 2022)2 pre-\nsented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Sci-\nentific Documents (EEKE2022).3 Flair, an open-source natural language processing (NLP) \nframework (Akbik et al., 2019) is used in our study to create a tool for the extraction of \nacknowledged entities because this library is easily customizable. It offers the possibility \nof creating a customized Named Entity Recognition (NER) tagger, which can be used for \nprocessing and analyzing acknowledgment texts. Furthermore, Flair has shown better accu-\nracy for NER tasks using pre-trained datasets in comparison with many other open source \nNLP tools.4\nIn the first experiment (Sect.  4.1) we trained and implemented a NER task using three \ndefault Flair NER models with two differently-sized corpora.5 All the descriptions of the \nFlair framework features refer to the releases 0.9 and 0.11. The models were trained to rec-\nognize six types of acknowledged entities: funding agency, grant number, individuals, uni-\nversity, corporation, and miscellaneous. The model with the best accuracy can be applied \nfor the comprehensive analysis of the acknowledgment texts. In Experiments 2 and 3 we \nperformed additional training with altered training parameters or altered training corpora \n(Sects  4.2 and 4.3). Most of the previous works on acknowledgment analysis were limited \nby the manual evaluation of data and therefore by the amount of processed data (Giles \n& Councill, 2004; Paul-Hus et al., 2017; Paul-Hus & Desrochers, 2019; Mccain, 2017). \nFurthermore, Thomer and Weber (2014) argues that using named entities can benefit the \nprocess of manual document classification and evaluation of the data. Therefore, a model \nthat is capable of extracting and classification of different types of entities may potentially \nmake a significant contribution to the field of automated acknowledgment analysis.\nResearch questions\nIn this paper, we address the following research questions:\n• RQ1: Is the few-shot or zero-shot approach able to identify predefined acknowledged \nentity classes?\n• RQ2: Which of the Flair default NER models is more suitable for the defined task of \nextraction and classification of acknowledged entities from scientific acknowledgments \nusing a small training dataset?\n• RQ3: How does the size of the training corpus affect the training accuracy for different \nNER models?\nCreating a training dataset for supervised learning is a time-consuming and expensive task, \nsince as a rule, such a model requires a reasonably large amount of training data. Annota-\ntion is a crucial moment, as wrongly annotated data will deteriorate training results. There-\nfore, more than one annotator is usually required to provide credible results. That is why \n2 In this paper we conducted an additional experiment (Experiment 3) with 2 new corpora (corpus Nos. 3 \nand 4).\n3 https:// eeke- works hop. github. io/ 2022/.\n4 https:// github. com/ flair NLP/ flair .\n5 The release 0.9 (https:// github. com/ flair NLP/ flair/ relea ses/ tag/ v0.9) was used in the experiments 1 and 2. \nExperiment 3 was performed using release 0.11 (https:// github. com/ flair NLP/ flair/ relea ses/ tag/ v0. 11). Scientometrics\n1 3\nit is of interest to test if the existing NER models can provide reasonable accuracy while \nusing small or no training data.\nBackground and related work\nResearch in the field of acknowledgments analysis has been carried out since the 1970s. \nThe first typology of acknowledgments was proposed by Mackintosh (1972) (as cited \nin Cronin, 1995) and comprised three categories: facilities, access to data, and help of indi-\nviduals. McCain (1991) distinguished five types of acknowledgements: research-related \ninformation, secondary access to research-related information, specific research-related \ncommunication, general peer communication, and technical or clerical support. Cronin and \nWeaver (1995) defined three broad categories: resource-, procedure- and concept-related. \nMejia and Kajikawa (2018) developed a four-level classification based on sponsored \nresearch field: change maker, incremental, breakthrough, and matured.\nDoehne and Herfeld (2023) distinguished acknowledgements from the perspective of \nappreciation of influential scholars and defined two axes: scientific influence and insti-\ntutional influence. Scientific influence refers to the productiveness and creativity of the \nresearcher, while institutional influence is associated with the scholar’s administrative posi-\ntion in the scientific community.\nWang and Shapira (2011) investigated the connection between research funding and \nthe development of science and technology using acknowledgments from articles from the \nfield of nanotechnology. Rose and Georg (2021) studied informal cooperation in academic \nresearch. The analysis revealed generational and gender differences in informal collabora-\ntion. The authors claim that information from informal collaboration networks makes bet-\nter predictions of the academic impact of researchers and articles than information from \nco-author networks. Mejia and Kajikawa (2018) argued that the classification of funders \ncould be useful in developing funding strategies for policymakers and funders.\nDoehne and Herfeld (2023) manually investigated acknowledgement sections of papers, \nwhich were published or preprinted in association with the Cowles Foundation between \nearly 1940 and 1970 to trace the influence of the informal social structure and academic \nleaders on the early acceptance of scientific innovations. Blockmodelling was applied to \nthe acknowledgement data. Their analysis showed that the adoption of scientific innova-\ntions was partly influenced by the social structure and by the scientific leaders at Cowles.\nRecent advances in NER\nNamed Entity Recognition (NER) is a form of NLP that aims to extract named entities \nfrom unstructured text and classify them into predefined categories. A named entity is a \nreal-world object that is important for understanding the text. Current approaches in NER \ncan be distinguished into supervised and unsupervised tasks. In a supervised NER a model \nis trained using a labelled dataset. This training dataset or corpus is usually split into sev -\neral datasets: training set, test set, and validation set. NER models require corpora with \nsemantic annotation, i.e., metadata about concepts attached to unstructured text data. The \nannotation process is crucial as insufficient or redundant metadata can slow down and bias \na learning process (Pustejovsky & Stubbs, 2012, Chapter 1).\nSupervised NER mainly relays on machine learning or deep learning methods. The \nstate-of-the-art models are based on deep recurrent models, convolution-based, or Scientometrics \n1 3\npre-trained transformer architectures (Iovine et  al., 2022). Thus, Akbik et  al. (2018) \nproposed a new character-based contextual string embeddings method. This approach \npasses a sequence of characters through the character-level language model to gener -\nate word-level embeddings. The model was pre-trained on large unlabeled corpora. The \ntraining was carried out using a character-based neural language model together with a \nBidirectional LSTM (BiLSTM) sequence-labelling model. This approach generates dif-\nferent embeddings for the same word depending on its context and showed good results \non downstream tasks such as NER. Devlin et al. (2018) presented BERT (Bidirectional \nEncoder Representations Transformers), a transformer-based language representa-\ntion model that models the representation of contextualized word embeddings. BERT \nshowed superior results on downstream tasks using different benchmarking datasets. \nLater, Liu et al. (2019) performed an optimization of the BERT model and introduced \nRoBERTa (Robustly Optimized BERT Pretraining Approach). RoBERTa was evaluated \non three benchmarks and demonstrated massive improvements over the reported BERT \nperformance.\nCurrently, several domain-specific models have been developed. Thus, Beltagy et  al.. \n(2019) released SciBERT a BERT-based language model pre-trained on a large number \nof unlabeled scientific articles from the computer science and biomedical domains. SciB-\nERT showed improvements over BERT on several downstream NLP tasks, including NER. \nRecently, Shen et al. (2022) introduced the SsciBERT, a language model based on BERT \nand pre-trained on abstracts published in the Social Science Citation Index (SSCI) journals. \nThe model showed good results in discipline classification and abstract structure-function \nrecognition in articles from the social sciences domain.\nUnsupervised methods are often based on lexicons or predefined rules. Thus, Etzioni \net al. (2005) uses lists of patterns and domain-specific rules to extract named entities. Efti-\nmov et al. (2017) developed a rule-based NER model to extract dietary information from \nscientific publications. Evaluation of the model performance showed good results. Opposed \nto previous unsupervised NER approaches, Iovine et al. (2022) proposed a cycle-consist-\nency approach for NER (CycleNER). CycleNER is unsupervised and does not require par -\nallel training data. The method showed 73% of supervised performance on CoNLL03.\nNER in scientometrics analysis\nNamed entities are widely used in scientometrics analysis. Thus, Kenekayoro (2018) devel-\noped a supervised method for the automatic extraction of named entities from academic \nbibliographies. The aim of the study was to create a database containing unified academic \ninformation about individuals to help in expert finding. A labeled training dataset was \ndeveloped using biographies extracted from ORCID.6 The authors tested several models \nfor NER. The Support Vector Machine classification algorithm (SVM) showed the best \nperformance.\nJiang et al. (2022) proposed a strategy for the identification of software in scientific bio-\ninformatics publications using the combination of SVM and CRF (Conditional Random \nField). Application of the method to the sample of articles from bioinformatics domains \nallowed them to observe interesting patterns in using software in scientific research.\nKusumegi and Sano (2022) analysed scholarly relationships by analysing acknowl-\nedged individuals from the acknowledgments statements from eight open-access journals. \n6 https:// orcid. org/. Scientometrics\n1 3\nIndividuals were extracted using the Stanford CoreNLP NER tagger. In the next steps, \nscholars were identified among the extracted individuals by mapping them to the Microsoft \nAcademic Graph (MAG).\nWe are aware of several works on automated information extraction from acknowledg-\nments. Giles and Councill (2004) developed an automated method for the extraction and \nanalysis of acknowledgment texts using regular expressions and SVM. Computer science \nresearch papers from the CiteSeer digital library were used as a data source. Extracted enti-\nties were analysed and manually assigned to the following four categories: funding agen-\ncies, corporations, universities, and individuals.\nThomer and Weber (2014) used the 4-class Stanford Entity Recognizer (Finkel et al., \n2005) to extract persons, locations, organizations, and miscellaneous entities from the col-\nlection of bioinformatics texts from PubMed Central’s Open Access corpus. The aim of \nthe study was to determine an approach to \"increase the speed of ... classification without \nsacrificing accuracy, nor reliability\" (Thomer & Weber, 2014, p. 1134).\nKayal et  al. (2017) introduced a method for extraction of funding organizations and \ngrants from acknowledgment texts using a combination of sequential learning models: con-\nditional random fields (CRF), hidden markov models (HMM), and maximum entropy mod-\nels (MaxEnt). The final model contained pooled outputs from the models used.\nAlexandera and Vries (2021) proposed AckNER, a tool for extracting financial informa-\ntion from the funding or acknowledgment section of a research article. AckNER works \nwith the use of dependency parse trees and regular expressions and is able to extract names \nof the organisations, projects, programs, and funds, as also numbers of contracts and \ngrants7.\nFollowing, Borst et  al. (2022) applied a question-answering (QA) based approach to \nidentify funding information in acknowledgments texts. This approach performs similarly \nto AckNER and requires a smaller set of training and test data.\nTable  1 shows an overview of works on NER in scientometrics. Overall, previous works \non the extraction of named entities from acknowledgements texts were mostly concerned \nwith the extraction of funding information, i.e., only names of funding bodies and grant \nnumbers, or extraction and linking of individuals. The special issue by Zhang et al. (2023) \nprovided a recent overview of current works in the extraction of knowledge entities.\nTo the best of our knowledge the work of Giles and Councill (2004) is the only \nattempt to extract and categorise multiple acknowledged entities. Nevertheless, entities \nwere extracted using the SVM algorithm but the classification of entities themselves \nwas produced manually, which limited the number of acknowledgement texts to be ana-\nlysed. Furthermore, as far as we know, there was no research done concerning the eval-\nuation of embedding models for extraction of information from acknowledgement texts \nand no tool for automatic extraction of different kinds of acknowledged entities was \ndeveloped.\n7 AckNER showed better performance as Flair, but is specifically designed to recognize two types of \nacknowledged entities (Alexandera & Vries, 2021), which was insufficient for the present project.Scientometrics \n1 3\nTable 1  Overview of works on NER in scientometrics\nPaper Area of application and aim of \nthe studyCorpus Entities Methods and tools\nGiles and Councill (2004) Extraction of acknowledged enti-\nties form acknowledgementsCiteSeer Funding agencies, Companies, \nEducational Institutions, \nIndividualsSVM for extracting entities and \ntheir manual classification\nThomer and Weber (2014) Using NER to improve classifica-\ntion of acknowledgementsPubMed Central’s Open Access Persons, locations, organizations, \nand miscellaneous4-class Stanford Entity Recognizer\nKayal et al. (2017) Extraction of funding information \nfrom acknowledgementsPubMed Central’s Open Access Funding bodies, grants CRF, HMM, MaxEnt\nKenekayoro (2018) Extraction of biography informa-\ntion from academic biographiesORCID Award, Location, Organization, \nPerson, Position, Specializa-\ntion, OthersSVM\nAlexandera and Vries (2021) Extraction of funding information \nfrom acknowledgementsTU Delft’s institutional repository Funding bodies, grants SpaCy dependency parser + regu-\nlar expressions\nJiang et al. (2022) Extraction of scientific software \nfrom scientific articles (full \ntexts) in bioinformaticsbioinformatics journals EnsembleSVMs-CRF\nBorst et al. (2022) Extraction of funding information \nfrom acknowledgementsEconStor Funding bodies, grants Haystack\nKusumegi and Sano (2022) Extraction and linking of \nacknowledged individuals from \nacknowledgementsPLOS Individuals Stanford CoreNLP NER tagger + \nMAG Scientometrics\n1 3\nMethod\nIn the present paper, different models for extraction and classification of acknowledged \nentities supported by the Flair NLP framework were evaluated. The choice of classification \nwas inspired by Giles and Councill’s (2004) classification: funding agencies (FUND), cor -\nporations (COR), universities (UNI), and individuals (IND). For our project, this classifica-\ntion was enhanced with the miscellaneous (MISC) and grant numbers (GRNB) categories. \nThe GRNB category was adopted from WoS funding information indexing. The entities \nin the miscellaneous category could provide useful information, but cannot be ascribed to \nother categories, e.g., names of projects and names of conferences. Figure  1 demonstrates \nan example of acknowledged entities of different types. To the best of our knowledge, Giles \nand Councill’s classification is the only existing classification of acknowledged entities and \ntherefore can be applied to the NER task. Other works on acknowledgment analysis were \nfocused on the classification of acknowledgment texts.\nThe Flair NLP framework\nFlair is an open-sourced NLP framework built on PyTorch (Paszke et  al., 2019), which \nis an open-source machine learning library. “The core idea of the framework is to pre-\nsent a simple, unified interface for conceptually very different types of word and document \nembeddings” (Akbik et  al., 2019, p.  54). Flair has three default training algorithms for \nNER which were used for the first experiment in the present research: a) NER Model with \nFlair Embeddings (later on Flair Embeddings) (Akbik et al., 2018), b) NER Model with \nTransformers (later on Transformers) (Schweter & Akbik, 2020), and c) Zero-shot NER \nwith TARS (later on TARS) (Halder et al., 2020) 8.\nThe Flair Embeddings model uses stacked embeddings, i.e., a combination of contex-\ntual string embeddings (Akbik et al., 2018) with a static embeddings model. This approach \nwill generate different embeddings for the same word depending on its context. Stacked \nembedding is an important Flair feature, as a combination of different embeddings might \nbring better results than their separate uses (Akbik et al., 2019).\nThe Transformers model or FLERT-extension (document-level features for NER) is a \nset of settings to perform a NER on the document level using fine-tuning and feature-based \nFig. 1  An example of acknowledged entities. Each entity type is marked with a distinct color\n8 New transformer models as SciBERT or SsciBERT were not evaluated in this study, as the objective of \nthe study is to evaluate the performance of the Flair default models.Scientometrics \n1 3\nLSTM-CRF with the multilingual XML-RoBERTa transformer model (Schweter & Akbik, \n2020).\nThe TARS (task-aware representation of sentences) is a transformer-based model, \nwhich allows performing training without any training data (zero-shot learning) or with a \nsmall dataset (few-short learning) (Halder et al., 2020). The TARS approach differs from \nthe traditional transfer learning approach in the way that the TARS model also considers \nsemantic information captured in the class labels themselves. For example, for analyzing \nacknowledgments, class labels like funding organization or university already carry seman-\ntic information.\nTraining data\nThe Web of Science (WoS) database was used to harvest the training data (funding \nacknowledgments).9 From 2008 on, WoS started indexing information about funders and \ngrants. WoS uses information from different funding reporting systems such as Research-\nfish,10 Medline11 and others. As WoS contains millions of metadata records (Singh et al., \n2021), the data chosen for the present study was restricted by year and scientific domain \n(for the corpora Nos. 1, 2, and 3) or additionally by the affiliation country (for corpus No.4). \nTo construct corpora Nos. 1-3 records from four different scientific domains published \nfrom 2014 to 2019 were considered: two domains from the social sciences (sociology and \neconomics) and oceanography and computer science. Different scientific domains were Table 2  Number of sentences/texts in the training corpora\nCorpus No. Training set (train) Test set (test) Validation set (dev) Total\n1 29/27 10/10 10/10 49/47\n2 339/282 165/150 150/136 654/441\n3 784/657 165/150 150/136 1099/816\n4 1148/885 165/150 150/136 1463/1044\nTable 3  Number of sentences/texts from each scientific domain in the training corpora\nCorpus No. Oceanography Economics Social Sciences Computer Science\n1 13/13 3/3 20/20 16/14\n2 127/75 92/58 351/234 173/129\n3 175/112 128/89 590/434 333/269\n9 The present research was conducted in scopes of two projects: MinAck (https:// kalaw inka. github. io/ \nminack/) and SEASON (https:// github. com/ kalaw inka/ season). Corpora Nos.1, 2, and 3 were created for the \nMinAck project and serve the purpose of a general evaluation of the impact of the size of the training cor -\npus on the model performance. Corpus No.4 was designed specifically for the SEASON project in the hope \nof improving the recognition of Indian funding information. The project SEASON aims to get insight into \nGerman-Indian scientific collaboration. Our other corpora mainly contain papers published by European \ninstitutions. That is why we enhance Corpus 4 with the papers published by Indian institutions.\n10 https:// resea rchfi sh. com/.\n11 https:// www. nlm. nih. gov/ bsd/ fundi ng_ suppo rt. html. Scientometrics\n1 3\nchosen since previous work on acknowledgment analysis revealed the relations between \nthe scientific domain and the types of acknowledged entities, i.e., acknowledged individu-\nals are more characteristic of theoretical and social-oriented domains. At the same time, \ninformation on technical and instrumental support is more common for the natural and \nlife sciences domains (Diaz-Faes & Bordons, 2017). Only the WoS record types “article” \nand “review” published in a scientific journal in English were selected; then 1000 distinct \nacknowledgments texts were randomly gathered from this sample for the training dataset. \nFurther different amounts of sentences containing acknowledged entities were distributed \ninto the differently-sized training corpora. Table  2 demonstrates the number of sentences \nin each set in the four corpora. We selected only sentences that contain an acknowledged \nentity, regardless of the scientific domain. Table  3 contains the number of sentences and \ntexts from each scientific domain in the training corpora.12 The same article can belong to \nseveral scientific domains, therefore, the number of sentences and texts in Tables  2 and 3 \ndoes not match. Corpus No.4 was designed in such a way that all the training data from the \nCorpus No.3 was enhanced with acknowledgments texts from the articles that have Indian \naffiliations regardless of scientific domain or publication date.\nPreliminary analysis of the WoS data showed that the indexing of WoS funding infor -\nmation has several issues. The WoS includes only acknowledgments containing funding \ninformation; therefore, not every WoS entry has an acknowledgment, individuals are not \nincluded, and indexed funding organizations are not divided into different entity types like \nuniversities, corporations, etc. Therefore, the existing indexing of funding organizations \nis incomplete. Furthermore, there is a disproportion between the occurrences of acknowl-\nedged entities of different types. Thus, the most frequent entity types in the dataset with \nthe training data are IND, FUND and GRNB, followed by UNI and MISC. COR is the \nFig. 2  The distribution of acknowledged entities in the training corpora\n12 Corpus No.4 is not in Table  2, because the corpus contains additional acknowledgment texts from arti-\ncles with Indian affiliations regardless of the scientific domain and therefore contains different scientific \ndomains.Scientometrics \n1 3\ncategory most underrepresented in the data set. Consequently, there are different amounts \nof entities of different types in the training corpora (as Fig.  2 demonstrates), which might \nhave influenced the training results. Training with the corpora Nos. 2, 3, and 4 was evalu-\nated on the same training and validation datasets to ensure plausible accuracy (Fig.  3-B). \nFig. 3  The distribution of acknowledged entities in the test and validation corpora\nFig. 4  Annotation flowchart Scientometrics\n1 3\nHowever, training with corpus No.1 was evaluated with the smaller test and validation sets, \nas corpus No.1 contains a smaller number of sentences (Fig. 3-A).\nData annotation\nThe training corpus was annotated with six types of entities. As WoS already contains \nsome indexed funding information, it was decided to develop a semi-automated approach \nfor data annotation (as Fig.  4 demonstrates) and use indexed information provided by WoS, \ntherefore, grant numbers were adopted from the WoS indexing unaltered.\nFlair has a pre-trained 4-class NER Flair model (CoNLL-03).13 The model can pre-\ndict four tags: PER (person name), LOC (location), ORG (organization name), and MISC \n(other names). As Flair showed adequate results in the extraction of names of individu-\nals, it was decided to apply the pre-trained 4-class CoNLL-03 Flair model to the training \ndataset. Entities that fell into the PER category were added as the IND annotation to the \ntraining corpus. Furthermore, we noticed that some funding information was partially cor -\nrectly extracted into the ORG and MISC categories. Therefore, WoS funding organization \nindexing and entities from the ORG and MISC categories were adopted and distinguished \nbetween three categories (FUND, COR, and UNI) using regular expressions. In addition, \nthe automatic classification of entities was manually examined and reviewed. Mismatched \ncategories, partially extracted entities, and not extracted entities were corrected. Acknowl-\nedged entities, which fall into the MISC category, were manually annotated by one annota-\ntor. In the miscellaneous category entities referring to names of the conferences and pro-\njects were included.\nExperiments\nIn the present paper, we evaluated three default Flair NER models with four differently-\nsized corpora. In total, we performed three experiments. In the first experiment, mod-\nels with the default parameter were evaluated using corpora Nos. 1 and 2. In the second \nexperiment, we evaluated Flair Embeddings and Transformers model with altered training \nparameters and corpus No.2. In the third experiment, the first experiment was replicated \nwith corpora Nos. 3 and 4.\nExperiment 1\nIn the first experiment, we tested the TARS model zero-shot and few-shot scenarios (with \ncorpus No. 1), as well as the performance of two default FLAIR models (Flair Embeddings \nand Transformers) with corpus No.2. Additionally, the performance of Flair Embeddings \nand Transformers models was tested with the corpus No.1 The training was conducted with \nthe recommended parameters for all algorithms, as Flair developers specifically ran vari-\nous tests to find the best hyperparameters for the default models. For the few-shot TARS, \nthe training was conducted with the small dataset (corpus No.1), and for Transformers and \nFlair Embeddings with a larger dataset (corpus No.2).\n13 https:// github. com/ flair NLP/ flairScientometrics \n1 3\nThe Flair Embeddings model was initiated as a combination of static and contextual \nstring embeddings. We applied GloVe (Pennington et  al., 2014) as a static word-level \nembedding model. Thus, in our case, stacked embeddings comprise GloVe embeddings, \nforward contextual string embeddings, and backward contextual string embeddings. The \nmodel was trained with the recommended parameters: the size of mini-batches was set to \n32 and the maximum number of epochs was set to 150.\nFor Transformers, training was initiated with the RoBERTa model (Liu et al., 2019). For \nthe present paper, a fine-tuning approach was used. The fine-tuning procedure consisted of \nadding a linear layer to a transformer and retraining the entire network with a small learn-\ning rate. We used a standard approach, where only a linear classifier layer was added on the \ntop of the transformer, as adding the additional CRF decoder between the transformer and \nlinear classifier did not increase accuracy compared with this standard approach (Schweter \n& Akbik, 2020). The chosen transformer model uses subword tokenization. We used the \nmean of embeddings of all subtokens and concatenation of all transformer layers to pro-\nduce embeddings. The context around the sentence was considered. The training was initi-\nated with a small learning rate using the Adam Optimisation Algorithm (Kingma & Ba, \n2014).\nThe TARS model requires labels to be defined in a natural language. Therefore, we \ntransformed our original coded labels into the natural language: FUND - “Funding \nAgency”, IND - “Person”, COR - “Corporation”, GRNB - “Grant Number\", UNI - “Uni-\nversity”, and MISC - “Miscellaneous”. The training for the few-shot approach was initiated \nwith the TARS NER model (Halder et al., 2020).\nResults\nOverall, the training demonstrated mixed results. Table  4 shows training results with cor -\npus No.1 and the TARS zero-shot approach. GRNB showed adequate results by training \nwith Flair Embeddings and TARSfew-shot models. IND was the best-recognized entity by \ntraining with Flair Embeddings and TARS (both zero- and few-shot) with an F1-score of \n0.8 (Flair Embeddings) and 0.86 (TARS) respectively. Training with Transformers was not \nsuccessful for IND with an F1-score of 0. In general, transformers were a less efficient \nalgorithm for training with a small dataset with an overall accuracy of 0.35. FUND dem-\nonstrated not satisfactory results with F1-score of less than 0.5 for all models. Entity types \nFig. 5  The training results with the training corpus No.2. A Comprises diagrams with the F1-scores of the \ntraining with three algorithms for each label class. B depicts the total accuracy of training algorithms Scientometrics\n1 3\nMISC, UNI, and COR showed the worst results with the F1-score equal to zero for all algo-\nrithms. The low accuracy for MISC, UNI, and COR resulted in low overall accuracy for \nall algorithms. Overall, training with corpus No.1 showed insufficient results for all algo-\nrithms. Flair Embeddings and TARS showed better accuracy compared to Transformers.\nFigure  5 shows the training results with corpus No.2. Similar to the training with corpus \nNo.1, IND and GRNB are the best-recognized categories. The best results for IND and \nGRNB demonstrated Flair embeddings with an F1-score of 0.98 (IND) and 0.96 (GRNB). \nTARS achieved the best results for FUND with an F1-score of 0.77 against 0.71 for Flair \nEmbeddings and 0.68 for Transformers. Miscellaneous demonstrated the worst accuracy \nfor Flair Embeddings (0.64) and Transformers (0.49), while for TARS the worst accuracy \nlies in the COR category with an F1-score of 0.54. The best result for UNI showed Flair \nEmbeddings with an F1-score over 0.7. The COR category showed a decent precision \nof 0.88 with Flair Embeddings but a low recall of 0.58 which resulted in a low F1-Score \n(0.7)14.\nTraining with corpus No.2 showed a significant improvement in training accuracy \n(Fig.  5B). Overall, Flair Embeddings was more accurate than other training algorithms, \nalthough training with TARS showed better results for the FUND category. The Trans-\nformers showed the worst results during training.\nAdditionally, a zero-shot approach was tested for the TARS model on corpus no.1. The \nmodel was able to successfully recognize individuals, but struggled with other categories, \nas Table 4 demonstrates. The total accuracy of the model comprises 0.23.\nExperiment 2\nOur first hypothesis to explain the pure model performance for the FUND, COR, MISC, \nand UNI categories is their semantic proximity that prevents successful recognition. \nEntities of these categories are often used in the same context. To examine this hypoth-\nesis, we conducted an experiment using Flair Embeddings with the dataset contain-\ning three types of entities: IND, GRNB, and ORG. The MISC category was excluded \nfrom the training, as one of the aims of the present research is to extract information \nabout acknowledged entities, and the MISC category contains only additional informa-\ntion. The new ORG category was established, which includes a combination of entities \nfrom the FUND, COR, and UNI categories. The training was performed with exactly \nthe same parameters as training with the Flair Embeddings model in Experiment 1 \n(Sect. 4.1).Table 4  F1-scores of the training with three algorithms for each label class with Corpus No. 1\nAlgorithm FUND GRNB IND UNI COR MISC accuracy\nTARS (zero-shot) 0.23 0.33 0.86 0 0 0 0.23\nTARS (few-shot) 0.32 0.76 0.86 0 0 0 0.35\nFlair embeddings 0.42 0.61 0.80 0 0 0 0.35\nTransformers 0.30 0.40 0 0 0 0 0.15\n14 Accuracy metrics by type of entity and total accuracy for all experiments can be found in Appendixes   A \nand BScientometrics \n1 3\nThe UNI and COR categories, though, have distinct patterns. In this case, the low \nperformance of the models for the COR and UNI categories could be explained by the \nsmall size of the training sample that contains these categories (see Fig.  2). Thus, the \nmodel was not able to identify patterns because of the lack of data.\nSecondly, low results for FUND, COR, MISC, and UNI categories might also lie in \nthe nature of the miscellaneous category, as some entities that fall into this category are \nsemantically very close to the FUND and COR categories. As a result, training without \na MISC category might potentially show better performance. To examine this hypoth-\nesis, we conducted training with Flair Embeddings with a dataset excluding the MISC \ncategory, i.e., with five entity types. Training results are shown in Fig. 6 A.\nAdditionally, the problem might lie in the nature of the training algorithms that \nwere used. On the one hand, Flair developers claimed Transformers to be the most effi-\ncient algorithm (Schweter & Akbik, 2020). On the other, the stacked embeddings are \nan important feature of the Flair tool, as a combination of different embeddings might \nbring better results than their separate uses (Akbik et  al., 2019). Thus, the combina-\ntion of the Transformer embeddings model with the contextual string embeddings might \nimprove the model performance. Thus, for the third additional training, we combined \ncontextual string embeddings with FLERT parameters.\nFig. 6  The results of Experiment 2. A–C comprise diagrams with the F1-scores of the training with three \nalgorithms for each label class. D Represents the total accuracy of the training algorithms Scientometrics\n1 3\nResults\nResults of the training are represented in Fig.  6. During the training with three types of \nentities (Fig.  6B) IND and GRNB still achieved high F1-scores of 0.96 (IND) and 0.95 \n(GRNB). Nevertheless, ORG gained only an F1-score of 0.64, which is worse than the \nprevious results with six entity types. The results of the training with five types of entities \nwere quite similar to those achieved during the training with six types of entities. FUND \nand UNI categories showed a small improvement in precision, recall, and F1 score com-\npared to training with 6 types of entities with Flair Embeddings. At the same time, the \nperformance of the COR category deteriorated noticeably (0.6 vs. the previous 0.7). The \nimprovement in overall accuracy (Fig.  6D) (0.80 vs. the previous 0.77) could be explained \nby the fact that the MISC category was not present in this training and could not affect \noverall accuracy with its low F1-score.\nAs Fig.  6C demonstrates, training with Flair Embeddings and RoBERTa showed no \nimprovements compared to the results of the primary training with Transformers and worse \nperformance compared with Flair Embeddings. As in Experiment 1, the COR category \nachieved high precision but low recall, resulting in a low F1-score (0.67). For some catego-\nries (COR and GRNB) Flair Embeddings combined with RoBERTa performed better than \nTransformers but still worse than Flair Embeddings.\nExperiment 3\nThe results of experiment 2 showed that altering the training parameters and decreasing the \nnumber of entity classes does not improve the model accuracy. We assume that increasing \nthe size of the training corpus would improve the performance of entities with low recogni-\ntion accuracy. Therefore, for this experiment, we designed two corpora with an increased \nnumber of acknowledged entities.\nAs the Flair Embeddings algorithm trained with Corpus No.2 showed the best perfor -\nmance, it was of interest if the increased training data will outperform its accuracy score. \nTraining in Experiments 1 and 2 was carried out using Flair version 0.9. As Flair recently \nupdated to version 0.11, we used this newest version for the following training. The training \nwas carried out with exactly the same parameters as the training with the Flair Embeddings \nFig. 7  The results of Experiment 3. A Comprises diagrams with the F1-scores of training with three cor -\npora for each label class. B Represents the total accuracy of the trainingScientometrics \n1 3\nmodel in Experiment 1 (Sect. 3.1). To achieve comparable results we also retrained, for \nnow, the best model (Flair Embeddings with Corpus No.2) with the Flair 0.11.\nResults\nResults of the training are represented in Fig.  7. Retraining of the original model with the \nFlair 0.11 Fig.  7-B showed slightly better performance (0.79 vs. 0.77) than training with \nversion 0.9. In general, no huge differences in accuracy were found during training with \nextended corpora.\nOverall, the best F1-Score for the FUND category (0.77) was reached with the TARS \nalgorithm and corpus No.2. COR gained the best accuracy (0.7) with Flair Embeddings \nand corpus No.2 using Flair version 0.9. The GRNB category showed the best perfor -\nmance (0.96) with Flair Embeddings trained on the corpus with five types of entities \n(Flair Embeddings 5 Ent). The best F1-Score of the IND category was achieved with \nFlair Embeddings trained on corpus No.2 with Flair version 0.11. MISC performed the \nbest (0.66) with Flair Embeddings trained on Corpus No.4 with Flair version 0.11. The \nbest accuracy of the UNI category was achieved with Flair Embeddings trained on corpus \nNo.3 with Flair version 0.11. In general, the best overall accuracy of 0.79 (for six entity \ntypes) had the Flair Embeddings model trained on corpus No.2 with Flair version 0.11.\nDiscussion\nAs expected, Experiment 1 showed a large improvement in accuracy for all algorithms \nwhen the size of a training corpus was increased from 49 to 654 sentences. However, fur -\nther enlargement of the corpus (in Experiment 3) did not make any progress. Some types \nof entity, such as IND and GRNB, showed great performance (GRNB with an F1-Score of \n0.96 or IND with 0.98) with the small training samples, i.e., 354 entities from the GRNB \ncategory or 439 entities from the IND category. At the same time, training with a sample of \n1322 labelled funding organisations achieved an F1-Score of only 0.75.\nThe TARS model is designed to perform NER with small or no training data. In \nexperiment 1, TARS without training data was able to extract individuals with quite high \naccuracy (F-1 score of 0.86). TARS trained with the small corpus (No. 1) did not show \nimprovement in the F-1 score of individuals, but greatly improved the F-1 score of the \nGRNB category. For other entity types, this model showed extremely weak results. It was \nexpected that training with Flair Embeddings and Transformers will not bring high recog-\nnition accuracy with corpus No.1, however, interesting results can be observed. Thus, Flair \nEmbeddings showed decent accuracy of 0.8 for individuals with the small training dataset.\nThe imbalance in the performance of different types of entities can be explained by the \nnature of the data, on which the original models were trained. Thus, Flair Embeddings were \ntrained on the 1-billion words English corpus (Chelba et al., 2013). RoBERTa was pre-trained \non the combination of five datasets containing news articles, blog entries, books, and Wikipe-\ndia articles. TARS was mainly pre-trained on datasets for text classification. Thus, the models \nused were not trained on domain-specific data. This can also explain the pure Transformers \nand TARS performance. The higher accuracy for the individuals category in the training with \nTARS can be explained by the fact, that the word ’person’ is semantically more straightforward \nthan other categories. The same could be applied to grant numbers. Furthermore, grant num-\nbers generally have similar patterns, which can be applied to all entities of this type, that can  Scientometrics\n1 3\nexplain a rapid improvement in F-1 score between zero-shot and few-shot models. Moreover, \nIND and GRNB categories showed better performance for other algorithms too, which could \nlie in the structure of these entities: names of individuals and grant numbers usually have undi-\nversified patterns and in acknowledgement texts are used in a small variety of contexts. At the \nsame time, other entity types, such as funding organisations and universities could have similar \npatterns and could be used in the same context. In some cases, even for human annotators, it \nis impossible to distinguish between university, funding body and corporation without back -\nground knowledge about the entity.\nPrevious works showed improvements in downstream tasks using embedding models \nfine-tuned for the domain used (Shen et al., 2022; Beltagy et al.., 2019). Therefore, fine-\ntuning the general language model on the sample of acknowledgment texts could improve \nthe performance of the NER model for acknowledgment texts. We are planning to fine-tune \nBERT and Flair Embeddings (contextual string embeddings) on a sample of approx. 5 mil-\nlion acknowledgment texts from WoS and evaluate the performance of the NER models.\nThe results of Experiment 2 generally did not show an improvement in accuracy. On the \ncontrary, training with the three entity types deteriorated the model performance. Train-\ning without the MISC category did not show significant performance progress either. \nMoreover, further analysis of acknowledged entities showed that the miscellaneous cate-\ngory contained very inhomogeneous and partly irrelevant data, making the analysis more \ncomplicated (Smirnova & Mayr, 2023). Therefore, we assume that the model would make \nbetter predictions if the number of entity types is expanded and miscellaneous categories \nexcluded, i.e., the MISC category could be split into the following categories: names of \nprojects, names of conferences, names of software and dataset. Different subcategories \ncould also be distinguished in the FUND category.\nCorpora No.2 and No.3 contain the same number of MISC and COR entities15, while \nin corpus 4 number of occurrences of MISC and COR entities is higher. For MISC and \nCOR, accuracy slightly increased with corpus 4, therefore we assume that the extraction \naccuracy for these entities will increase with the increase of the training data. The situa-\ntion is different for funding organizations and universities. The number of UNI and FUND \nentities increased evenly from corpus No.1 to corpus No.4. Nevertheless, the best result for \nthe UNI category was achieved with corpus No.3. The poor performance of corpus No.4 \ncould be explained by the inclusion of Indian funders. Thus, the names of many Indian \nfunders are very similar to the entities which usually fall into the UNI category, e.g., the \nDepartment of Science and Technology or the Department of Biotechnology. This pattern \nis more common to the entities which fall into the UNI category. Therefore, that might \nmake the exact extraction of UNI and FUND entities more confusing. Moreover, many \nIndian Universities contain the name of individuals, e.g., Rajiv Gandhi University, which \ncan cause confusion of the UNI category with the IND category. Generally, no improve-\nment in increasing the size of the corpus for the FUND category can be explained by the \nambiguous nature of the entities which fall into the FUND category and their semantical \nproximity with other types of entities. Analysis of the extracted entities showed that many \nentities were extracted correctly, but were assigned to the wrong category (Smirnova & \nMayr, 2023). Therefore, an additional classification algorithm applied to extracted entities \ncould improve the model’s performance.\n15 These differences in entity distribution are caused by the peculiarities of acknowledgement information \nstored in WoS. As only acknowledgements with indexed funding information are stored in the database, it \nwas difficult to find an adequate number of acknowledged entities of other typesScientometrics \n1 3\nConclusion\nIn this paper, we evaluated different embedding models for the task of automatic extraction \nand classification of acknowledged entities from acknowledgment texts16. The annotation \nof the training corpora was the most challenging and time-consuming task of all data prep-\naration procedures. Therefore, a semi-automated approach was used to help significantly \naccelerate the procedure.\nThe study’s main limitations were its small size and just one annotator of the training \ncorpora. Additionally, we used acknowledgments texts collected in WoS. WoS only stores \nacknowledgments containing funding information, therefore there was a lack of other types \nof entities, such as corporations or universities in the training data.\nIn the present paper, we aimed to answer three questions. Thus, regarding research ques-\ntion 1, the few-shot and zero-shot models showed very low total recognition accuracy. At \nthe same time, it was observed that some entities performed better than others with all \nalgorithms and training corpora. Thus, individuals gained a good F1-score over 0.8 with \nzero-shot and few-shot models, as also with Flair embeddings trained with the smallest \ncorpus. With the enlargement of the training corpora, the performance of the IND category \nalso increased and achieved an F1-score over 0.9. The GRNB category showed an adequate \nF-1 score of 0.76 with the few-shot algorithm trained with the smallest corpus, following \ntraining with corpus No.2 boosts the F-1 score to over 0.9. Therefore, few-shot and zero-\nshot approaches were not able to identify all the defined acknowledged entity classes.\nWith respect to research question 2, Flair Embeddings showed the best accuracy in \ntraining with corpus No.2 (and version 0.11) and the fastest training time compared to the \nother models; thus, it is recommended to further use the Flair Embeddings model for the \nrecognition of acknowledged entities.\nExploring research question 3 we observed, that the expansion of the size of a training \ncorpus from very small (corpus No.1) to medium size (corpus No.2) massively increased the \naccuracy of all training algorithms. The best-performing model (Flair Embedding) was further \nretrained with the two bigger corpora, but the following expansion of the training corpus did \nnot bring further improvement. Moreover, the performance of the model slightly deteriorated.\nAcknowledgements The original work was funded by the German Center for Higher Education Research \nand Science Studies (DZHW) via the project “Mining Acknowledgement Texts in Web of Science \n(MinAck)”17. Access to the WoS data was granted via the Competence Centre for Bibliometrics18. Data \naccess was funded by BMBF (Federal Ministry of Education and Research, Germany) under grant number \n01PQ17001. Nina Smirnova received funding from the German Research Foundation (DFG) via the project \n“POLLUX”19. The present paper is an extended version of the paper “Evaluation of Embedding Models for \nAutomatic Extraction and Classification of Acknowledged Entities in Scientific Documents” (Smirnova & \nMayr, 2022) presented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Sci-\nentific Documents (EEKE2022).\nAppendix A: Accuracy metrics by type of entity (label) for all \nexperiments\nSee Table 5.\n16 The best model can be tested  at https:// huggi ngface. co/ kalaw inka/ flair- ner- ackno  wledg ments\n17 https:// kalaw inka. github. io/ minack/.\n18 https:// www. bibli ometr  ie. info/ en/ index. php? id= home.\n19 https:// www. pollux- fid. de/ about. Scientometrics\n1 3\nTable 5  Accuracy metrics by type of entity (label) for all experiments\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings No.1 9 IND 0.7692 0.8333 0.8000 12 1\nFlair embeddings No.1 9 GRNB 0.5385 0.7000 0.6087 10 1\nFlair embeddings No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nFlair embeddings No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nFlair embeddings No.1 9 COR 0.0000 0.0000 0.0000 1 1\nFlair embeddings No.1 9 FUND 0.4000 0.4444 0.4211 18 1\nFlair embeddings No.2 9 FUND 0.6524 0.7771 0.7093 157 1\nFlair embeddings No.2 9 IND 0.9764 0.9831 0.9797 295 1\nFlair embeddings No.2 9 GRNB 0.9398 0.9750 0.9571 160 1\nFlair embeddings No.2 9 UNI 0.7527 0.7071 0.7292 99 1\nFlair embeddings No.2 9 MISC 0.6420 0.6341 0.6380 82 1\nFlair embeddings No.2 9 COR 0.8750 0.5833 0.7000 12 1\nTARS (pretrained) No.1 9 IND 1.0000 0.7500 0.8571 12 1\nTARS (pretrained) No.1 9 GRNB 0.7273 0.8000 0.7619 10 1\nTARS (pretrained) No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTARS (pretrained) No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTARS (pretrained) No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTARS (pretrained) No.1 9 FUND 0.3158 0.3333 0.3243 18 1\nTARS (pretrained) No.2 9 FUND 0.7257 0.8089 0.7651 157 1\nTARS (pretrained) No.2 9 IND 0.9281 0.8746 0.9005 295 1\nTARS (pretrained) No.2 9 GRNB 0.8895 0.9563 0.9217 160 1\nTARS (pretrained) No.2 9 UNI 0.7407 0.6061 0.6667 99 1\nTARS (pretrained) No.2 9 MISC 0.6719 0.5244 0.5890 82 1\nTARS (pretrained) No.2 9 COR 0.5000 0.5833 0.5385 12 1\nTransformers No.1 9 GRNB 0.3000 0.6000 0.4000 10 1\nTransformers No.1 9 IND 0.0000 0.0000 0.0000 12 1\nTransformers No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTransformers No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTransformers No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTransformers No.1 9 FUND 0.2414 0.3889 0.2979 18 1\nTransformers No.2 9 FUND 0.6211 0.7516 0.6801 157 1\nTransformers No.2 9 IND 0.9346 0.9695 0.9517 295 1\nTransformers No.2 9 GRNB 0.8704 0.8812 0.8758 160 1\nTransformers No.2 9 UNI 0.6476 0.6869 0.6667 99 1\nTransformers No.2 9 MISC 0.4767 0.5000 0.4881 82 1\nTransformers No.2 9 COR 0.7500 0.5000 0.6000 12 1\nFlair embeddings (3 Ent) No.2 9 IND 0.9577 0.9703 0.9639 303 2\nFlair embeddings (3 Ent) No.2 9 ORG 0.6400 0.6154 0.6275 208 2\nFlair embeddings (3 Ent) No.2 9 GRNB 0.9286 0.9750 0.9512 160 2\nFlair embeddings (5 Ent) No.2 9 IND 0.9764 0.9797 0.9780 295 2\nFlair embeddings (5 Ent) No.2 9 GRNB 0.9345 0.9812 0.9573 160 2\nFlair embeddings (5 Ent) No.2 9 UNI 0.7802 0.7172 0.7474 99 2\nFlair embeddings (5 Ent) No.2 9 COR 0.7500 0.5000 0.6000 12 2\nFlair embeddings (5 Ent) No.2 9 FUND 0.6722 0.7707 0.7181 157 2Scientometrics \n1 3\nAppendix B: Overall accuracy for all experiments\nSee Table 6.Rows are sorted by experiment number and algorithmTable 5  (continued)\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings (RoB-\nERTa)No.2 9 IND 0.9206 0.9831 0.9508 295 2\nFlair embeddings (RoB-\nERTa)No.2 9 GRNB 0.8896 0.9062 0.8978 160 2\nFlair embeddings (RoB-\nERTa)No.2 9 UNI 0.5963 0.6566 0.6250 99 2\nFlair embeddings (RoB-\nERTa)No.2 9 MISC 0.4135 0.5244 0.4624 82 2\nFlair embeddings (RoB-\nERTa)No.2 9 COR 1.0000 0.5000 0.6667 12 2\nFlair embeddings (RoB-\nERTa)No.2 9 FUND 0.6096 0.7261 0.6628 157 2\nFlair embeddings No.2 11 GRNB 0.9345 0.9812 0.9573 160 3\nFlair embeddings No.2 11 IND 0.9797 0.9831 0.9814 295 3\nFlair embeddings No.2 11 FUND 0.7027 0.8280 0.7602 157 3\nFlair embeddings No.2 11 UNI 0.7684 0.7374 0.7526 99 3\nFlair embeddings No.2 11 MISC 0.6543 0.6463 0.6503 82 3\nFlair embeddings No.2 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 UNI 0.8000 0.7273 0.7619 99 3\nFlair embeddings No.3 11 IND 0.9731 0.9797 0.9764 295 3\nFlair embeddings No.3 11 GRNB 0.9281 0.9688 0.9480 160 3\nFlair embeddings No.3 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 MISC 0.6571 0.5610 0.6053 82 3\nFlair embeddings No.3 11 FUND 0.6757 0.7962 0.7310 157 3\nFlair embeddings No.4 11 MISC 0.7424 0.5976 0.6622 82 3\nFlair embeddings No.4 11 COR 0.8571 0.5000 0.6316 12 3\nFlair embeddings No.4 11 UNI 0.7753 0.6970 0.7340 99 3\nFlair embeddings No.4 11 IND 0.9698 0.9797 0.9747 295 3\nFlair embeddings No.4 11 FUND 0.6823 0.8344 0.7507 157 3\nFlair embeddings No.4 11 GRNB 0.9162 0.9563 0.9358 160 3 Scientometrics\n1 3\nFunding Open access funding enabled and organized by Projekt DEAL.\nDeclarations \n Conflict of interest Philipp Mayr, the co-author of this paper, has a conflict of interest because he serves on \nthe editorial board of the journal Scientometrics. In addition, he is a co-guest editor of the special issue on \n\"Extraction and Evaluation of Knowledge Entities from Scientific Documents\". He declares that he has noth-\ning to do with the decision about this paper submission.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAkbik, A., Bergmann, T., Blythe, D., Rasul, K., Schweter, S., & Vollgraf, R. 2019. FLAIR: An Easy-to-Use \nFramework for State-of-the-Art NLP. Minneapolis, Minnesota (pp. 54–59). Association for Computa-\ntional Linguistics.\nAkbik, A., Blythe, D., & Vollgraf, R. (2018). Contextual string embeddings for sequence labeling. In 2018, \n27th International Conference on Computational Linguistics (pp. 1638–1649).\nAlexandera, D. & Vries, A. P. (2021). This research is funded by...”: Named Entity Recognition of financial \ninformation in research papers. In BIR 2021: 11th International Workshop on Bibliometric-enhanced \nInformation Retrieval at ECIR (pp. 102–110).\nBeltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. \nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing \nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)  \n(pp. 3613–3618). Association for Computational Linguistics.Table 6  Overall accuracy for all experiments\nRows are sorted by experiment number and algorithmAlgorithm Corpus Version Accuracy Experiment\nFlair embeddings No.2 9 0.7702 1\nFlair embeddings No.1 9 0.3472 1\nTARS (pretrained) No.2 9 0.7113 1\nTARS (pretrained) No.1 9 0.3485 1\nTransformers No.2 9 0.6783 1\nTransformers No.1 9 0.1477 1\nFlair Embeddings (3 Entity Types) No.2 9 0.7536 2\nFlair embeddings (5 Entity Types) No.2 9 0.7990 2\nFlair embeddings + RoBERTa No.2 9 0.6697 2\nFlair Embeddings No.2 11 0.7869 3\nFlair embeddings No.4 11 0.7814 3\nFlair embeddings No.3 11 0.7691 3Scientometrics \n1 3\nBorst, T., Mielck, J., Nannt, M., & Riese, W. (2022). Extracting funder information from scien-\ntific papers—Experiences with question answering. In Silvello, G., O.  Corcho, P.  Manghi, G.M. \nDi Nunzio, K. Golub, N. Ferro, and A. Poggi (Eds.),Linking theory and practice of digital libraries  \n(Vol. 13541, pp. 289–296). Springer International Publishing. Series Title: Lecture Notes in Com-\nputer Science. https:// doi. org/ 10. 1007/ 978-3- 031- 16802-4_ 24.\nChelba, C., T.  Mikolov, M.  Schuster, Q.  Ge, T.  Brants, P.  Koehn, & Robinson, T. (2013). One Bil-\nlion Word Benchmark for Measuring Progress in Statistical Language Modeling. 10.48550/\nARXIV.1312.3005 .\nChen, H., Song, X., Jin, Q., & Wang, X. (2022). Network dynamics in university-industry collaboration: \nA collaboration-knowledge dual-layer network perspective. Scientometrics, 127(11), 6637–6660. \nhttps:// doi. org/ 10. 1007/ s11192- 022- 04330-9\nCronin, B. (1995). The Scholar’s courtesy: The role of acknowledgement in the primary communication \nprocess. Taylor Graham.\nCronin, B., & Weaver, S. (1995). The praxis of acknowledgement: From bibliometrics to influmetrics. \nRevista Española de Documentación Científica, 18(2), 172.\nDevlin, J., Chang, M.W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional \ntransformers for language understanding. 10.48550/ARXIV.1810.04805 .\nDiaz-Faes, A. A., & Bordons, M. (2017). Making visible the invisible through the analysis of acknowl-\nedgements in the humanities. Aslib Journal of Information Management, 69(5), 576–590. https://  \ndoi. org/ 10. 1108/ AJIM- 01- 2017- 0008\nDoehne, M., & Herfeld, C. (2023). How academic opinion leaders shape scientific ideas: an acknowl-\nedgment analysis., 128(4), 2507–2533. https:// doi. org/ 10. 1007/ s11192- 022- 04623-z\nDzieżyc, M., & Kazienko, P. (2022). Effectiveness of research grants funded by European research coun-\ncil and polish national science centre. Journal of Informetrics, 16(1), 101243. https:// doi. org/ 10.  \n1016/j. joi. 2021. 101243\nEftimov, T., Koroušić Seljak, B., & Korošec, P. (2017). A rule-based named-entity recognition \nmethod for knowledge extraction of evidence-based dietary recommendations. PLoS ONE, 12(6), \ne0179488. https:// doi. org/ 10. 1371/ journ al. pone. 01794 88\nEtzioni, O., Cafarella, M., Downey, D., Popescu, A. M., Shaked, T., Soderland, S., Weld, D. S., & Yates, \nA. (2005). Unsupervised named-entity extraction from the web: An experimental study. Artificial \nIntelligence, 165(1), 91–134. https:// doi. org/ 10. 1016/j. artint. 2005. 03. 001\nFinkel, J.R., Grenager, T., & Manning, C. (2005). Incorporating non-local information into information \nextraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL’05), Ann Arbor, Michigan (pp. 363–370). Association for \nComputational Linguistics.\nGiles, C. L., & Councill, I. G. (2004). Who gets acknowledged: Measuring scientific contributions \nthrough automatic acknowledgment indexing. Proceedings of the National Academy of Sciences,  \n101(51), 17599–17604. https:// doi. org/ 10. 1073/ pnas. 04077 43101\nHalder, K., Akbik, A., Krapac, J., & Vollgraf, R. (2020). Task-Aware Representation of Sentences for \nGeneric Text Classification. In Proceedings of the 28th International Conference on Computational \nLinguistics, Barcelona, Spain (Online) (pp. 3202–3213). International Committee on Computa-\ntional Linguistics.\nHubbard, D., Laddusaw, S., Tan, Q., & Hu, X. (2022). Analysis of acknowledgments of libraries in the \njournal literature using machine learning. Proceedings of the Association for Information Science \nand Technology, 59(1), 709–711. https:// doi. org/ 10. 1002/ pra2. 698\nIovine, A., Fang, A., Fetahu, B., Rokhlenko, O., & Malmasi, S. (2022). CycleNER: An unsupervised \ntraining approach for named entity recognition. In Proceedings of the ACM Web Conference 2022  \n(pp. 2916–2924). ACM.\nJiang, L., Kang, X., Huang, S., & Yang, B. (2022). A refinement strategy for identification of scientific \nsoftware from bioinformatics publications. Scientometrics, 127(6), 3293–3316. https:// doi. org/ 10.  \n1007/ s11192- 022- 04381-y\nKassirer, J. P., & Angell, M. (1991). On authorship and acknowledgments. The New England Journal of \nMedicine, 325(21), 1510–1512. https:// doi. org/ 10. 1056/ NEJM1 99111 21325 2112\nKayal, S., Afzal, Z., Tsatsaronis, G., Katrenko, S., Coupet, P., Doornenbal, M., & Gregory, M. (2017). \nTagging funding agencies and grants in scientific articles using sequential learning models. In \nBioNLP 2017, Vancouver, Canada (pp. 216–221). Association for Computational Linguistics.\nKenekayoro, P. (2018). Identifying named entities in academic biographies with supervised learning. \nScientometrics, 116(2), 751–765. https:// doi. org/ 10. 1007/ s11192- 018- 2797-4\nKingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. 10.48550/\nARXIV.1412.6980 . Scientometrics\n1 3\nKusumegi, K., & Sano, Y. (2022). Dataset of identified scholars mentioned in acknowledgement state-\nments. Scientific Data, 9(1), 461. https:// doi. org/ 10. 1038/ s41597- 022- 01585-y\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoy -\nanov, V. (2019). RoBERTa: A robustly optimized BERT pretraining approach. arXiv: 1907. 11692  \n[cs] .\nMackintosh, K. (1972). Acknowledgements patterns in sociology. Ph. D. thesis, University of Oregon.\nMccain, K. (2017). Beyond Garfield’s citation index: An assessment of some issues in building a per -\nsonal name acknowledgments index. Scientometrics. https:// doi. org/ 10. 1007/ s11192- 017- 2598-1\nMcCain, K. W. (1991). Communication, competition, and secrecy: The production and dissemination of \nresearch-related information in genetics. Science, Technology, & Human Values, 16(4), 491–516. \nhttps:// doi. org/ 10. 1177/ 01622 43991 01600 404\nMejia, C., & Kajikawa, Y. (2018). Using acknowledgement data to characterize funding organizations \nby the types of research sponsored: the case of robotics research. Scientometrics, 114(3), 883–904. \nhttps:// doi. org/ 10. 1007/ s11192- 017- 2617-2\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, \nN., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkur -\nthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: An imperative style, high-\nperformance deep learning library. arXiv: 1912. 01703 [cs, stat].\nPaul-Hus, A., & Desrochers, N. (2019). Acknowledgements are not just thank you notes: A qualitative \nanalysis of acknowledgements content in scientific articles and reviews published in 2015. PLoS \nONE, 14, e0226727. https:// doi. org/ 10. 1371/ journ al. pone. 02267 27\nPaul-Hus, A., Díaz-Faes, A., Sainte-Marie, M., Desrochers, N., Costas, R., & Larivière, V. (2017). \nBeyond funding: Acknowledgement patterns in biomedical, natural and social sciences. PLoS ONE,  \n12, e0185578. https:// doi. org/ 10. 1371/ journ al. pone. 01855 78\nPennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors for Word Representation. \nIn Empirical Methods in Natural Language Processing (EMNLP) (pp. 1532–1543).\nPustejovsky, J., & Stubbs, A. (2012). Natural language annotation for machine learning. O’Reilly \nMedia Inc.\nRose, M., & Georg, C. P. (2021). What 5,000 acknowledgements tell us about informal collaboration \nin financial economics. Research Policy, 50, 104236. https:// doi. org/ 10. 1016/j. respol. 2021. 104236\nSang, T. K., & E. F., & De Meulder, F. (2003). Introduction to the CoNLL-2003 shared task: Language-\nindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Lan-\nguage Learning at HLT-NAACL (pp. 142–147).\nSchweter, S., & Akbik, A. (2020). FLERT: Document-level features for named entity recognition. ArXiv. \n10.48550/arXiv.2011.06993 .\nShen, S., Liu, J., Lin, L., Huang, Y., Zhang, L., Liu, C., Feng, Y., & Wang, D. (2022). SsciBERT: A \npre-trained language model for social science texts. Scientometrics. https:// doi. org/ 10. 1007/  \ns11192- 022- 04602-4\nSingh, V. K., Singh, P., Karmakar, M., Leta, J., & Mayr, P. (2021). The journal coverage of web of sci-\nence, scopus and dimensions: A comparative analysis. Scientometrics, 126(6), 5113–5142. https://  \ndoi. org/ 10. 1007/ s11192- 021- 03948-5\nSmirnova, N., & Mayr, P. (2022). Evaluation of embedding models for automatic extraction and classifi-\ncation of acknowledged entities in scientific documents. In 3rd Workshop on Extraction and Evalu-\nation of Knowledge Entities from Scientific Documents 2022 (EEKE 2022) (pp. 48–55). CEUR-WS.\norg.\nSmirnova, N., & Mayr, P. (2023). A comprehensive analysis of acknowledgement texts in web of sci-\nence: A case study on four scientific domains. Scientometrics, 1(128), 709–734. https:// doi. org/ 10.  \n1007/ s11192- 022- 04554-9\nSong, M., Kang, K. Y., Timakum, T., & Zhang, X. (2020). Examining influential factors for acknowl-\nedgements classification using supervised learning. PLoS ONE. https:// doi. org/ 10. 1371/ journ al.  \npone. 02289 28\nThomer, A. K., & Weber, N. M. (2014). Using named entity recognition as a classification heuristic. In \niConference 2014 Proceedings (pp. 1133 – 1138). iSchools.\nWang, J., & Shapira, P. (2011). Funding acknowledgement analysis: An enhanced tool to investigate \nresearch sponsorship impacts: The case of nanotechnology. Scientometrics, 87(3), 563–586. https://  \ndoi. org/ 10. 1007/ s11192- 011- 0362-5\nYamada, I., Asai, A., Shindo, H., Takeda, H., & Matsumoto, Y. (2020). LUKE: Deep contextualized \nentity representations with entity-aware self-attention. In Proceedings of the 2020 Conference on \nEmpirical Methods in Natural Language Processing (EMNLP) (pp. 6442–6454). Association for \nComputational Linguistics.Scientometrics \n1 3\nYu, J., Bohnet, B., & Poesio, M. (2020). Named entity recognition as dependency parsing. 10.48550/\nARXIV.2005.07150.\nZhang, C., Mayr, P., Lu, W., & Zhang, Y. (2023). Guest editorial: Extraction and evaluation of knowledge \nentities in the age of artificial intelligence. Aslib Journal of Information Management, 75, 433–437. \nhttps:// doi. org/ 10. 1108/ AJIM- 05- 2023- 507",
        "type": "direct"
    },
    {
        "citation": "corpora Nos, 2022",
        "content": ". Furthermore, large bibliographic databases \nsuch as Web of Science (WoS)1 and Scopus selectively index only funding information, \ni.e., names of funding organizations and grant identification numbers. Consequently, we \nwant to extend that to other types of acknowledged entities: individuals, universities, cor -\nporations, and other miscellaneous information. Analysis of the acknowledged individuals \nprovides insight into informal scientific collaboration (Rose & Georg, 2021; Kusumegi & \nSano, 2022). Acknowledged universities and corporations reveal interactions and knowl-\nedge exchange between industry and universities",
        "type": "regular"
    },
    {
        "citation": "com/.\n, 2003",
        "content": ". Entities from the \nmiscellaneous category include other information like project names, which could uncover \ninternational scientific collaborations.\nThe state-of-the-art named entity recognition (NER) models showed a great perfor -\nmance on the CoNLL-2003 dataset (Akbik et al., 2018; Devlin et al., 2018; Yamada et al., \n2020; Yu et  al., 2020). CoNLL-2003 corpus",
        "type": "regular"
    },
    {
        "citation": "corpora Nos, 2022",
        "content": "is a benchmark dataset \nfor language-independent named entity recognition, i.e., designed to train and evaluate \nNER models. English data for the corpus were taken from the Reuters corpus. The dataset \ncomprises four types of named entities: person, location, organisation, and miscellaneous. \nHowever, specific domains require specifically labelled training data. The development of a \ntraining dataset for the specific domain is an expensive and time-consuming process since \nNER usually requires a quite large training corpus. Therefore, the objective of this paper is \nto evaluate the performance of existing embedding models for the task of automatic extrac-\ntion and classification of acknowledged entities from the acknowledgment text in scientific \npapers using small training datasets or without training data (zero-short approach).\n1 http:// wokin fo. com/ produ cts_ tools/ multi disci plina ry/ webof  scien ce/ fundi ngsea rch/.Scientometrics \n1 3\nThe present paper is an extended version of the article",
        "type": "regular"
    },
    {
        "citation": "al., 2019",
        "content": "2 pre-\nsented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Sci-\nentific Documents (EEKE2022).3 Flair, an open-source natural language processing (NLP) \nframework",
        "type": "regular"
    },
    {
        "citation": "\nAdditionally 2014",
        "content": "is used in our study to create a tool for the extraction of \nacknowledged entities because this library is easily customizable. It offers the possibility \nof creating a customized Named Entity Recognition (NER) tagger, which can be used for \nprocessing and analyzing acknowledgment texts. Furthermore, Flair has shown better accu-\nracy for NER tasks using pre-trained datasets in comparison with many other open source \nNLP tools.4\nIn the first experiment (Sect.  4.1) we trained and implemented a NER task using three \ndefault Flair NER models with two differently-sized corpora.5 All the descriptions of the \nFlair framework features refer to the releases 0.9 and 0.11. The models were trained to rec-\nognize six types of acknowledged entities: funding agency, grant number, individuals, uni-\nversity, corporation, and miscellaneous. The model with the best accuracy can be applied \nfor the comprehensive analysis of the acknowledgment texts. In Experiments 2 and 3 we \nperformed additional training with altered training parameters or altered training corpora \n(Sects  4.2 and 4.3). Most of the previous works on acknowledgment analysis were limited \nby the manual evaluation of data and therefore by the amount of processed data (Giles \n& Councill, 2004; Paul-Hus et al., 2017; Paul-Hus & Desrochers, 2019; Mccain, 2017). \nFurthermore, Thomer and Weber",
        "type": "regular"
    },
    {
        "citation": "Experiment 2 1972",
        "content": "argues that using named entities can benefit the \nprocess of manual document classification and evaluation of the data. Therefore, a model \nthat is capable of extracting and classification of different types of entities may potentially \nmake a significant contribution to the field of automated acknowledgment analysis.\nResearch questions\nIn this paper, we address the following research questions:\n• RQ1: Is the few-shot or zero-shot approach able to identify predefined acknowledged \nentity classes?\n• RQ2: Which of the Flair default NER models is more suitable for the defined task of \nextraction and classification of acknowledged entities from scientific acknowledgments \nusing a small training dataset?\n• RQ3: How does the size of the training corpus affect the training accuracy for different \nNER models?\nCreating a training dataset for supervised learning is a time-consuming and expensive task, \nsince as a rule, such a model requires a reasonably large amount of training data. Annota-\ntion is a crucial moment, as wrongly annotated data will deteriorate training results. There-\nfore, more than one annotator is usually required to provide credible results. That is why \n2 In this paper we conducted an additional experiment (Experiment 3) with 2 new corpora (corpus Nos. 3 \nand 4).\n3 https:// eeke- works hop. github. io/ 2022/.\n4 https:// github. com/ flair NLP/ flair .\n5 The release 0.9 (https:// github. com/ flair NLP/ flair/ relea ses/ tag/ v0.9) was used in the experiments 1 and 2. \nExperiment 3 was performed using release 0.11 (https:// github. com/ flair NLP/ flair/ relea ses/ tag/ v0. 11). Scientometrics\n1 3\nit is of interest to test if the existing NER models can provide reasonable accuracy while \nusing small or no training data.\nBackground and related work\nResearch in the field of acknowledgments analysis has been carried out since the 1970s. \nThe first typology of acknowledgments was proposed by Mackintosh",
        "type": "regular"
    },
    {
        "citation": "Experiment 2 1991",
        "content": "(as cited \nin Cronin, 1995) and comprised three categories: facilities, access to data, and help of indi-\nviduals. McCain",
        "type": "regular"
    },
    {
        "citation": "Experiment 2 1995",
        "content": "distinguished five types of acknowledgements: research-related \ninformation, secondary access to research-related information, specific research-related \ncommunication, general peer communication, and technical or clerical support. Cronin and \nWeaver",
        "type": "regular"
    },
    {
        "citation": "Experiment 2 2018",
        "content": "defined three broad categories: resource-, procedure- and concept-related. \nMejia and Kajikawa",
        "type": "regular"
    },
    {
        "citation": "Nina Smirnova 2023",
        "content": "developed a four-level classification based on sponsored \nresearch field: change maker, incremental, breakthrough, and matured.\nDoehne and Herfeld",
        "type": "regular"
    },
    {
        "citation": "Nina Smirnova 2011",
        "content": "distinguished acknowledgements from the perspective of \nappreciation of influential scholars and defined two axes: scientific influence and insti-\ntutional influence. Scientific influence refers to the productiveness and creativity of the \nresearcher, while institutional influence is associated with the scholar’s administrative posi-\ntion in the scientific community.\nWang and Shapira",
        "type": "regular"
    },
    {
        "citation": "Nina Smirnova 2021",
        "content": "investigated the connection between research funding and \nthe development of science and technology using acknowledgments from articles from the \nfield of nanotechnology. Rose and Georg",
        "type": "regular"
    },
    {
        "citation": "Philipp Mayr 2018",
        "content": "studied informal cooperation in academic \nresearch. The analysis revealed generational and gender differences in informal collabora-\ntion. The authors claim that information from informal collaboration networks makes bet-\nter predictions of the academic impact of researchers and articles than information from \nco-author networks. Mejia and Kajikawa",
        "type": "regular"
    },
    {
        "citation": "Philipp Mayr 2023",
        "content": "argued that the classification of funders \ncould be useful in developing funding strategies for policymakers and funders.\nDoehne and Herfeld",
        "type": "regular"
    },
    {
        "citation": "I. G., 2012, Chapter 1",
        "content": "manually investigated acknowledgement sections of papers, \nwhich were published or preprinted in association with the Cowles Foundation between \nearly 1940 and 1970 to trace the influence of the informal social structure and academic \nleaders on the early acceptance of scientific innovations. Blockmodelling was applied to \nthe acknowledgement data. Their analysis showed that the adoption of scientific innova-\ntions was partly influenced by the social structure and by the scientific leaders at Cowles.\nRecent advances in NER\nNamed Entity Recognition (NER) is a form of NLP that aims to extract named entities \nfrom unstructured text and classify them into predefined categories. A named entity is a \nreal-world object that is important for understanding the text. Current approaches in NER \ncan be distinguished into supervised and unsupervised tasks. In a supervised NER a model \nis trained using a labelled dataset. This training dataset or corpus is usually split into sev -\neral datasets: training set, test set, and validation set. NER models require corpora with \nsemantic annotation, i.e., metadata about concepts attached to unstructured text data. The \nannotation process is crucial as insufficient or redundant metadata can slow down and bias \na learning process .\nSupervised NER mainly relays on machine learning or deep learning methods. The \nstate-of-the-art models are based on deep recurrent models, convolution-based, or Scientometrics \n1 3\npre-trained transformer architectures (Iovine et  al., 2022). Thus, Akbik et  al. (2018) \nproposed a new character-based contextual string embeddings method. This approach \npasses a sequence of characters through the character-level language model to gener -\nate word-level embeddings. The model was pre-trained on large unlabeled corpora. The \ntraining was carried out using a character-based neural language model together with a \nBidirectional LSTM (BiLSTM) sequence-labelling model. This approach generates dif-\nferent embeddings for the same word depending on its context and showed good results \non downstream tasks such as NER. Devlin et al. (2018) presented BERT (Bidirectional \nEncoder Representations Transformers), a transformer-based language representa-\ntion model that models the representation of contextualized word embeddings. BERT \nshowed superior results on downstream tasks using different benchmarking datasets. \nLater, Liu et al. (2019) performed an optimization of the BERT model and introduced \nRoBERTa (Robustly Optimized BERT Pretraining Approach). RoBERTa was evaluated \non three benchmarks and demonstrated massive improvements over the reported BERT \nperformance.\nCurrently, several domain-specific models have been developed. Thus, Beltagy et  al.. \n(2019) released SciBERT a BERT-based language model pre-trained on a large number \nof unlabeled scientific articles from the computer science and biomedical domains. SciB-\nERT showed improvements over BERT on several downstream NLP tasks, including NER. \nRecently, Shen et al. (2022) introduced the SsciBERT, a language model based on BERT \nand pre-trained on abstracts published in the Social Science Citation Index (SSCI) journals. \nThe model showed good results in discipline classification and abstract structure-function \nrecognition in articles from the social sciences domain.\nUnsupervised methods are often based on lexicons or predefined rules. Thus, Etzioni \net al. (2005) uses lists of patterns and domain-specific rules to extract named entities. Efti-\nmov et al. (2017) developed a rule-based NER model to extract dietary information from \nscientific publications. Evaluation of the model performance showed good results. Opposed \nto previous unsupervised NER approaches, Iovine et al. (2022) proposed a cycle-consist-\nency approach for NER (CycleNER). CycleNER is unsupervised and does not require par -\nallel training data. The method showed 73% of supervised performance on CoNLL03.\nNER in scientometrics analysis\nNamed entities are widely used in scientometrics analysis. Thus, Kenekayoro (2018) devel-\noped a supervised method for the automatic extraction of named entities from academic \nbibliographies. The aim of the study was to create a database containing unified academic \ninformation about individuals to help in expert finding. A labeled training dataset was \ndeveloped using biographies extracted from ORCID.6 The authors tested several models \nfor NER. The Support Vector Machine classification algorithm (SVM) showed the best \nperformance.\nJiang et al. (2022) proposed a strategy for the identification of software in scientific bio-\ninformatics publications using the combination of SVM and CRF (Conditional Random \nField). Application of the method to the sample of articles from bioinformatics domains \nallowed them to observe interesting patterns in using software in scientific research.\nKusumegi and Sano (2022) analysed scholarly relationships by analysing acknowl-\nedged individuals from the acknowledgments statements from eight open-access journals. \n6 https:// orcid. org/. Scientometrics\n1 3\nIndividuals were extracted using the Stanford CoreNLP NER tagger. In the next steps, \nscholars were identified among the extracted individuals by mapping them to the Microsoft \nAcademic Graph (MAG).\nWe are aware of several works on automated information extraction from acknowledg-\nments. Giles and Councill (2004) developed an automated method for the extraction and \nanalysis of acknowledgment texts using regular expressions and SVM. Computer science \nresearch papers from the CiteSeer digital library were used as a data source. Extracted enti-\nties were analysed and manually assigned to the following four categories: funding agen-\ncies, corporations, universities, and individuals.\nThomer and Weber (2014) used the 4-class Stanford Entity Recognizer (Finkel et al., \n2005) to extract persons, locations, organizations, and miscellaneous entities from the col-\nlection of bioinformatics texts from PubMed Central’s Open Access corpus. The aim of \nthe study was to determine an approach to \"increase the speed of ... classification without \nsacrificing accuracy, nor reliability\" (Thomer & Weber, 2014, p. 1134).\nKayal et  al. (2017) introduced a method for extraction of funding organizations and \ngrants from acknowledgment texts using a combination of sequential learning models: con-\nditional random fields (CRF), hidden markov models (HMM), and maximum entropy mod-\nels (MaxEnt). The final model contained pooled outputs from the models used.\nAlexandera and Vries (2021) proposed AckNER, a tool for extracting financial informa-\ntion from the funding or acknowledgment section of a research article. AckNER works \nwith the use of dependency parse trees and regular expressions and is able to extract names \nof the organisations, projects, programs, and funds, as also numbers of contracts and \ngrants7.\nFollowing, Borst et  al. (2022) applied a question-answering (QA) based approach to \nidentify funding information in acknowledgments texts. This approach performs similarly \nto AckNER and requires a smaller set of training and test data.\nTable  1 shows an overview of works on NER in scientometrics. Overall, previous works \non the extraction of named entities from acknowledgements texts were mostly concerned \nwith the extraction of funding information, i.e., only names of funding bodies and grant \nnumbers, or extraction and linking of individuals. The special issue by Zhang et al. (2023) \nprovided a recent overview of current works in the extraction of knowledge entities.\nTo the best of our knowledge the work of Giles and Councill (2004) is the only \nattempt to extract and categorise multiple acknowledged entities. Nevertheless, entities \nwere extracted using the SVM algorithm but the classification of entities themselves \nwas produced manually, which limited the number of acknowledgement texts to be ana-\nlysed. Furthermore, as far as we know, there was no research done concerning the eval-\nuation of embedding models for extraction of information from acknowledgement texts \nand no tool for automatic extraction of different kinds of acknowledged entities was \ndeveloped.\n7 AckNER showed better performance as Flair, but is specifically designed to recognize two types of \nacknowledged entities (Alexandera & Vries, 2021), which was insufficient for the present project.Scientometrics \n1 3\nTable 1  Overview of works on NER in scientometrics\nPaper Area of application and aim of \nthe studyCorpus Entities Methods and tools\nGiles and Councill (2004) Extraction of acknowledged enti-\nties form acknowledgementsCiteSeer Funding agencies, Companies, \nEducational Institutions, \nIndividualsSVM for extracting entities and \ntheir manual classification\nThomer and Weber (2014) Using NER to improve classifica-\ntion of acknowledgementsPubMed Central’s Open Access Persons, locations, organizations, \nand miscellaneous4-class Stanford Entity Recognizer\nKayal et al. (2017) Extraction of funding information \nfrom acknowledgementsPubMed Central’s Open Access Funding bodies, grants CRF, HMM, MaxEnt\nKenekayoro (2018) Extraction of biography informa-\ntion from academic biographiesORCID Award, Location, Organization, \nPerson, Position, Specializa-\ntion, OthersSVM\nAlexandera and Vries (2021) Extraction of funding information \nfrom acknowledgementsTU Delft’s institutional repository Funding bodies, grants SpaCy dependency parser + regu-\nlar expressions\nJiang et al. (2022) Extraction of scientific software \nfrom scientific articles (full \ntexts) in bioinformaticsbioinformatics journals EnsembleSVMs-CRF\nBorst et al. (2022) Extraction of funding information \nfrom acknowledgementsEconStor Funding bodies, grants Haystack\nKusumegi and Sano (2022) Extraction and linking of \nacknowledged individuals from \nacknowledgementsPLOS Individuals Stanford CoreNLP NER tagger + \nMAG Scientometrics\n1 3\nMethod\nIn the present paper, different models for extraction and classification of acknowledged \nentities supported by the Flair NLP framework were evaluated. The choice of classification \nwas inspired by Giles and Councill’s (2004) classification: funding agencies (FUND), cor -\nporations (COR), universities (UNI), and individuals (IND). For our project, this classifica-\ntion was enhanced with the miscellaneous (MISC) and grant numbers (GRNB) categories. \nThe GRNB category was adopted from WoS funding information indexing. The entities \nin the miscellaneous category could provide useful information, but cannot be ascribed to \nother categories, e.g., names of projects and names of conferences. Figure  1 demonstrates \nan example of acknowledged entities of different types. To the best of our knowledge, Giles \nand Councill’s classification is the only existing classification of acknowledged entities and \ntherefore can be applied to the NER task. Other works on acknowledgment analysis were \nfocused on the classification of acknowledgment texts.\nThe Flair NLP framework\nFlair is an open-sourced NLP framework built on PyTorch (Paszke et  al., 2019), which \nis an open-source machine learning library. “The core idea of the framework is to pre-\nsent a simple, unified interface for conceptually very different types of word and document \nembeddings” (Akbik et  al., 2019, p.  54). Flair has three default training algorithms for \nNER which were used for the first experiment in the present research: a) NER Model with \nFlair Embeddings (later on Flair Embeddings) (Akbik et al., 2018), b) NER Model with \nTransformers (later on Transformers) (Schweter & Akbik, 2020), and c) Zero-shot NER \nwith TARS (later on TARS) (Halder et al., 2020) 8.\nThe Flair Embeddings model uses stacked embeddings, i.e., a combination of contex-\ntual string embeddings (Akbik et al., 2018) with a static embeddings model. This approach \nwill generate different embeddings for the same word depending on its context. Stacked \nembedding is an important Flair feature, as a combination of different embeddings might \nbring better results than their separate uses (Akbik et al., 2019).\nThe Transformers model or FLERT-extension (document-level features for NER) is a \nset of settings to perform a NER on the document level using fine-tuning and feature-based \nFig. 1  An example of acknowledged entities. Each entity type is marked with a distinct color\n8 New transformer models as SciBERT or SsciBERT were not evaluated in this study, as the objective of \nthe study is to evaluate the performance of the Flair default models.Scientometrics \n1 3\nLSTM-CRF with the multilingual XML-RoBERTa transformer model (Schweter & Akbik, \n2020).\nThe TARS (task-aware representation of sentences) is a transformer-based model, \nwhich allows performing training without any training data (zero-shot learning) or with a \nsmall dataset (few-short learning) (Halder et al., 2020). The TARS approach differs from \nthe traditional transfer learning approach in the way that the TARS model also considers \nsemantic information captured in the class labels themselves. For example, for analyzing \nacknowledgments, class labels like funding organization or university already carry seman-\ntic information.\nTraining data\nThe Web of Science (WoS) database was used to harvest the training data (funding \nacknowledgments).9 From 2008 on, WoS started indexing information about funders and \ngrants. WoS uses information from different funding reporting systems such as Research-\nfish,10 Medline11 and others. As WoS contains millions of metadata records (Singh et al., \n2021), the data chosen for the present study was restricted by year and scientific domain \n(for the corpora Nos. 1, 2, and 3) or additionally by the affiliation country (for corpus No.4). \nTo construct corpora Nos. 1-3 records from four different scientific domains published \nfrom 2014 to 2019 were considered: two domains from the social sciences (sociology and \neconomics) and oceanography and computer science. Different scientific domains were Table 2  Number of sentences/texts in the training corpora\nCorpus No. Training set (train) Test set (test) Validation set (dev) Total\n1 29/27 10/10 10/10 49/47\n2 339/282 165/150 150/136 654/441\n3 784/657 165/150 150/136 1099/816\n4 1148/885 165/150 150/136 1463/1044\nTable 3  Number of sentences/texts from each scientific domain in the training corpora\nCorpus No. Oceanography Economics Social Sciences Computer Science\n1 13/13 3/3 20/20 16/14\n2 127/75 92/58 351/234 173/129\n3 175/112 128/89 590/434 333/269\n9 The present research was conducted in scopes of two projects: MinAck (https:// kalaw inka. github. io/ \nminack/) and SEASON (https:// github. com/ kalaw inka/ season). Corpora Nos.1, 2, and 3 were created for the \nMinAck project and serve the purpose of a general evaluation of the impact of the size of the training cor -\npus on the model performance. Corpus No.4 was designed specifically for the SEASON project in the hope \nof improving the recognition of Indian funding information. The project SEASON aims to get insight into \nGerman-Indian scientific collaboration. Our other corpora mainly contain papers published by European \ninstitutions. That is why we enhance Corpus 4 with the papers published by Indian institutions.\n10 https:// resea rchfi sh. com/.\n11 https:// www. nlm. nih. gov/ bsd/ fundi ng_ suppo rt. html. Scientometrics\n1 3\nchosen since previous work on acknowledgment analysis revealed the relations between \nthe scientific domain and the types of acknowledged entities, i.e., acknowledged individu-\nals are more characteristic of theoretical and social-oriented domains. At the same time, \ninformation on technical and instrumental support is more common for the natural and \nlife sciences domains (Diaz-Faes & Bordons, 2017). Only the WoS record types “article” \nand “review” published in a scientific journal in English were selected; then 1000 distinct \nacknowledgments texts were randomly gathered from this sample for the training dataset. \nFurther different amounts of sentences containing acknowledged entities were distributed \ninto the differently-sized training corpora. Table  2 demonstrates the number of sentences \nin each set in the four corpora. We selected only sentences that contain an acknowledged \nentity, regardless of the scientific domain. Table  3 contains the number of sentences and \ntexts from each scientific domain in the training corpora.12 The same article can belong to \nseveral scientific domains, therefore, the number of sentences and texts in Tables  2 and 3 \ndoes not match. Corpus No.4 was designed in such a way that all the training data from the \nCorpus No.3 was enhanced with acknowledgments texts from the articles that have Indian \naffiliations regardless of scientific domain or publication date.\nPreliminary analysis of the WoS data showed that the indexing of WoS funding infor -\nmation has several issues. The WoS includes only acknowledgments containing funding \ninformation; therefore, not every WoS entry has an acknowledgment, individuals are not \nincluded, and indexed funding organizations are not divided into different entity types like \nuniversities, corporations, etc. Therefore, the existing indexing of funding organizations \nis incomplete. Furthermore, there is a disproportion between the occurrences of acknowl-\nedged entities of different types. Thus, the most frequent entity types in the dataset with \nthe training data are IND, FUND and GRNB, followed by UNI and MISC. COR is the \nFig. 2  The distribution of acknowledged entities in the training corpora\n12 Corpus No.4 is not in Table  2, because the corpus contains additional acknowledgment texts from arti-\ncles with Indian affiliations regardless of the scientific domain and therefore contains different scientific \ndomains.Scientometrics \n1 3\ncategory most underrepresented in the data set. Consequently, there are different amounts \nof entities of different types in the training corpora (as Fig.  2 demonstrates), which might \nhave influenced the training results. Training with the corpora Nos. 2, 3, and 4 was evalu-\nated on the same training and validation datasets to ensure plausible accuracy (Fig.  3-B). \nFig. 3  The distribution of acknowledged entities in the test and validation corpora\nFig. 4  Annotation flowchart Scientometrics\n1 3\nHowever, training with corpus No.1 was evaluated with the smaller test and validation sets, \nas corpus No.1 contains a smaller number of sentences (Fig. 3-A).\nData annotation\nThe training corpus was annotated with six types of entities. As WoS already contains \nsome indexed funding information, it was decided to develop a semi-automated approach \nfor data annotation (as Fig.  4 demonstrates) and use indexed information provided by WoS, \ntherefore, grant numbers were adopted from the WoS indexing unaltered.\nFlair has a pre-trained 4-class NER Flair model (CoNLL-03).13 The model can pre-\ndict four tags: PER (person name), LOC (location), ORG (organization name), and MISC \n(other names). As Flair showed adequate results in the extraction of names of individu-\nals, it was decided to apply the pre-trained 4-class CoNLL-03 Flair model to the training \ndataset. Entities that fell into the PER category were added as the IND annotation to the \ntraining corpus. Furthermore, we noticed that some funding information was partially cor -\nrectly extracted into the ORG and MISC categories. Therefore, WoS funding organization \nindexing and entities from the ORG and MISC categories were adopted and distinguished \nbetween three categories (FUND, COR, and UNI) using regular expressions. In addition, \nthe automatic classification of entities was manually examined and reviewed. Mismatched \ncategories, partially extracted entities, and not extracted entities were corrected. Acknowl-\nedged entities, which fall into the MISC category, were manually annotated by one annota-\ntor. In the miscellaneous category entities referring to names of the conferences and pro-\njects were included.\nExperiments\nIn the present paper, we evaluated three default Flair NER models with four differently-\nsized corpora. In total, we performed three experiments. In the first experiment, mod-\nels with the default parameter were evaluated using corpora Nos. 1 and 2. In the second \nexperiment, we evaluated Flair Embeddings and Transformers model with altered training \nparameters and corpus No.2. In the third experiment, the first experiment was replicated \nwith corpora Nos. 3 and 4.\nExperiment 1\nIn the first experiment, we tested the TARS model zero-shot and few-shot scenarios (with \ncorpus No. 1), as well as the performance of two default FLAIR models (Flair Embeddings \nand Transformers) with corpus No.2. Additionally, the performance of Flair Embeddings \nand Transformers models was tested with the corpus No.1 The training was conducted with \nthe recommended parameters for all algorithms, as Flair developers specifically ran vari-\nous tests to find the best hyperparameters for the default models. For the few-shot TARS, \nthe training was conducted with the small dataset (corpus No.1), and for Transformers and \nFlair Embeddings with a larger dataset (corpus No.2).\n13 https:// github. com/ flair NLP/ flairScientometrics \n1 3\nThe Flair Embeddings model was initiated as a combination of static and contextual \nstring embeddings. We applied GloVe (Pennington et  al., 2014) as a static word-level \nembedding model. Thus, in our case, stacked embeddings comprise GloVe embeddings, \nforward contextual string embeddings, and backward contextual string embeddings. The \nmodel was trained with the recommended parameters: the size of mini-batches was set to \n32 and the maximum number of epochs was set to 150.\nFor Transformers, training was initiated with the RoBERTa model (Liu et al., 2019). For \nthe present paper, a fine-tuning approach was used. The fine-tuning procedure consisted of \nadding a linear layer to a transformer and retraining the entire network with a small learn-\ning rate. We used a standard approach, where only a linear classifier layer was added on the \ntop of the transformer, as adding the additional CRF decoder between the transformer and \nlinear classifier did not increase accuracy compared with this standard approach (Schweter \n& Akbik, 2020). The chosen transformer model uses subword tokenization. We used the \nmean of embeddings of all subtokens and concatenation of all transformer layers to pro-\nduce embeddings. The context around the sentence was considered. The training was initi-\nated with a small learning rate using the Adam Optimisation Algorithm (Kingma & Ba, \n2014).\nThe TARS model requires labels to be defined in a natural language. Therefore, we \ntransformed our original coded labels into the natural language: FUND - “Funding \nAgency”, IND - “Person”, COR - “Corporation”, GRNB - “Grant Number\", UNI - “Uni-\nversity”, and MISC - “Miscellaneous”. The training for the few-shot approach was initiated \nwith the TARS NER model (Halder et al., 2020).\nResults\nOverall, the training demonstrated mixed results. Table  4 shows training results with cor -\npus No.1 and the TARS zero-shot approach. GRNB showed adequate results by training \nwith Flair Embeddings and TARSfew-shot models. IND was the best-recognized entity by \ntraining with Flair Embeddings and TARS (both zero- and few-shot) with an F1-score of \n0.8 (Flair Embeddings) and 0.86 (TARS) respectively. Training with Transformers was not \nsuccessful for IND with an F1-score of 0. In general, transformers were a less efficient \nalgorithm for training with a small dataset with an overall accuracy of 0.35. FUND dem-\nonstrated not satisfactory results with F1-score of less than 0.5 for all models. Entity types \nFig. 5  The training results with the training corpus No.2. A Comprises diagrams with the F1-scores of the \ntraining with three algorithms for each label class. B depicts the total accuracy of training algorithms Scientometrics\n1 3\nMISC, UNI, and COR showed the worst results with the F1-score equal to zero for all algo-\nrithms. The low accuracy for MISC, UNI, and COR resulted in low overall accuracy for \nall algorithms. Overall, training with corpus No.1 showed insufficient results for all algo-\nrithms. Flair Embeddings and TARS showed better accuracy compared to Transformers.\nFigure  5 shows the training results with corpus No.2. Similar to the training with corpus \nNo.1, IND and GRNB are the best-recognized categories. The best results for IND and \nGRNB demonstrated Flair embeddings with an F1-score of 0.98 (IND) and 0.96 (GRNB). \nTARS achieved the best results for FUND with an F1-score of 0.77 against 0.71 for Flair \nEmbeddings and 0.68 for Transformers. Miscellaneous demonstrated the worst accuracy \nfor Flair Embeddings (0.64) and Transformers (0.49), while for TARS the worst accuracy \nlies in the COR category with an F1-score of 0.54. The best result for UNI showed Flair \nEmbeddings with an F1-score over 0.7. The COR category showed a decent precision \nof 0.88 with Flair Embeddings but a low recall of 0.58 which resulted in a low F1-Score \n(0.7)14.\nTraining with corpus No.2 showed a significant improvement in training accuracy \n(Fig.  5B). Overall, Flair Embeddings was more accurate than other training algorithms, \nalthough training with TARS showed better results for the FUND category. The Trans-\nformers showed the worst results during training.\nAdditionally, a zero-shot approach was tested for the TARS model on corpus no.1. The \nmodel was able to successfully recognize individuals, but struggled with other categories, \nas Table 4 demonstrates. The total accuracy of the model comprises 0.23.\nExperiment 2\nOur first hypothesis to explain the pure model performance for the FUND, COR, MISC, \nand UNI categories is their semantic proximity that prevents successful recognition. \nEntities of these categories are often used in the same context. To examine this hypoth-\nesis, we conducted an experiment using Flair Embeddings with the dataset contain-\ning three types of entities: IND, GRNB, and ORG. The MISC category was excluded \nfrom the training, as one of the aims of the present research is to extract information \nabout acknowledged entities, and the MISC category contains only additional informa-\ntion. The new ORG category was established, which includes a combination of entities \nfrom the FUND, COR, and UNI categories. The training was performed with exactly \nthe same parameters as training with the Flair Embeddings model in Experiment 1 \n(Sect. 4.1).Table 4  F1-scores of the training with three algorithms for each label class with Corpus No. 1\nAlgorithm FUND GRNB IND UNI COR MISC accuracy\nTARS (zero-shot) 0.23 0.33 0.86 0 0 0 0.23\nTARS (few-shot) 0.32 0.76 0.86 0 0 0 0.35\nFlair embeddings 0.42 0.61 0.80 0 0 0 0.35\nTransformers 0.30 0.40 0 0 0 0 0.15\n14 Accuracy metrics by type of entity and total accuracy for all experiments can be found in Appendixes   A \nand BScientometrics \n1 3\nThe UNI and COR categories, though, have distinct patterns. In this case, the low \nperformance of the models for the COR and UNI categories could be explained by the \nsmall size of the training sample that contains these categories (see Fig.  2). Thus, the \nmodel was not able to identify patterns because of the lack of data.\nSecondly, low results for FUND, COR, MISC, and UNI categories might also lie in \nthe nature of the miscellaneous category, as some entities that fall into this category are \nsemantically very close to the FUND and COR categories. As a result, training without \na MISC category might potentially show better performance. To examine this hypoth-\nesis, we conducted training with Flair Embeddings with a dataset excluding the MISC \ncategory, i.e., with five entity types. Training results are shown in Fig. 6 A.\nAdditionally, the problem might lie in the nature of the training algorithms that \nwere used. On the one hand, Flair developers claimed Transformers to be the most effi-\ncient algorithm (Schweter & Akbik, 2020). On the other, the stacked embeddings are \nan important feature of the Flair tool, as a combination of different embeddings might \nbring better results than their separate uses (Akbik et  al., 2019). Thus, the combina-\ntion of the Transformer embeddings model with the contextual string embeddings might \nimprove the model performance. Thus, for the third additional training, we combined \ncontextual string embeddings with FLERT parameters.\nFig. 6  The results of Experiment 2. A–C comprise diagrams with the F1-scores of the training with three \nalgorithms for each label class. D Represents the total accuracy of the training algorithms Scientometrics\n1 3\nResults\nResults of the training are represented in Fig.  6. During the training with three types of \nentities (Fig.  6B) IND and GRNB still achieved high F1-scores of 0.96 (IND) and 0.95 \n(GRNB). Nevertheless, ORG gained only an F1-score of 0.64, which is worse than the \nprevious results with six entity types. The results of the training with five types of entities \nwere quite similar to those achieved during the training with six types of entities. FUND \nand UNI categories showed a small improvement in precision, recall, and F1 score com-\npared to training with 6 types of entities with Flair Embeddings. At the same time, the \nperformance of the COR category deteriorated noticeably (0.6 vs. the previous 0.7). The \nimprovement in overall accuracy (Fig.  6D) (0.80 vs. the previous 0.77) could be explained \nby the fact that the MISC category was not present in this training and could not affect \noverall accuracy with its low F1-score.\nAs Fig.  6C demonstrates, training with Flair Embeddings and RoBERTa showed no \nimprovements compared to the results of the primary training with Transformers and worse \nperformance compared with Flair Embeddings. As in Experiment 1, the COR category \nachieved high precision but low recall, resulting in a low F1-score (0.67). For some catego-\nries (COR and GRNB) Flair Embeddings combined with RoBERTa performed better than \nTransformers but still worse than Flair Embeddings.\nExperiment 3\nThe results of experiment 2 showed that altering the training parameters and decreasing the \nnumber of entity classes does not improve the model accuracy. We assume that increasing \nthe size of the training corpus would improve the performance of entities with low recogni-\ntion accuracy. Therefore, for this experiment, we designed two corpora with an increased \nnumber of acknowledged entities.\nAs the Flair Embeddings algorithm trained with Corpus No.2 showed the best perfor -\nmance, it was of interest if the increased training data will outperform its accuracy score. \nTraining in Experiments 1 and 2 was carried out using Flair version 0.9. As Flair recently \nupdated to version 0.11, we used this newest version for the following training. The training \nwas carried out with exactly the same parameters as the training with the Flair Embeddings \nFig. 7  The results of Experiment 3. A Comprises diagrams with the F1-scores of training with three cor -\npora for each label class. B Represents the total accuracy of the trainingScientometrics \n1 3\nmodel in Experiment 1 (Sect. 3.1). To achieve comparable results we also retrained, for \nnow, the best model (Flair Embeddings with Corpus No.2) with the Flair 0.11.\nResults\nResults of the training are represented in Fig.  7. Retraining of the original model with the \nFlair 0.11 Fig.  7-B showed slightly better performance (0.79 vs. 0.77) than training with \nversion 0.9. In general, no huge differences in accuracy were found during training with \nextended corpora.\nOverall, the best F1-Score for the FUND category (0.77) was reached with the TARS \nalgorithm and corpus No.2. COR gained the best accuracy (0.7) with Flair Embeddings \nand corpus No.2 using Flair version 0.9. The GRNB category showed the best perfor -\nmance (0.96) with Flair Embeddings trained on the corpus with five types of entities \n(Flair Embeddings 5 Ent). The best F1-Score of the IND category was achieved with \nFlair Embeddings trained on corpus No.2 with Flair version 0.11. MISC performed the \nbest (0.66) with Flair Embeddings trained on Corpus No.4 with Flair version 0.11. The \nbest accuracy of the UNI category was achieved with Flair Embeddings trained on corpus \nNo.3 with Flair version 0.11. In general, the best overall accuracy of 0.79 (for six entity \ntypes) had the Flair Embeddings model trained on corpus No.2 with Flair version 0.11.\nDiscussion\nAs expected, Experiment 1 showed a large improvement in accuracy for all algorithms \nwhen the size of a training corpus was increased from 49 to 654 sentences. However, fur -\nther enlargement of the corpus (in Experiment 3) did not make any progress. Some types \nof entity, such as IND and GRNB, showed great performance (GRNB with an F1-Score of \n0.96 or IND with 0.98) with the small training samples, i.e., 354 entities from the GRNB \ncategory or 439 entities from the IND category. At the same time, training with a sample of \n1322 labelled funding organisations achieved an F1-Score of only 0.75.\nThe TARS model is designed to perform NER with small or no training data. In \nexperiment 1, TARS without training data was able to extract individuals with quite high \naccuracy (F-1 score of 0.86). TARS trained with the small corpus (No. 1) did not show \nimprovement in the F-1 score of individuals, but greatly improved the F-1 score of the \nGRNB category. For other entity types, this model showed extremely weak results. It was \nexpected that training with Flair Embeddings and Transformers will not bring high recog-\nnition accuracy with corpus No.1, however, interesting results can be observed. Thus, Flair \nEmbeddings showed decent accuracy of 0.8 for individuals with the small training dataset.\nThe imbalance in the performance of different types of entities can be explained by the \nnature of the data, on which the original models were trained. Thus, Flair Embeddings were \ntrained on the 1-billion words English corpus (Chelba et al., 2013). RoBERTa was pre-trained \non the combination of five datasets containing news articles, blog entries, books, and Wikipe-\ndia articles. TARS was mainly pre-trained on datasets for text classification. Thus, the models \nused were not trained on domain-specific data. This can also explain the pure Transformers \nand TARS performance. The higher accuracy for the individuals category in the training with \nTARS can be explained by the fact, that the word ’person’ is semantically more straightforward \nthan other categories. The same could be applied to grant numbers. Furthermore, grant num-\nbers generally have similar patterns, which can be applied to all entities of this type, that can  Scientometrics\n1 3\nexplain a rapid improvement in F-1 score between zero-shot and few-shot models. Moreover, \nIND and GRNB categories showed better performance for other algorithms too, which could \nlie in the structure of these entities: names of individuals and grant numbers usually have undi-\nversified patterns and in acknowledgement texts are used in a small variety of contexts. At the \nsame time, other entity types, such as funding organisations and universities could have similar \npatterns and could be used in the same context. In some cases, even for human annotators, it \nis impossible to distinguish between university, funding body and corporation without back -\nground knowledge about the entity.\nPrevious works showed improvements in downstream tasks using embedding models \nfine-tuned for the domain used (Shen et al., 2022; Beltagy et al.., 2019). Therefore, fine-\ntuning the general language model on the sample of acknowledgment texts could improve \nthe performance of the NER model for acknowledgment texts. We are planning to fine-tune \nBERT and Flair Embeddings (contextual string embeddings) on a sample of approx. 5 mil-\nlion acknowledgment texts from WoS and evaluate the performance of the NER models.\nThe results of Experiment 2 generally did not show an improvement in accuracy. On the \ncontrary, training with the three entity types deteriorated the model performance. Train-\ning without the MISC category did not show significant performance progress either. \nMoreover, further analysis of acknowledged entities showed that the miscellaneous cate-\ngory contained very inhomogeneous and partly irrelevant data, making the analysis more \ncomplicated (Smirnova & Mayr, 2023). Therefore, we assume that the model would make \nbetter predictions if the number of entity types is expanded and miscellaneous categories \nexcluded, i.e., the MISC category could be split into the following categories: names of \nprojects, names of conferences, names of software and dataset. Different subcategories \ncould also be distinguished in the FUND category.\nCorpora No.2 and No.3 contain the same number of MISC and COR entities15, while \nin corpus 4 number of occurrences of MISC and COR entities is higher. For MISC and \nCOR, accuracy slightly increased with corpus 4, therefore we assume that the extraction \naccuracy for these entities will increase with the increase of the training data. The situa-\ntion is different for funding organizations and universities. The number of UNI and FUND \nentities increased evenly from corpus No.1 to corpus No.4. Nevertheless, the best result for \nthe UNI category was achieved with corpus No.3. The poor performance of corpus No.4 \ncould be explained by the inclusion of Indian funders. Thus, the names of many Indian \nfunders are very similar to the entities which usually fall into the UNI category, e.g., the \nDepartment of Science and Technology or the Department of Biotechnology. This pattern \nis more common to the entities which fall into the UNI category. Therefore, that might \nmake the exact extraction of UNI and FUND entities more confusing. Moreover, many \nIndian Universities contain the name of individuals, e.g., Rajiv Gandhi University, which \ncan cause confusion of the UNI category with the IND category. Generally, no improve-\nment in increasing the size of the corpus for the FUND category can be explained by the \nambiguous nature of the entities which fall into the FUND category and their semantical \nproximity with other types of entities. Analysis of the extracted entities showed that many \nentities were extracted correctly, but were assigned to the wrong category (Smirnova & \nMayr, 2023). Therefore, an additional classification algorithm applied to extracted entities \ncould improve the model’s performance.\n15 These differences in entity distribution are caused by the peculiarities of acknowledgement information \nstored in WoS. As only acknowledgements with indexed funding information are stored in the database, it \nwas difficult to find an adequate number of acknowledged entities of other typesScientometrics \n1 3\nConclusion\nIn this paper, we evaluated different embedding models for the task of automatic extraction \nand classification of acknowledged entities from acknowledgment texts16. The annotation \nof the training corpora was the most challenging and time-consuming task of all data prep-\naration procedures. Therefore, a semi-automated approach was used to help significantly \naccelerate the procedure.\nThe study’s main limitations were its small size and just one annotator of the training \ncorpora. Additionally, we used acknowledgments texts collected in WoS. WoS only stores \nacknowledgments containing funding information, therefore there was a lack of other types \nof entities, such as corporations or universities in the training data.\nIn the present paper, we aimed to answer three questions. Thus, regarding research ques-\ntion 1, the few-shot and zero-shot models showed very low total recognition accuracy. At \nthe same time, it was observed that some entities performed better than others with all \nalgorithms and training corpora. Thus, individuals gained a good F1-score over 0.8 with \nzero-shot and few-shot models, as also with Flair embeddings trained with the smallest \ncorpus. With the enlargement of the training corpora, the performance of the IND category \nalso increased and achieved an F1-score over 0.9. The GRNB category showed an adequate \nF-1 score of 0.76 with the few-shot algorithm trained with the smallest corpus, following \ntraining with corpus No.2 boosts the F-1 score to over 0.9. Therefore, few-shot and zero-\nshot approaches were not able to identify all the defined acknowledged entity classes.\nWith respect to research question 2, Flair Embeddings showed the best accuracy in \ntraining with corpus No.2 (and version 0.11) and the fastest training time compared to the \nother models; thus, it is recommended to further use the Flair Embeddings model for the \nrecognition of acknowledged entities.\nExploring research question 3 we observed, that the expansion of the size of a training \ncorpus from very small (corpus No.1) to medium size (corpus No.2) massively increased the \naccuracy of all training algorithms. The best-performing model (Flair Embedding) was further \nretrained with the two bigger corpora, but the following expansion of the training corpus did \nnot bring further improvement. Moreover, the performance of the model slightly deteriorated.\nAcknowledgements The original work was funded by the German Center for Higher Education Research \nand Science Studies (DZHW) via the project “Mining Acknowledgement Texts in Web of Science \n(MinAck)”17. Access to the WoS data was granted via the Competence Centre for Bibliometrics18. Data \naccess was funded by BMBF (Federal Ministry of Education and Research, Germany) under grant number \n01PQ17001. Nina Smirnova received funding from the German Research Foundation (DFG) via the project \n“POLLUX”19. The present paper is an extended version of the paper “Evaluation of Embedding Models for \nAutomatic Extraction and Classification of Acknowledged Entities in Scientific Documents” (Smirnova & \nMayr, 2022) presented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Sci-\nentific Documents (EEKE2022).\nAppendix A: Accuracy metrics by type of entity (label) for all \nexperiments\nSee Table 5.\n16 The best model can be tested  at https:// huggi ngface. co/ kalaw inka/ flair- ner- ackno  wledg ments\n17 https:// kalaw inka. github. io/ minack/.\n18 https:// www. bibli ometr  ie. info/ en/ index. php? id= home.\n19 https:// www. pollux- fid. de/ about. Scientometrics\n1 3\nTable 5  Accuracy metrics by type of entity (label) for all experiments\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings No.1 9 IND 0.7692 0.8333 0.8000 12 1\nFlair embeddings No.1 9 GRNB 0.5385 0.7000 0.6087 10 1\nFlair embeddings No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nFlair embeddings No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nFlair embeddings No.1 9 COR 0.0000 0.0000 0.0000 1 1\nFlair embeddings No.1 9 FUND 0.4000 0.4444 0.4211 18 1\nFlair embeddings No.2 9 FUND 0.6524 0.7771 0.7093 157 1\nFlair embeddings No.2 9 IND 0.9764 0.9831 0.9797 295 1\nFlair embeddings No.2 9 GRNB 0.9398 0.9750 0.9571 160 1\nFlair embeddings No.2 9 UNI 0.7527 0.7071 0.7292 99 1\nFlair embeddings No.2 9 MISC 0.6420 0.6341 0.6380 82 1\nFlair embeddings No.2 9 COR 0.8750 0.5833 0.7000 12 1\nTARS (pretrained) No.1 9 IND 1.0000 0.7500 0.8571 12 1\nTARS (pretrained) No.1 9 GRNB 0.7273 0.8000 0.7619 10 1\nTARS (pretrained) No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTARS (pretrained) No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTARS (pretrained) No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTARS (pretrained) No.1 9 FUND 0.3158 0.3333 0.3243 18 1\nTARS (pretrained) No.2 9 FUND 0.7257 0.8089 0.7651 157 1\nTARS (pretrained) No.2 9 IND 0.9281 0.8746 0.9005 295 1\nTARS (pretrained) No.2 9 GRNB 0.8895 0.9563 0.9217 160 1\nTARS (pretrained) No.2 9 UNI 0.7407 0.6061 0.6667 99 1\nTARS (pretrained) No.2 9 MISC 0.6719 0.5244 0.5890 82 1\nTARS (pretrained) No.2 9 COR 0.5000 0.5833 0.5385 12 1\nTransformers No.1 9 GRNB 0.3000 0.6000 0.4000 10 1\nTransformers No.1 9 IND 0.0000 0.0000 0.0000 12 1\nTransformers No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTransformers No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTransformers No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTransformers No.1 9 FUND 0.2414 0.3889 0.2979 18 1\nTransformers No.2 9 FUND 0.6211 0.7516 0.6801 157 1\nTransformers No.2 9 IND 0.9346 0.9695 0.9517 295 1\nTransformers No.2 9 GRNB 0.8704 0.8812 0.8758 160 1\nTransformers No.2 9 UNI 0.6476 0.6869 0.6667 99 1\nTransformers No.2 9 MISC 0.4767 0.5000 0.4881 82 1\nTransformers No.2 9 COR 0.7500 0.5000 0.6000 12 1\nFlair embeddings (3 Ent) No.2 9 IND 0.9577 0.9703 0.9639 303 2\nFlair embeddings (3 Ent) No.2 9 ORG 0.6400 0.6154 0.6275 208 2\nFlair embeddings (3 Ent) No.2 9 GRNB 0.9286 0.9750 0.9512 160 2\nFlair embeddings (5 Ent) No.2 9 IND 0.9764 0.9797 0.9780 295 2\nFlair embeddings (5 Ent) No.2 9 GRNB 0.9345 0.9812 0.9573 160 2\nFlair embeddings (5 Ent) No.2 9 UNI 0.7802 0.7172 0.7474 99 2\nFlair embeddings (5 Ent) No.2 9 COR 0.7500 0.5000 0.6000 12 2\nFlair embeddings (5 Ent) No.2 9 FUND 0.6722 0.7707 0.7181 157 2Scientometrics \n1 3\nAppendix B: Overall accuracy for all experiments\nSee Table 6.Rows are sorted by experiment number and algorithmTable 5  (continued)\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings (RoB-\nERTa)No.2 9 IND 0.9206 0.9831 0.9508 295 2\nFlair embeddings (RoB-\nERTa)No.2 9 GRNB 0.8896 0.9062 0.8978 160 2\nFlair embeddings (RoB-\nERTa)No.2 9 UNI 0.5963 0.6566 0.6250 99 2\nFlair embeddings (RoB-\nERTa)No.2 9 MISC 0.4135 0.5244 0.4624 82 2\nFlair embeddings (RoB-\nERTa)No.2 9 COR 1.0000 0.5000 0.6667 12 2\nFlair embeddings (RoB-\nERTa)No.2 9 FUND 0.6096 0.7261 0.6628 157 2\nFlair embeddings No.2 11 GRNB 0.9345 0.9812 0.9573 160 3\nFlair embeddings No.2 11 IND 0.9797 0.9831 0.9814 295 3\nFlair embeddings No.2 11 FUND 0.7027 0.8280 0.7602 157 3\nFlair embeddings No.2 11 UNI 0.7684 0.7374 0.7526 99 3\nFlair embeddings No.2 11 MISC 0.6543 0.6463 0.6503 82 3\nFlair embeddings No.2 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 UNI 0.8000 0.7273 0.7619 99 3\nFlair embeddings No.3 11 IND 0.9731 0.9797 0.9764 295 3\nFlair embeddings No.3 11 GRNB 0.9281 0.9688 0.9480 160 3\nFlair embeddings No.3 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 MISC 0.6571 0.5610 0.6053 82 3\nFlair embeddings No.3 11 FUND 0.6757 0.7962 0.7310 157 3\nFlair embeddings No.4 11 MISC 0.7424 0.5976 0.6622 82 3\nFlair embeddings No.4 11 COR 0.8571 0.5000 0.6316 12 3\nFlair embeddings No.4 11 UNI 0.7753 0.6970 0.7340 99 3\nFlair embeddings No.4 11 IND 0.9698 0.9797 0.9747 295 3\nFlair embeddings No.4 11 FUND 0.6823 0.8344 0.7507 157 3\nFlair embeddings No.4 11 GRNB 0.9162 0.9563 0.9358 160 3 Scientometrics\n1 3\nFunding Open access funding enabled and organized by Projekt DEAL.\nDeclarations \n Conflict of interest Philipp Mayr, the co-author of this paper, has a conflict of interest because he serves on \nthe editorial board of the journal Scientometrics. In addition, he is a co-guest editor of the special issue on \n\"Extraction and Evaluation of Knowledge Entities from Scientific Documents\". He declares that he has noth-\ning to do with the decision about this paper submission.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAkbik, A., Bergmann, T., Blythe, D., Rasul, K., Schweter, S., & Vollgraf, R. 2019. FLAIR: An Easy-to-Use \nFramework for State-of-the-Art NLP. Minneapolis, Minnesota (pp. 54–59). Association for Computa-\ntional Linguistics.\nAkbik, A., Blythe, D., & Vollgraf, R. (2018). Contextual string embeddings for sequence labeling. In 2018, \n27th International Conference on Computational Linguistics (pp. 1638–1649).\nAlexandera, D. & Vries, A. P. (2021). This research is funded by...”: Named Entity Recognition of financial \ninformation in research papers. In BIR 2021: 11th International Workshop on Bibliometric-enhanced \nInformation Retrieval at ECIR (pp. 102–110).\nBeltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. \nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing \nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)  \n(pp. 3613–3618). Association for Computational Linguistics.Table 6  Overall accuracy for all experiments\nRows are sorted by experiment number and algorithmAlgorithm Corpus Version Accuracy Experiment\nFlair embeddings No.2 9 0.7702 1\nFlair embeddings No.1 9 0.3472 1\nTARS (pretrained) No.2 9 0.7113 1\nTARS (pretrained) No.1 9 0.3485 1\nTransformers No.2 9 0.6783 1\nTransformers No.1 9 0.1477 1\nFlair Embeddings (3 Entity Types) No.2 9 0.7536 2\nFlair embeddings (5 Entity Types) No.2 9 0.7990 2\nFlair embeddings + RoBERTa No.2 9 0.6697 2\nFlair Embeddings No.2 11 0.7869 3\nFlair embeddings No.4 11 0.7814 3\nFlair embeddings No.3 11 0.7691 3Scientometrics \n1 3\nBorst, T., Mielck, J., Nannt, M., & Riese, W. (2022). Extracting funder information from scien-\ntific papers—Experiences with question answering. In Silvello, G., O.  Corcho, P.  Manghi, G.M. \nDi Nunzio, K. Golub, N. Ferro, and A. Poggi (Eds.),Linking theory and practice of digital libraries  \n(Vol. 13541, pp. 289–296). Springer International Publishing. Series Title: Lecture Notes in Com-\nputer Science. https:// doi. org/ 10. 1007/ 978-3- 031- 16802-4_ 24.\nChelba, C., T.  Mikolov, M.  Schuster, Q.  Ge, T.  Brants, P.  Koehn, & Robinson, T. (2013). One Bil-\nlion Word Benchmark for Measuring Progress in Statistical Language Modeling. 10.48550/\nARXIV.1312.3005 .\nChen, H., Song, X., Jin, Q., & Wang, X. (2022). Network dynamics in university-industry collaboration: \nA collaboration-knowledge dual-layer network perspective. Scientometrics, 127(11), 6637–6660. \nhttps:// doi. org/ 10. 1007/ s11192- 022- 04330-9\nCronin, B. (1995). The Scholar’s courtesy: The role of acknowledgement in the primary communication \nprocess. Taylor Graham.\nCronin, B., & Weaver, S. (1995). The praxis of acknowledgement: From bibliometrics to influmetrics. \nRevista Española de Documentación Científica, 18(2), 172.\nDevlin, J., Chang, M.W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional \ntransformers for language understanding. 10.48550/ARXIV.1810.04805 .\nDiaz-Faes, A. A., & Bordons, M. (2017). Making visible the invisible through the analysis of acknowl-\nedgements in the humanities. Aslib Journal of Information Management, 69(5), 576–590. https://  \ndoi. org/ 10. 1108/ AJIM- 01- 2017- 0008\nDoehne, M., & Herfeld, C. (2023). How academic opinion leaders shape scientific ideas: an acknowl-\nedgment analysis., 128(4), 2507–2533. https:// doi. org/ 10. 1007/ s11192- 022- 04623-z\nDzieżyc, M., & Kazienko, P. (2022). Effectiveness of research grants funded by European research coun-\ncil and polish national science centre. Journal of Informetrics, 16(1), 101243. https:// doi. org/ 10.  \n1016/j. joi. 2021. 101243\nEftimov, T., Koroušić Seljak, B., & Korošec, P. (2017). A rule-based named-entity recognition \nmethod for knowledge extraction of evidence-based dietary recommendations. PLoS ONE, 12(6), \ne0179488. https:// doi. org/ 10. 1371/ journ al. pone. 01794 88\nEtzioni, O., Cafarella, M., Downey, D., Popescu, A. M., Shaked, T., Soderland, S., Weld, D. S., & Yates, \nA. (2005). Unsupervised named-entity extraction from the web: An experimental study. Artificial \nIntelligence, 165(1), 91–134. https:// doi. org/ 10. 1016/j. artint. 2005. 03. 001\nFinkel, J.R., Grenager, T., & Manning, C. (2005). Incorporating non-local information into information \nextraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL’05), Ann Arbor, Michigan (pp. 363–370). Association for \nComputational Linguistics.\nGiles, C. L., & Councill, I. G. (2004). Who gets acknowledged: Measuring scientific contributions \nthrough automatic acknowledgment indexing. Proceedings of the National Academy of Sciences,  \n101(51), 17599–17604. https:// doi. org/ 10. 1073/ pnas. 04077 43101\nHalder, K., Akbik, A., Krapac, J., & Vollgraf, R. (2020). Task-Aware Representation of Sentences for \nGeneric Text Classification. In Proceedings of the 28th International Conference on Computational \nLinguistics, Barcelona, Spain (Online) (pp. 3202–3213). International Committee on Computa-\ntional Linguistics.\nHubbard, D., Laddusaw, S., Tan, Q., & Hu, X. (2022). Analysis of acknowledgments of libraries in the \njournal literature using machine learning. Proceedings of the Association for Information Science \nand Technology, 59(1), 709–711. https:// doi. org/ 10. 1002/ pra2. 698\nIovine, A., Fang, A., Fetahu, B., Rokhlenko, O., & Malmasi, S. (2022). CycleNER: An unsupervised \ntraining approach for named entity recognition. In Proceedings of the ACM Web Conference 2022  \n(pp. 2916–2924). ACM.\nJiang, L., Kang, X., Huang, S., & Yang, B. (2022). A refinement strategy for identification of scientific \nsoftware from bioinformatics publications. Scientometrics, 127(6), 3293–3316. https:// doi. org/ 10.  \n1007/ s11192- 022- 04381-y\nKassirer, J. P., & Angell, M. (1991). On authorship and acknowledgments. The New England Journal of \nMedicine, 325(21), 1510–1512. https:// doi. org/ 10. 1056/ NEJM1 99111 21325 2112\nKayal, S., Afzal, Z., Tsatsaronis, G., Katrenko, S., Coupet, P., Doornenbal, M., & Gregory, M. (2017). \nTagging funding agencies and grants in scientific articles using sequential learning models. In \nBioNLP 2017, Vancouver, Canada (pp. 216–221). Association for Computational Linguistics.\nKenekayoro, P. (2018). Identifying named entities in academic biographies with supervised learning. \nScientometrics, 116(2), 751–765. https:// doi. org/ 10. 1007/ s11192- 018- 2797-4\nKingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. 10.48550/\nARXIV.1412.6980 . Scientometrics\n1 3\nKusumegi, K., & Sano, Y. (2022). Dataset of identified scholars mentioned in acknowledgement state-\nments. Scientific Data, 9(1), 461. https:// doi. org/ 10. 1038/ s41597- 022- 01585-y\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoy -\nanov, V. (2019). RoBERTa: A robustly optimized BERT pretraining approach. arXiv: 1907. 11692  \n[cs] .\nMackintosh, K. (1972). Acknowledgements patterns in sociology. Ph. D. thesis, University of Oregon.\nMccain, K. (2017). Beyond Garfield’s citation index: An assessment of some issues in building a per -\nsonal name acknowledgments index. Scientometrics. https:// doi. org/ 10. 1007/ s11192- 017- 2598-1\nMcCain, K. W. (1991). Communication, competition, and secrecy: The production and dissemination of \nresearch-related information in genetics. Science, Technology, & Human Values, 16(4), 491–516. \nhttps:// doi. org/ 10. 1177/ 01622 43991 01600 404\nMejia, C., & Kajikawa, Y. (2018). Using acknowledgement data to characterize funding organizations \nby the types of research sponsored: the case of robotics research. Scientometrics, 114(3), 883–904. \nhttps:// doi. org/ 10. 1007/ s11192- 017- 2617-2\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, \nN., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkur -\nthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: An imperative style, high-\nperformance deep learning library. arXiv: 1912. 01703 [cs, stat].\nPaul-Hus, A., & Desrochers, N. (2019). Acknowledgements are not just thank you notes: A qualitative \nanalysis of acknowledgements content in scientific articles and reviews published in 2015. PLoS \nONE, 14, e0226727. https:// doi. org/ 10. 1371/ journ al. pone. 02267 27\nPaul-Hus, A., Díaz-Faes, A., Sainte-Marie, M., Desrochers, N., Costas, R., & Larivière, V. (2017). \nBeyond funding: Acknowledgement patterns in biomedical, natural and social sciences. PLoS ONE,  \n12, e0185578. https:// doi. org/ 10. 1371/ journ al. pone. 01855 78\nPennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors for Word Representation. \nIn Empirical Methods in Natural Language Processing (EMNLP) (pp. 1532–1543).\nPustejovsky, J., & Stubbs, A. (2012). Natural language annotation for machine learning. O’Reilly \nMedia Inc.\nRose, M., & Georg, C. P. (2021). What 5,000 acknowledgements tell us about informal collaboration \nin financial economics. Research Policy, 50, 104236. https:// doi. org/ 10. 1016/j. respol. 2021. 104236\nSang, T. K., & E. F., & De Meulder, F. (2003). Introduction to the CoNLL-2003 shared task: Language-\nindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Lan-\nguage Learning at HLT-NAACL (pp. 142–147).\nSchweter, S., & Akbik, A. (2020). FLERT: Document-level features for named entity recognition. ArXiv. \n10.48550/arXiv.2011.06993 .\nShen, S., Liu, J., Lin, L., Huang, Y., Zhang, L., Liu, C., Feng, Y., & Wang, D. (2022). SsciBERT: A \npre-trained language model for social science texts. Scientometrics. https:// doi. org/ 10. 1007/  \ns11192- 022- 04602-4\nSingh, V. K., Singh, P., Karmakar, M., Leta, J., & Mayr, P. (2021). The journal coverage of web of sci-\nence, scopus and dimensions: A comparative analysis. Scientometrics, 126(6), 5113–5142. https://  \ndoi. org/ 10. 1007/ s11192- 021- 03948-5\nSmirnova, N., & Mayr, P. (2022). Evaluation of embedding models for automatic extraction and classifi-\ncation of acknowledged entities in scientific documents. In 3rd Workshop on Extraction and Evalu-\nation of Knowledge Entities from Scientific Documents 2022 (EEKE 2022) (pp. 48–55). CEUR-WS.\norg.\nSmirnova, N., & Mayr, P. (2023). A comprehensive analysis of acknowledgement texts in web of sci-\nence: A case study on four scientific domains. Scientometrics, 1(128), 709–734. https:// doi. org/ 10.  \n1007/ s11192- 022- 04554-9\nSong, M., Kang, K. Y., Timakum, T., & Zhang, X. (2020). Examining influential factors for acknowl-\nedgements classification using supervised learning. PLoS ONE. https:// doi. org/ 10. 1371/ journ al.  \npone. 02289 28\nThomer, A. K., & Weber, N. M. (2014). Using named entity recognition as a classification heuristic. In \niConference 2014 Proceedings (pp. 1133 – 1138). iSchools.\nWang, J., & Shapira, P. (2011). Funding acknowledgement analysis: An enhanced tool to investigate \nresearch sponsorship impacts: The case of nanotechnology. Scientometrics, 87(3), 563–586. https://  \ndoi. org/ 10. 1007/ s11192- 011- 0362-5\nYamada, I., Asai, A., Shindo, H., Takeda, H., & Matsumoto, Y. (2020). LUKE: Deep contextualized \nentity representations with entity-aware self-attention. In Proceedings of the 2020 Conference on \nEmpirical Methods in Natural Language Processing (EMNLP) (pp. 6442–6454). Association for \nComputational Linguistics.Scientometrics \n1 3\nYu, J., Bohnet, B., & Poesio, M. (2020). Named entity recognition as dependency parsing. 10.48550/\nARXIV.2005.07150.\nZhang, C., Mayr, P., Lu, W., & Zhang, Y. (2023). Guest editorial: Extraction and evaluation of knowledge \nentities in the age of artificial intelligence. Aslib Journal of Information Management, 75, 433–437. \nhttps:// doi. org/ 10. 1108/ AJIM- 05- 2023- 507",
        "type": "direct"
    },
    {
        "citation": "J. P., 2022",
        "content": ".\nSupervised NER mainly relays on machine learning or deep learning methods. The \nstate-of-the-art models are based on deep recurrent models, convolution-based, or Scientometrics \n1 3\npre-trained transformer architectures",
        "type": "regular"
    },
    {
        "citation": "J. P. 2018",
        "content": ". Thus, Akbik et  al.",
        "type": "regular"
    },
    {
        "citation": "PLoS ONE 2018",
        "content": "proposed a new character-based contextual string embeddings method. This approach \npasses a sequence of characters through the character-level language model to gener -\nate word-level embeddings. The model was pre-trained on large unlabeled corpora. The \ntraining was carried out using a character-based neural language model together with a \nBidirectional LSTM (BiLSTM) sequence-labelling model. This approach generates dif-\nferent embeddings for the same word depending on its context and showed good results \non downstream tasks such as NER. Devlin et al.",
        "type": "regular"
    },
    {
        "citation": "Research Policy 2019",
        "content": "presented BERT (Bidirectional \nEncoder Representations Transformers), a transformer-based language representa-\ntion model that models the representation of contextualized word embeddings. BERT \nshowed superior results on downstream tasks using different benchmarking datasets. \nLater, Liu et al.",
        "type": "regular"
    },
    {
        "citation": "N. M. 2019",
        "content": "performed an optimization of the BERT model and introduced \nRoBERTa (Robustly Optimized BERT Pretraining Approach). RoBERTa was evaluated \non three benchmarks and demonstrated massive improvements over the reported BERT \nperformance.\nCurrently, several domain-specific models have been developed. Thus, Beltagy et  al..",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": "released SciBERT a BERT-based language model pre-trained on a large number \nof unlabeled scientific articles from the computer science and biomedical domains. SciB-\nERT showed improvements over BERT on several downstream NLP tasks, including NER. \nRecently, Shen et al.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2005",
        "content": "introduced the SsciBERT, a language model based on BERT \nand pre-trained on abstracts published in the Social Science Citation Index (SSCI) journals. \nThe model showed good results in discipline classification and abstract structure-function \nrecognition in articles from the social sciences domain.\nUnsupervised methods are often based on lexicons or predefined rules. Thus, Etzioni \net al.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2017",
        "content": "uses lists of patterns and domain-specific rules to extract named entities. Efti-\nmov et al.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": "developed a rule-based NER model to extract dietary information from \nscientific publications. Evaluation of the model performance showed good results. Opposed \nto previous unsupervised NER approaches, Iovine et al.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2018",
        "content": "proposed a cycle-consist-\nency approach for NER (CycleNER). CycleNER is unsupervised and does not require par -\nallel training data. The method showed 73% of supervised performance on CoNLL03.\nNER in scientometrics analysis\nNamed entities are widely used in scientometrics analysis. Thus, Kenekayoro",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": "devel-\noped a supervised method for the automatic extraction of named entities from academic \nbibliographies. The aim of the study was to create a database containing unified academic \ninformation about individuals to help in expert finding. A labeled training dataset was \ndeveloped using biographies extracted from ORCID.6 The authors tested several models \nfor NER. The Support Vector Machine classification algorithm (SVM) showed the best \nperformance.\nJiang et al.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": "proposed a strategy for the identification of software in scientific bio-\ninformatics publications using the combination of SVM and CRF (Conditional Random \nField). Application of the method to the sample of articles from bioinformatics domains \nallowed them to observe interesting patterns in using software in scientific research.\nKusumegi and Sano",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2004",
        "content": "analysed scholarly relationships by analysing acknowl-\nedged individuals from the acknowledgments statements from eight open-access journals. \n6 https:// orcid. org/. Scientometrics\n1 3\nIndividuals were extracted using the Stanford CoreNLP NER tagger. In the next steps, \nscholars were identified among the extracted individuals by mapping them to the Microsoft \nAcademic Graph (MAG).\nWe are aware of several works on automated information extraction from acknowledg-\nments. Giles and Councill",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2014",
        "content": "developed an automated method for the extraction and \nanalysis of acknowledgment texts using regular expressions and SVM. Computer science \nresearch papers from the CiteSeer digital library were used as a data source. Extracted enti-\nties were analysed and manually assigned to the following four categories: funding agen-\ncies, corporations, universities, and individuals.\nThomer and Weber",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2014, p. 1134",
        "content": "used the 4-class Stanford Entity Recognizer (Finkel et al., \n2005) to extract persons, locations, organizations, and miscellaneous entities from the col-\nlection of bioinformatics texts from PubMed Central’s Open Access corpus. The aim of \nthe study was to determine an approach to \"increase the speed of ... classification without \nsacrificing accuracy, nor reliability\" .\nKayal et  al. (2017) introduced a method for extraction of funding organizations and \ngrants from acknowledgment texts using a combination of sequential learning models: con-\nditional random fields (CRF), hidden markov models (HMM), and maximum entropy mod-\nels (MaxEnt). The final model contained pooled outputs from the models used.\nAlexandera and Vries (2021) proposed AckNER, a tool for extracting financial informa-\ntion from the funding or acknowledgment section of a research article. AckNER works \nwith the use of dependency parse trees and regular expressions and is able to extract names \nof the organisations, projects, programs, and funds, as also numbers of contracts and \ngrants7.\nFollowing, Borst et  al. (2022) applied a question-answering (QA) based approach to \nidentify funding information in acknowledgments texts. This approach performs similarly \nto AckNER and requires a smaller set of training and test data.\nTable  1 shows an overview of works on NER in scientometrics. Overall, previous works \non the extraction of named entities from acknowledgements texts were mostly concerned \nwith the extraction of funding information, i.e., only names of funding bodies and grant \nnumbers, or extraction and linking of individuals. The special issue by Zhang et al. (2023) \nprovided a recent overview of current works in the extraction of knowledge entities.\nTo the best of our knowledge the work of Giles and Councill (2004) is the only \nattempt to extract and categorise multiple acknowledged entities. Nevertheless, entities \nwere extracted using the SVM algorithm but the classification of entities themselves \nwas produced manually, which limited the number of acknowledgement texts to be ana-\nlysed. Furthermore, as far as we know, there was no research done concerning the eval-\nuation of embedding models for extraction of information from acknowledgement texts \nand no tool for automatic extraction of different kinds of acknowledged entities was \ndeveloped.\n7 AckNER showed better performance as Flair, but is specifically designed to recognize two types of \nacknowledged entities (Alexandera & Vries, 2021), which was insufficient for the present project.Scientometrics \n1 3\nTable 1  Overview of works on NER in scientometrics\nPaper Area of application and aim of \nthe studyCorpus Entities Methods and tools\nGiles and Councill (2004) Extraction of acknowledged enti-\nties form acknowledgementsCiteSeer Funding agencies, Companies, \nEducational Institutions, \nIndividualsSVM for extracting entities and \ntheir manual classification\nThomer and Weber (2014) Using NER to improve classifica-\ntion of acknowledgementsPubMed Central’s Open Access Persons, locations, organizations, \nand miscellaneous4-class Stanford Entity Recognizer\nKayal et al. (2017) Extraction of funding information \nfrom acknowledgementsPubMed Central’s Open Access Funding bodies, grants CRF, HMM, MaxEnt\nKenekayoro (2018) Extraction of biography informa-\ntion from academic biographiesORCID Award, Location, Organization, \nPerson, Position, Specializa-\ntion, OthersSVM\nAlexandera and Vries (2021) Extraction of funding information \nfrom acknowledgementsTU Delft’s institutional repository Funding bodies, grants SpaCy dependency parser + regu-\nlar expressions\nJiang et al. (2022) Extraction of scientific software \nfrom scientific articles (full \ntexts) in bioinformaticsbioinformatics journals EnsembleSVMs-CRF\nBorst et al. (2022) Extraction of funding information \nfrom acknowledgementsEconStor Funding bodies, grants Haystack\nKusumegi and Sano (2022) Extraction and linking of \nacknowledged individuals from \nacknowledgementsPLOS Individuals Stanford CoreNLP NER tagger + \nMAG Scientometrics\n1 3\nMethod\nIn the present paper, different models for extraction and classification of acknowledged \nentities supported by the Flair NLP framework were evaluated. The choice of classification \nwas inspired by Giles and Councill’s (2004) classification: funding agencies (FUND), cor -\nporations (COR), universities (UNI), and individuals (IND). For our project, this classifica-\ntion was enhanced with the miscellaneous (MISC) and grant numbers (GRNB) categories. \nThe GRNB category was adopted from WoS funding information indexing. The entities \nin the miscellaneous category could provide useful information, but cannot be ascribed to \nother categories, e.g., names of projects and names of conferences. Figure  1 demonstrates \nan example of acknowledged entities of different types. To the best of our knowledge, Giles \nand Councill’s classification is the only existing classification of acknowledged entities and \ntherefore can be applied to the NER task. Other works on acknowledgment analysis were \nfocused on the classification of acknowledgment texts.\nThe Flair NLP framework\nFlair is an open-sourced NLP framework built on PyTorch (Paszke et  al., 2019), which \nis an open-source machine learning library. “The core idea of the framework is to pre-\nsent a simple, unified interface for conceptually very different types of word and document \nembeddings” (Akbik et  al., 2019, p.  54). Flair has three default training algorithms for \nNER which were used for the first experiment in the present research: a) NER Model with \nFlair Embeddings (later on Flair Embeddings) (Akbik et al., 2018), b) NER Model with \nTransformers (later on Transformers) (Schweter & Akbik, 2020), and c) Zero-shot NER \nwith TARS (later on TARS) (Halder et al., 2020) 8.\nThe Flair Embeddings model uses stacked embeddings, i.e., a combination of contex-\ntual string embeddings (Akbik et al., 2018) with a static embeddings model. This approach \nwill generate different embeddings for the same word depending on its context. Stacked \nembedding is an important Flair feature, as a combination of different embeddings might \nbring better results than their separate uses (Akbik et al., 2019).\nThe Transformers model or FLERT-extension (document-level features for NER) is a \nset of settings to perform a NER on the document level using fine-tuning and feature-based \nFig. 1  An example of acknowledged entities. Each entity type is marked with a distinct color\n8 New transformer models as SciBERT or SsciBERT were not evaluated in this study, as the objective of \nthe study is to evaluate the performance of the Flair default models.Scientometrics \n1 3\nLSTM-CRF with the multilingual XML-RoBERTa transformer model (Schweter & Akbik, \n2020).\nThe TARS (task-aware representation of sentences) is a transformer-based model, \nwhich allows performing training without any training data (zero-shot learning) or with a \nsmall dataset (few-short learning) (Halder et al., 2020). The TARS approach differs from \nthe traditional transfer learning approach in the way that the TARS model also considers \nsemantic information captured in the class labels themselves. For example, for analyzing \nacknowledgments, class labels like funding organization or university already carry seman-\ntic information.\nTraining data\nThe Web of Science (WoS) database was used to harvest the training data (funding \nacknowledgments).9 From 2008 on, WoS started indexing information about funders and \ngrants. WoS uses information from different funding reporting systems such as Research-\nfish,10 Medline11 and others. As WoS contains millions of metadata records (Singh et al., \n2021), the data chosen for the present study was restricted by year and scientific domain \n(for the corpora Nos. 1, 2, and 3) or additionally by the affiliation country (for corpus No.4). \nTo construct corpora Nos. 1-3 records from four different scientific domains published \nfrom 2014 to 2019 were considered: two domains from the social sciences (sociology and \neconomics) and oceanography and computer science. Different scientific domains were Table 2  Number of sentences/texts in the training corpora\nCorpus No. Training set (train) Test set (test) Validation set (dev) Total\n1 29/27 10/10 10/10 49/47\n2 339/282 165/150 150/136 654/441\n3 784/657 165/150 150/136 1099/816\n4 1148/885 165/150 150/136 1463/1044\nTable 3  Number of sentences/texts from each scientific domain in the training corpora\nCorpus No. Oceanography Economics Social Sciences Computer Science\n1 13/13 3/3 20/20 16/14\n2 127/75 92/58 351/234 173/129\n3 175/112 128/89 590/434 333/269\n9 The present research was conducted in scopes of two projects: MinAck (https:// kalaw inka. github. io/ \nminack/) and SEASON (https:// github. com/ kalaw inka/ season). Corpora Nos.1, 2, and 3 were created for the \nMinAck project and serve the purpose of a general evaluation of the impact of the size of the training cor -\npus on the model performance. Corpus No.4 was designed specifically for the SEASON project in the hope \nof improving the recognition of Indian funding information. The project SEASON aims to get insight into \nGerman-Indian scientific collaboration. Our other corpora mainly contain papers published by European \ninstitutions. That is why we enhance Corpus 4 with the papers published by Indian institutions.\n10 https:// resea rchfi sh. com/.\n11 https:// www. nlm. nih. gov/ bsd/ fundi ng_ suppo rt. html. Scientometrics\n1 3\nchosen since previous work on acknowledgment analysis revealed the relations between \nthe scientific domain and the types of acknowledged entities, i.e., acknowledged individu-\nals are more characteristic of theoretical and social-oriented domains. At the same time, \ninformation on technical and instrumental support is more common for the natural and \nlife sciences domains (Diaz-Faes & Bordons, 2017). Only the WoS record types “article” \nand “review” published in a scientific journal in English were selected; then 1000 distinct \nacknowledgments texts were randomly gathered from this sample for the training dataset. \nFurther different amounts of sentences containing acknowledged entities were distributed \ninto the differently-sized training corpora. Table  2 demonstrates the number of sentences \nin each set in the four corpora. We selected only sentences that contain an acknowledged \nentity, regardless of the scientific domain. Table  3 contains the number of sentences and \ntexts from each scientific domain in the training corpora.12 The same article can belong to \nseveral scientific domains, therefore, the number of sentences and texts in Tables  2 and 3 \ndoes not match. Corpus No.4 was designed in such a way that all the training data from the \nCorpus No.3 was enhanced with acknowledgments texts from the articles that have Indian \naffiliations regardless of scientific domain or publication date.\nPreliminary analysis of the WoS data showed that the indexing of WoS funding infor -\nmation has several issues. The WoS includes only acknowledgments containing funding \ninformation; therefore, not every WoS entry has an acknowledgment, individuals are not \nincluded, and indexed funding organizations are not divided into different entity types like \nuniversities, corporations, etc. Therefore, the existing indexing of funding organizations \nis incomplete. Furthermore, there is a disproportion between the occurrences of acknowl-\nedged entities of different types. Thus, the most frequent entity types in the dataset with \nthe training data are IND, FUND and GRNB, followed by UNI and MISC. COR is the \nFig. 2  The distribution of acknowledged entities in the training corpora\n12 Corpus No.4 is not in Table  2, because the corpus contains additional acknowledgment texts from arti-\ncles with Indian affiliations regardless of the scientific domain and therefore contains different scientific \ndomains.Scientometrics \n1 3\ncategory most underrepresented in the data set. Consequently, there are different amounts \nof entities of different types in the training corpora (as Fig.  2 demonstrates), which might \nhave influenced the training results. Training with the corpora Nos. 2, 3, and 4 was evalu-\nated on the same training and validation datasets to ensure plausible accuracy (Fig.  3-B). \nFig. 3  The distribution of acknowledged entities in the test and validation corpora\nFig. 4  Annotation flowchart Scientometrics\n1 3\nHowever, training with corpus No.1 was evaluated with the smaller test and validation sets, \nas corpus No.1 contains a smaller number of sentences (Fig. 3-A).\nData annotation\nThe training corpus was annotated with six types of entities. As WoS already contains \nsome indexed funding information, it was decided to develop a semi-automated approach \nfor data annotation (as Fig.  4 demonstrates) and use indexed information provided by WoS, \ntherefore, grant numbers were adopted from the WoS indexing unaltered.\nFlair has a pre-trained 4-class NER Flair model (CoNLL-03).13 The model can pre-\ndict four tags: PER (person name), LOC (location), ORG (organization name), and MISC \n(other names). As Flair showed adequate results in the extraction of names of individu-\nals, it was decided to apply the pre-trained 4-class CoNLL-03 Flair model to the training \ndataset. Entities that fell into the PER category were added as the IND annotation to the \ntraining corpus. Furthermore, we noticed that some funding information was partially cor -\nrectly extracted into the ORG and MISC categories. Therefore, WoS funding organization \nindexing and entities from the ORG and MISC categories were adopted and distinguished \nbetween three categories (FUND, COR, and UNI) using regular expressions. In addition, \nthe automatic classification of entities was manually examined and reviewed. Mismatched \ncategories, partially extracted entities, and not extracted entities were corrected. Acknowl-\nedged entities, which fall into the MISC category, were manually annotated by one annota-\ntor. In the miscellaneous category entities referring to names of the conferences and pro-\njects were included.\nExperiments\nIn the present paper, we evaluated three default Flair NER models with four differently-\nsized corpora. In total, we performed three experiments. In the first experiment, mod-\nels with the default parameter were evaluated using corpora Nos. 1 and 2. In the second \nexperiment, we evaluated Flair Embeddings and Transformers model with altered training \nparameters and corpus No.2. In the third experiment, the first experiment was replicated \nwith corpora Nos. 3 and 4.\nExperiment 1\nIn the first experiment, we tested the TARS model zero-shot and few-shot scenarios (with \ncorpus No. 1), as well as the performance of two default FLAIR models (Flair Embeddings \nand Transformers) with corpus No.2. Additionally, the performance of Flair Embeddings \nand Transformers models was tested with the corpus No.1 The training was conducted with \nthe recommended parameters for all algorithms, as Flair developers specifically ran vari-\nous tests to find the best hyperparameters for the default models. For the few-shot TARS, \nthe training was conducted with the small dataset (corpus No.1), and for Transformers and \nFlair Embeddings with a larger dataset (corpus No.2).\n13 https:// github. com/ flair NLP/ flairScientometrics \n1 3\nThe Flair Embeddings model was initiated as a combination of static and contextual \nstring embeddings. We applied GloVe (Pennington et  al., 2014) as a static word-level \nembedding model. Thus, in our case, stacked embeddings comprise GloVe embeddings, \nforward contextual string embeddings, and backward contextual string embeddings. The \nmodel was trained with the recommended parameters: the size of mini-batches was set to \n32 and the maximum number of epochs was set to 150.\nFor Transformers, training was initiated with the RoBERTa model (Liu et al., 2019). For \nthe present paper, a fine-tuning approach was used. The fine-tuning procedure consisted of \nadding a linear layer to a transformer and retraining the entire network with a small learn-\ning rate. We used a standard approach, where only a linear classifier layer was added on the \ntop of the transformer, as adding the additional CRF decoder between the transformer and \nlinear classifier did not increase accuracy compared with this standard approach (Schweter \n& Akbik, 2020). The chosen transformer model uses subword tokenization. We used the \nmean of embeddings of all subtokens and concatenation of all transformer layers to pro-\nduce embeddings. The context around the sentence was considered. The training was initi-\nated with a small learning rate using the Adam Optimisation Algorithm (Kingma & Ba, \n2014).\nThe TARS model requires labels to be defined in a natural language. Therefore, we \ntransformed our original coded labels into the natural language: FUND - “Funding \nAgency”, IND - “Person”, COR - “Corporation”, GRNB - “Grant Number\", UNI - “Uni-\nversity”, and MISC - “Miscellaneous”. The training for the few-shot approach was initiated \nwith the TARS NER model (Halder et al., 2020).\nResults\nOverall, the training demonstrated mixed results. Table  4 shows training results with cor -\npus No.1 and the TARS zero-shot approach. GRNB showed adequate results by training \nwith Flair Embeddings and TARSfew-shot models. IND was the best-recognized entity by \ntraining with Flair Embeddings and TARS (both zero- and few-shot) with an F1-score of \n0.8 (Flair Embeddings) and 0.86 (TARS) respectively. Training with Transformers was not \nsuccessful for IND with an F1-score of 0. In general, transformers were a less efficient \nalgorithm for training with a small dataset with an overall accuracy of 0.35. FUND dem-\nonstrated not satisfactory results with F1-score of less than 0.5 for all models. Entity types \nFig. 5  The training results with the training corpus No.2. A Comprises diagrams with the F1-scores of the \ntraining with three algorithms for each label class. B depicts the total accuracy of training algorithms Scientometrics\n1 3\nMISC, UNI, and COR showed the worst results with the F1-score equal to zero for all algo-\nrithms. The low accuracy for MISC, UNI, and COR resulted in low overall accuracy for \nall algorithms. Overall, training with corpus No.1 showed insufficient results for all algo-\nrithms. Flair Embeddings and TARS showed better accuracy compared to Transformers.\nFigure  5 shows the training results with corpus No.2. Similar to the training with corpus \nNo.1, IND and GRNB are the best-recognized categories. The best results for IND and \nGRNB demonstrated Flair embeddings with an F1-score of 0.98 (IND) and 0.96 (GRNB). \nTARS achieved the best results for FUND with an F1-score of 0.77 against 0.71 for Flair \nEmbeddings and 0.68 for Transformers. Miscellaneous demonstrated the worst accuracy \nfor Flair Embeddings (0.64) and Transformers (0.49), while for TARS the worst accuracy \nlies in the COR category with an F1-score of 0.54. The best result for UNI showed Flair \nEmbeddings with an F1-score over 0.7. The COR category showed a decent precision \nof 0.88 with Flair Embeddings but a low recall of 0.58 which resulted in a low F1-Score \n(0.7)14.\nTraining with corpus No.2 showed a significant improvement in training accuracy \n(Fig.  5B). Overall, Flair Embeddings was more accurate than other training algorithms, \nalthough training with TARS showed better results for the FUND category. The Trans-\nformers showed the worst results during training.\nAdditionally, a zero-shot approach was tested for the TARS model on corpus no.1. The \nmodel was able to successfully recognize individuals, but struggled with other categories, \nas Table 4 demonstrates. The total accuracy of the model comprises 0.23.\nExperiment 2\nOur first hypothesis to explain the pure model performance for the FUND, COR, MISC, \nand UNI categories is their semantic proximity that prevents successful recognition. \nEntities of these categories are often used in the same context. To examine this hypoth-\nesis, we conducted an experiment using Flair Embeddings with the dataset contain-\ning three types of entities: IND, GRNB, and ORG. The MISC category was excluded \nfrom the training, as one of the aims of the present research is to extract information \nabout acknowledged entities, and the MISC category contains only additional informa-\ntion. The new ORG category was established, which includes a combination of entities \nfrom the FUND, COR, and UNI categories. The training was performed with exactly \nthe same parameters as training with the Flair Embeddings model in Experiment 1 \n(Sect. 4.1).Table 4  F1-scores of the training with three algorithms for each label class with Corpus No. 1\nAlgorithm FUND GRNB IND UNI COR MISC accuracy\nTARS (zero-shot) 0.23 0.33 0.86 0 0 0 0.23\nTARS (few-shot) 0.32 0.76 0.86 0 0 0 0.35\nFlair embeddings 0.42 0.61 0.80 0 0 0 0.35\nTransformers 0.30 0.40 0 0 0 0 0.15\n14 Accuracy metrics by type of entity and total accuracy for all experiments can be found in Appendixes   A \nand BScientometrics \n1 3\nThe UNI and COR categories, though, have distinct patterns. In this case, the low \nperformance of the models for the COR and UNI categories could be explained by the \nsmall size of the training sample that contains these categories (see Fig.  2). Thus, the \nmodel was not able to identify patterns because of the lack of data.\nSecondly, low results for FUND, COR, MISC, and UNI categories might also lie in \nthe nature of the miscellaneous category, as some entities that fall into this category are \nsemantically very close to the FUND and COR categories. As a result, training without \na MISC category might potentially show better performance. To examine this hypoth-\nesis, we conducted training with Flair Embeddings with a dataset excluding the MISC \ncategory, i.e., with five entity types. Training results are shown in Fig. 6 A.\nAdditionally, the problem might lie in the nature of the training algorithms that \nwere used. On the one hand, Flair developers claimed Transformers to be the most effi-\ncient algorithm (Schweter & Akbik, 2020). On the other, the stacked embeddings are \nan important feature of the Flair tool, as a combination of different embeddings might \nbring better results than their separate uses (Akbik et  al., 2019). Thus, the combina-\ntion of the Transformer embeddings model with the contextual string embeddings might \nimprove the model performance. Thus, for the third additional training, we combined \ncontextual string embeddings with FLERT parameters.\nFig. 6  The results of Experiment 2. A–C comprise diagrams with the F1-scores of the training with three \nalgorithms for each label class. D Represents the total accuracy of the training algorithms Scientometrics\n1 3\nResults\nResults of the training are represented in Fig.  6. During the training with three types of \nentities (Fig.  6B) IND and GRNB still achieved high F1-scores of 0.96 (IND) and 0.95 \n(GRNB). Nevertheless, ORG gained only an F1-score of 0.64, which is worse than the \nprevious results with six entity types. The results of the training with five types of entities \nwere quite similar to those achieved during the training with six types of entities. FUND \nand UNI categories showed a small improvement in precision, recall, and F1 score com-\npared to training with 6 types of entities with Flair Embeddings. At the same time, the \nperformance of the COR category deteriorated noticeably (0.6 vs. the previous 0.7). The \nimprovement in overall accuracy (Fig.  6D) (0.80 vs. the previous 0.77) could be explained \nby the fact that the MISC category was not present in this training and could not affect \noverall accuracy with its low F1-score.\nAs Fig.  6C demonstrates, training with Flair Embeddings and RoBERTa showed no \nimprovements compared to the results of the primary training with Transformers and worse \nperformance compared with Flair Embeddings. As in Experiment 1, the COR category \nachieved high precision but low recall, resulting in a low F1-score (0.67). For some catego-\nries (COR and GRNB) Flair Embeddings combined with RoBERTa performed better than \nTransformers but still worse than Flair Embeddings.\nExperiment 3\nThe results of experiment 2 showed that altering the training parameters and decreasing the \nnumber of entity classes does not improve the model accuracy. We assume that increasing \nthe size of the training corpus would improve the performance of entities with low recogni-\ntion accuracy. Therefore, for this experiment, we designed two corpora with an increased \nnumber of acknowledged entities.\nAs the Flair Embeddings algorithm trained with Corpus No.2 showed the best perfor -\nmance, it was of interest if the increased training data will outperform its accuracy score. \nTraining in Experiments 1 and 2 was carried out using Flair version 0.9. As Flair recently \nupdated to version 0.11, we used this newest version for the following training. The training \nwas carried out with exactly the same parameters as the training with the Flair Embeddings \nFig. 7  The results of Experiment 3. A Comprises diagrams with the F1-scores of training with three cor -\npora for each label class. B Represents the total accuracy of the trainingScientometrics \n1 3\nmodel in Experiment 1 (Sect. 3.1). To achieve comparable results we also retrained, for \nnow, the best model (Flair Embeddings with Corpus No.2) with the Flair 0.11.\nResults\nResults of the training are represented in Fig.  7. Retraining of the original model with the \nFlair 0.11 Fig.  7-B showed slightly better performance (0.79 vs. 0.77) than training with \nversion 0.9. In general, no huge differences in accuracy were found during training with \nextended corpora.\nOverall, the best F1-Score for the FUND category (0.77) was reached with the TARS \nalgorithm and corpus No.2. COR gained the best accuracy (0.7) with Flair Embeddings \nand corpus No.2 using Flair version 0.9. The GRNB category showed the best perfor -\nmance (0.96) with Flair Embeddings trained on the corpus with five types of entities \n(Flair Embeddings 5 Ent). The best F1-Score of the IND category was achieved with \nFlair Embeddings trained on corpus No.2 with Flair version 0.11. MISC performed the \nbest (0.66) with Flair Embeddings trained on Corpus No.4 with Flair version 0.11. The \nbest accuracy of the UNI category was achieved with Flair Embeddings trained on corpus \nNo.3 with Flair version 0.11. In general, the best overall accuracy of 0.79 (for six entity \ntypes) had the Flair Embeddings model trained on corpus No.2 with Flair version 0.11.\nDiscussion\nAs expected, Experiment 1 showed a large improvement in accuracy for all algorithms \nwhen the size of a training corpus was increased from 49 to 654 sentences. However, fur -\nther enlargement of the corpus (in Experiment 3) did not make any progress. Some types \nof entity, such as IND and GRNB, showed great performance (GRNB with an F1-Score of \n0.96 or IND with 0.98) with the small training samples, i.e., 354 entities from the GRNB \ncategory or 439 entities from the IND category. At the same time, training with a sample of \n1322 labelled funding organisations achieved an F1-Score of only 0.75.\nThe TARS model is designed to perform NER with small or no training data. In \nexperiment 1, TARS without training data was able to extract individuals with quite high \naccuracy (F-1 score of 0.86). TARS trained with the small corpus (No. 1) did not show \nimprovement in the F-1 score of individuals, but greatly improved the F-1 score of the \nGRNB category. For other entity types, this model showed extremely weak results. It was \nexpected that training with Flair Embeddings and Transformers will not bring high recog-\nnition accuracy with corpus No.1, however, interesting results can be observed. Thus, Flair \nEmbeddings showed decent accuracy of 0.8 for individuals with the small training dataset.\nThe imbalance in the performance of different types of entities can be explained by the \nnature of the data, on which the original models were trained. Thus, Flair Embeddings were \ntrained on the 1-billion words English corpus (Chelba et al., 2013). RoBERTa was pre-trained \non the combination of five datasets containing news articles, blog entries, books, and Wikipe-\ndia articles. TARS was mainly pre-trained on datasets for text classification. Thus, the models \nused were not trained on domain-specific data. This can also explain the pure Transformers \nand TARS performance. The higher accuracy for the individuals category in the training with \nTARS can be explained by the fact, that the word ’person’ is semantically more straightforward \nthan other categories. The same could be applied to grant numbers. Furthermore, grant num-\nbers generally have similar patterns, which can be applied to all entities of this type, that can  Scientometrics\n1 3\nexplain a rapid improvement in F-1 score between zero-shot and few-shot models. Moreover, \nIND and GRNB categories showed better performance for other algorithms too, which could \nlie in the structure of these entities: names of individuals and grant numbers usually have undi-\nversified patterns and in acknowledgement texts are used in a small variety of contexts. At the \nsame time, other entity types, such as funding organisations and universities could have similar \npatterns and could be used in the same context. In some cases, even for human annotators, it \nis impossible to distinguish between university, funding body and corporation without back -\nground knowledge about the entity.\nPrevious works showed improvements in downstream tasks using embedding models \nfine-tuned for the domain used (Shen et al., 2022; Beltagy et al.., 2019). Therefore, fine-\ntuning the general language model on the sample of acknowledgment texts could improve \nthe performance of the NER model for acknowledgment texts. We are planning to fine-tune \nBERT and Flair Embeddings (contextual string embeddings) on a sample of approx. 5 mil-\nlion acknowledgment texts from WoS and evaluate the performance of the NER models.\nThe results of Experiment 2 generally did not show an improvement in accuracy. On the \ncontrary, training with the three entity types deteriorated the model performance. Train-\ning without the MISC category did not show significant performance progress either. \nMoreover, further analysis of acknowledged entities showed that the miscellaneous cate-\ngory contained very inhomogeneous and partly irrelevant data, making the analysis more \ncomplicated (Smirnova & Mayr, 2023). Therefore, we assume that the model would make \nbetter predictions if the number of entity types is expanded and miscellaneous categories \nexcluded, i.e., the MISC category could be split into the following categories: names of \nprojects, names of conferences, names of software and dataset. Different subcategories \ncould also be distinguished in the FUND category.\nCorpora No.2 and No.3 contain the same number of MISC and COR entities15, while \nin corpus 4 number of occurrences of MISC and COR entities is higher. For MISC and \nCOR, accuracy slightly increased with corpus 4, therefore we assume that the extraction \naccuracy for these entities will increase with the increase of the training data. The situa-\ntion is different for funding organizations and universities. The number of UNI and FUND \nentities increased evenly from corpus No.1 to corpus No.4. Nevertheless, the best result for \nthe UNI category was achieved with corpus No.3. The poor performance of corpus No.4 \ncould be explained by the inclusion of Indian funders. Thus, the names of many Indian \nfunders are very similar to the entities which usually fall into the UNI category, e.g., the \nDepartment of Science and Technology or the Department of Biotechnology. This pattern \nis more common to the entities which fall into the UNI category. Therefore, that might \nmake the exact extraction of UNI and FUND entities more confusing. Moreover, many \nIndian Universities contain the name of individuals, e.g., Rajiv Gandhi University, which \ncan cause confusion of the UNI category with the IND category. Generally, no improve-\nment in increasing the size of the corpus for the FUND category can be explained by the \nambiguous nature of the entities which fall into the FUND category and their semantical \nproximity with other types of entities. Analysis of the extracted entities showed that many \nentities were extracted correctly, but were assigned to the wrong category (Smirnova & \nMayr, 2023). Therefore, an additional classification algorithm applied to extracted entities \ncould improve the model’s performance.\n15 These differences in entity distribution are caused by the peculiarities of acknowledgement information \nstored in WoS. As only acknowledgements with indexed funding information are stored in the database, it \nwas difficult to find an adequate number of acknowledged entities of other typesScientometrics \n1 3\nConclusion\nIn this paper, we evaluated different embedding models for the task of automatic extraction \nand classification of acknowledged entities from acknowledgment texts16. The annotation \nof the training corpora was the most challenging and time-consuming task of all data prep-\naration procedures. Therefore, a semi-automated approach was used to help significantly \naccelerate the procedure.\nThe study’s main limitations were its small size and just one annotator of the training \ncorpora. Additionally, we used acknowledgments texts collected in WoS. WoS only stores \nacknowledgments containing funding information, therefore there was a lack of other types \nof entities, such as corporations or universities in the training data.\nIn the present paper, we aimed to answer three questions. Thus, regarding research ques-\ntion 1, the few-shot and zero-shot models showed very low total recognition accuracy. At \nthe same time, it was observed that some entities performed better than others with all \nalgorithms and training corpora. Thus, individuals gained a good F1-score over 0.8 with \nzero-shot and few-shot models, as also with Flair embeddings trained with the smallest \ncorpus. With the enlargement of the training corpora, the performance of the IND category \nalso increased and achieved an F1-score over 0.9. The GRNB category showed an adequate \nF-1 score of 0.76 with the few-shot algorithm trained with the smallest corpus, following \ntraining with corpus No.2 boosts the F-1 score to over 0.9. Therefore, few-shot and zero-\nshot approaches were not able to identify all the defined acknowledged entity classes.\nWith respect to research question 2, Flair Embeddings showed the best accuracy in \ntraining with corpus No.2 (and version 0.11) and the fastest training time compared to the \nother models; thus, it is recommended to further use the Flair Embeddings model for the \nrecognition of acknowledged entities.\nExploring research question 3 we observed, that the expansion of the size of a training \ncorpus from very small (corpus No.1) to medium size (corpus No.2) massively increased the \naccuracy of all training algorithms. The best-performing model (Flair Embedding) was further \nretrained with the two bigger corpora, but the following expansion of the training corpus did \nnot bring further improvement. Moreover, the performance of the model slightly deteriorated.\nAcknowledgements The original work was funded by the German Center for Higher Education Research \nand Science Studies (DZHW) via the project “Mining Acknowledgement Texts in Web of Science \n(MinAck)”17. Access to the WoS data was granted via the Competence Centre for Bibliometrics18. Data \naccess was funded by BMBF (Federal Ministry of Education and Research, Germany) under grant number \n01PQ17001. Nina Smirnova received funding from the German Research Foundation (DFG) via the project \n“POLLUX”19. The present paper is an extended version of the paper “Evaluation of Embedding Models for \nAutomatic Extraction and Classification of Acknowledged Entities in Scientific Documents” (Smirnova & \nMayr, 2022) presented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Sci-\nentific Documents (EEKE2022).\nAppendix A: Accuracy metrics by type of entity (label) for all \nexperiments\nSee Table 5.\n16 The best model can be tested  at https:// huggi ngface. co/ kalaw inka/ flair- ner- ackno  wledg ments\n17 https:// kalaw inka. github. io/ minack/.\n18 https:// www. bibli ometr  ie. info/ en/ index. php? id= home.\n19 https:// www. pollux- fid. de/ about. Scientometrics\n1 3\nTable 5  Accuracy metrics by type of entity (label) for all experiments\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings No.1 9 IND 0.7692 0.8333 0.8000 12 1\nFlair embeddings No.1 9 GRNB 0.5385 0.7000 0.6087 10 1\nFlair embeddings No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nFlair embeddings No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nFlair embeddings No.1 9 COR 0.0000 0.0000 0.0000 1 1\nFlair embeddings No.1 9 FUND 0.4000 0.4444 0.4211 18 1\nFlair embeddings No.2 9 FUND 0.6524 0.7771 0.7093 157 1\nFlair embeddings No.2 9 IND 0.9764 0.9831 0.9797 295 1\nFlair embeddings No.2 9 GRNB 0.9398 0.9750 0.9571 160 1\nFlair embeddings No.2 9 UNI 0.7527 0.7071 0.7292 99 1\nFlair embeddings No.2 9 MISC 0.6420 0.6341 0.6380 82 1\nFlair embeddings No.2 9 COR 0.8750 0.5833 0.7000 12 1\nTARS (pretrained) No.1 9 IND 1.0000 0.7500 0.8571 12 1\nTARS (pretrained) No.1 9 GRNB 0.7273 0.8000 0.7619 10 1\nTARS (pretrained) No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTARS (pretrained) No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTARS (pretrained) No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTARS (pretrained) No.1 9 FUND 0.3158 0.3333 0.3243 18 1\nTARS (pretrained) No.2 9 FUND 0.7257 0.8089 0.7651 157 1\nTARS (pretrained) No.2 9 IND 0.9281 0.8746 0.9005 295 1\nTARS (pretrained) No.2 9 GRNB 0.8895 0.9563 0.9217 160 1\nTARS (pretrained) No.2 9 UNI 0.7407 0.6061 0.6667 99 1\nTARS (pretrained) No.2 9 MISC 0.6719 0.5244 0.5890 82 1\nTARS (pretrained) No.2 9 COR 0.5000 0.5833 0.5385 12 1\nTransformers No.1 9 GRNB 0.3000 0.6000 0.4000 10 1\nTransformers No.1 9 IND 0.0000 0.0000 0.0000 12 1\nTransformers No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTransformers No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTransformers No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTransformers No.1 9 FUND 0.2414 0.3889 0.2979 18 1\nTransformers No.2 9 FUND 0.6211 0.7516 0.6801 157 1\nTransformers No.2 9 IND 0.9346 0.9695 0.9517 295 1\nTransformers No.2 9 GRNB 0.8704 0.8812 0.8758 160 1\nTransformers No.2 9 UNI 0.6476 0.6869 0.6667 99 1\nTransformers No.2 9 MISC 0.4767 0.5000 0.4881 82 1\nTransformers No.2 9 COR 0.7500 0.5000 0.6000 12 1\nFlair embeddings (3 Ent) No.2 9 IND 0.9577 0.9703 0.9639 303 2\nFlair embeddings (3 Ent) No.2 9 ORG 0.6400 0.6154 0.6275 208 2\nFlair embeddings (3 Ent) No.2 9 GRNB 0.9286 0.9750 0.9512 160 2\nFlair embeddings (5 Ent) No.2 9 IND 0.9764 0.9797 0.9780 295 2\nFlair embeddings (5 Ent) No.2 9 GRNB 0.9345 0.9812 0.9573 160 2\nFlair embeddings (5 Ent) No.2 9 UNI 0.7802 0.7172 0.7474 99 2\nFlair embeddings (5 Ent) No.2 9 COR 0.7500 0.5000 0.6000 12 2\nFlair embeddings (5 Ent) No.2 9 FUND 0.6722 0.7707 0.7181 157 2Scientometrics \n1 3\nAppendix B: Overall accuracy for all experiments\nSee Table 6.Rows are sorted by experiment number and algorithmTable 5  (continued)\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings (RoB-\nERTa)No.2 9 IND 0.9206 0.9831 0.9508 295 2\nFlair embeddings (RoB-\nERTa)No.2 9 GRNB 0.8896 0.9062 0.8978 160 2\nFlair embeddings (RoB-\nERTa)No.2 9 UNI 0.5963 0.6566 0.6250 99 2\nFlair embeddings (RoB-\nERTa)No.2 9 MISC 0.4135 0.5244 0.4624 82 2\nFlair embeddings (RoB-\nERTa)No.2 9 COR 1.0000 0.5000 0.6667 12 2\nFlair embeddings (RoB-\nERTa)No.2 9 FUND 0.6096 0.7261 0.6628 157 2\nFlair embeddings No.2 11 GRNB 0.9345 0.9812 0.9573 160 3\nFlair embeddings No.2 11 IND 0.9797 0.9831 0.9814 295 3\nFlair embeddings No.2 11 FUND 0.7027 0.8280 0.7602 157 3\nFlair embeddings No.2 11 UNI 0.7684 0.7374 0.7526 99 3\nFlair embeddings No.2 11 MISC 0.6543 0.6463 0.6503 82 3\nFlair embeddings No.2 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 UNI 0.8000 0.7273 0.7619 99 3\nFlair embeddings No.3 11 IND 0.9731 0.9797 0.9764 295 3\nFlair embeddings No.3 11 GRNB 0.9281 0.9688 0.9480 160 3\nFlair embeddings No.3 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 MISC 0.6571 0.5610 0.6053 82 3\nFlair embeddings No.3 11 FUND 0.6757 0.7962 0.7310 157 3\nFlair embeddings No.4 11 MISC 0.7424 0.5976 0.6622 82 3\nFlair embeddings No.4 11 COR 0.8571 0.5000 0.6316 12 3\nFlair embeddings No.4 11 UNI 0.7753 0.6970 0.7340 99 3\nFlair embeddings No.4 11 IND 0.9698 0.9797 0.9747 295 3\nFlair embeddings No.4 11 FUND 0.6823 0.8344 0.7507 157 3\nFlair embeddings No.4 11 GRNB 0.9162 0.9563 0.9358 160 3 Scientometrics\n1 3\nFunding Open access funding enabled and organized by Projekt DEAL.\nDeclarations \n Conflict of interest Philipp Mayr, the co-author of this paper, has a conflict of interest because he serves on \nthe editorial board of the journal Scientometrics. In addition, he is a co-guest editor of the special issue on \n\"Extraction and Evaluation of Knowledge Entities from Scientific Documents\". He declares that he has noth-\ning to do with the decision about this paper submission.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAkbik, A., Bergmann, T., Blythe, D., Rasul, K., Schweter, S., & Vollgraf, R. 2019. FLAIR: An Easy-to-Use \nFramework for State-of-the-Art NLP. Minneapolis, Minnesota (pp. 54–59). Association for Computa-\ntional Linguistics.\nAkbik, A., Blythe, D., & Vollgraf, R. (2018). Contextual string embeddings for sequence labeling. In 2018, \n27th International Conference on Computational Linguistics (pp. 1638–1649).\nAlexandera, D. & Vries, A. P. (2021). This research is funded by...”: Named Entity Recognition of financial \ninformation in research papers. In BIR 2021: 11th International Workshop on Bibliometric-enhanced \nInformation Retrieval at ECIR (pp. 102–110).\nBeltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. \nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing \nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)  \n(pp. 3613–3618). Association for Computational Linguistics.Table 6  Overall accuracy for all experiments\nRows are sorted by experiment number and algorithmAlgorithm Corpus Version Accuracy Experiment\nFlair embeddings No.2 9 0.7702 1\nFlair embeddings No.1 9 0.3472 1\nTARS (pretrained) No.2 9 0.7113 1\nTARS (pretrained) No.1 9 0.3485 1\nTransformers No.2 9 0.6783 1\nTransformers No.1 9 0.1477 1\nFlair Embeddings (3 Entity Types) No.2 9 0.7536 2\nFlair embeddings (5 Entity Types) No.2 9 0.7990 2\nFlair embeddings + RoBERTa No.2 9 0.6697 2\nFlair Embeddings No.2 11 0.7869 3\nFlair embeddings No.4 11 0.7814 3\nFlair embeddings No.3 11 0.7691 3Scientometrics \n1 3\nBorst, T., Mielck, J., Nannt, M., & Riese, W. (2022). Extracting funder information from scien-\ntific papers—Experiences with question answering. In Silvello, G., O.  Corcho, P.  Manghi, G.M. \nDi Nunzio, K. Golub, N. Ferro, and A. Poggi (Eds.),Linking theory and practice of digital libraries  \n(Vol. 13541, pp. 289–296). Springer International Publishing. Series Title: Lecture Notes in Com-\nputer Science. https:// doi. org/ 10. 1007/ 978-3- 031- 16802-4_ 24.\nChelba, C., T.  Mikolov, M.  Schuster, Q.  Ge, T.  Brants, P.  Koehn, & Robinson, T. (2013). One Bil-\nlion Word Benchmark for Measuring Progress in Statistical Language Modeling. 10.48550/\nARXIV.1312.3005 .\nChen, H., Song, X., Jin, Q., & Wang, X. (2022). Network dynamics in university-industry collaboration: \nA collaboration-knowledge dual-layer network perspective. Scientometrics, 127(11), 6637–6660. \nhttps:// doi. org/ 10. 1007/ s11192- 022- 04330-9\nCronin, B. (1995). The Scholar’s courtesy: The role of acknowledgement in the primary communication \nprocess. Taylor Graham.\nCronin, B., & Weaver, S. (1995). The praxis of acknowledgement: From bibliometrics to influmetrics. \nRevista Española de Documentación Científica, 18(2), 172.\nDevlin, J., Chang, M.W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional \ntransformers for language understanding. 10.48550/ARXIV.1810.04805 .\nDiaz-Faes, A. A., & Bordons, M. (2017). Making visible the invisible through the analysis of acknowl-\nedgements in the humanities. Aslib Journal of Information Management, 69(5), 576–590. https://  \ndoi. org/ 10. 1108/ AJIM- 01- 2017- 0008\nDoehne, M., & Herfeld, C. (2023). How academic opinion leaders shape scientific ideas: an acknowl-\nedgment analysis., 128(4), 2507–2533. https:// doi. org/ 10. 1007/ s11192- 022- 04623-z\nDzieżyc, M., & Kazienko, P. (2022). Effectiveness of research grants funded by European research coun-\ncil and polish national science centre. Journal of Informetrics, 16(1), 101243. https:// doi. org/ 10.  \n1016/j. joi. 2021. 101243\nEftimov, T., Koroušić Seljak, B., & Korošec, P. (2017). A rule-based named-entity recognition \nmethod for knowledge extraction of evidence-based dietary recommendations. PLoS ONE, 12(6), \ne0179488. https:// doi. org/ 10. 1371/ journ al. pone. 01794 88\nEtzioni, O., Cafarella, M., Downey, D., Popescu, A. M., Shaked, T., Soderland, S., Weld, D. S., & Yates, \nA. (2005). Unsupervised named-entity extraction from the web: An experimental study. Artificial \nIntelligence, 165(1), 91–134. https:// doi. org/ 10. 1016/j. artint. 2005. 03. 001\nFinkel, J.R., Grenager, T., & Manning, C. (2005). Incorporating non-local information into information \nextraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL’05), Ann Arbor, Michigan (pp. 363–370). Association for \nComputational Linguistics.\nGiles, C. L., & Councill, I. G. (2004). Who gets acknowledged: Measuring scientific contributions \nthrough automatic acknowledgment indexing. Proceedings of the National Academy of Sciences,  \n101(51), 17599–17604. https:// doi. org/ 10. 1073/ pnas. 04077 43101\nHalder, K., Akbik, A., Krapac, J., & Vollgraf, R. (2020). Task-Aware Representation of Sentences for \nGeneric Text Classification. In Proceedings of the 28th International Conference on Computational \nLinguistics, Barcelona, Spain (Online) (pp. 3202–3213). International Committee on Computa-\ntional Linguistics.\nHubbard, D., Laddusaw, S., Tan, Q., & Hu, X. (2022). Analysis of acknowledgments of libraries in the \njournal literature using machine learning. Proceedings of the Association for Information Science \nand Technology, 59(1), 709–711. https:// doi. org/ 10. 1002/ pra2. 698\nIovine, A., Fang, A., Fetahu, B., Rokhlenko, O., & Malmasi, S. (2022). CycleNER: An unsupervised \ntraining approach for named entity recognition. In Proceedings of the ACM Web Conference 2022  \n(pp. 2916–2924). ACM.\nJiang, L., Kang, X., Huang, S., & Yang, B. (2022). A refinement strategy for identification of scientific \nsoftware from bioinformatics publications. Scientometrics, 127(6), 3293–3316. https:// doi. org/ 10.  \n1007/ s11192- 022- 04381-y\nKassirer, J. P., & Angell, M. (1991). On authorship and acknowledgments. The New England Journal of \nMedicine, 325(21), 1510–1512. https:// doi. org/ 10. 1056/ NEJM1 99111 21325 2112\nKayal, S., Afzal, Z., Tsatsaronis, G., Katrenko, S., Coupet, P., Doornenbal, M., & Gregory, M. (2017). \nTagging funding agencies and grants in scientific articles using sequential learning models. In \nBioNLP 2017, Vancouver, Canada (pp. 216–221). Association for Computational Linguistics.\nKenekayoro, P. (2018). Identifying named entities in academic biographies with supervised learning. \nScientometrics, 116(2), 751–765. https:// doi. org/ 10. 1007/ s11192- 018- 2797-4\nKingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. 10.48550/\nARXIV.1412.6980 . Scientometrics\n1 3\nKusumegi, K., & Sano, Y. (2022). Dataset of identified scholars mentioned in acknowledgement state-\nments. Scientific Data, 9(1), 461. https:// doi. org/ 10. 1038/ s41597- 022- 01585-y\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoy -\nanov, V. (2019). RoBERTa: A robustly optimized BERT pretraining approach. arXiv: 1907. 11692  \n[cs] .\nMackintosh, K. (1972). Acknowledgements patterns in sociology. Ph. D. thesis, University of Oregon.\nMccain, K. (2017). Beyond Garfield’s citation index: An assessment of some issues in building a per -\nsonal name acknowledgments index. Scientometrics. https:// doi. org/ 10. 1007/ s11192- 017- 2598-1\nMcCain, K. W. (1991). Communication, competition, and secrecy: The production and dissemination of \nresearch-related information in genetics. Science, Technology, & Human Values, 16(4), 491–516. \nhttps:// doi. org/ 10. 1177/ 01622 43991 01600 404\nMejia, C., & Kajikawa, Y. (2018). Using acknowledgement data to characterize funding organizations \nby the types of research sponsored: the case of robotics research. Scientometrics, 114(3), 883–904. \nhttps:// doi. org/ 10. 1007/ s11192- 017- 2617-2\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, \nN., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkur -\nthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: An imperative style, high-\nperformance deep learning library. arXiv: 1912. 01703 [cs, stat].\nPaul-Hus, A., & Desrochers, N. (2019). Acknowledgements are not just thank you notes: A qualitative \nanalysis of acknowledgements content in scientific articles and reviews published in 2015. PLoS \nONE, 14, e0226727. https:// doi. org/ 10. 1371/ journ al. pone. 02267 27\nPaul-Hus, A., Díaz-Faes, A., Sainte-Marie, M., Desrochers, N., Costas, R., & Larivière, V. (2017). \nBeyond funding: Acknowledgement patterns in biomedical, natural and social sciences. PLoS ONE,  \n12, e0185578. https:// doi. org/ 10. 1371/ journ al. pone. 01855 78\nPennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors for Word Representation. \nIn Empirical Methods in Natural Language Processing (EMNLP) (pp. 1532–1543).\nPustejovsky, J., & Stubbs, A. (2012). Natural language annotation for machine learning. O’Reilly \nMedia Inc.\nRose, M., & Georg, C. P. (2021). What 5,000 acknowledgements tell us about informal collaboration \nin financial economics. Research Policy, 50, 104236. https:// doi. org/ 10. 1016/j. respol. 2021. 104236\nSang, T. K., & E. F., & De Meulder, F. (2003). Introduction to the CoNLL-2003 shared task: Language-\nindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Lan-\nguage Learning at HLT-NAACL (pp. 142–147).\nSchweter, S., & Akbik, A. (2020). FLERT: Document-level features for named entity recognition. ArXiv. \n10.48550/arXiv.2011.06993 .\nShen, S., Liu, J., Lin, L., Huang, Y., Zhang, L., Liu, C., Feng, Y., & Wang, D. (2022). SsciBERT: A \npre-trained language model for social science texts. Scientometrics. https:// doi. org/ 10. 1007/  \ns11192- 022- 04602-4\nSingh, V. K., Singh, P., Karmakar, M., Leta, J., & Mayr, P. (2021). The journal coverage of web of sci-\nence, scopus and dimensions: A comparative analysis. Scientometrics, 126(6), 5113–5142. https://  \ndoi. org/ 10. 1007/ s11192- 021- 03948-5\nSmirnova, N., & Mayr, P. (2022). Evaluation of embedding models for automatic extraction and classifi-\ncation of acknowledged entities in scientific documents. In 3rd Workshop on Extraction and Evalu-\nation of Knowledge Entities from Scientific Documents 2022 (EEKE 2022) (pp. 48–55). CEUR-WS.\norg.\nSmirnova, N., & Mayr, P. (2023). A comprehensive analysis of acknowledgement texts in web of sci-\nence: A case study on four scientific domains. Scientometrics, 1(128), 709–734. https:// doi. org/ 10.  \n1007/ s11192- 022- 04554-9\nSong, M., Kang, K. Y., Timakum, T., & Zhang, X. (2020). Examining influential factors for acknowl-\nedgements classification using supervised learning. PLoS ONE. https:// doi. org/ 10. 1371/ journ al.  \npone. 02289 28\nThomer, A. K., & Weber, N. M. (2014). Using named entity recognition as a classification heuristic. In \niConference 2014 Proceedings (pp. 1133 – 1138). iSchools.\nWang, J., & Shapira, P. (2011). Funding acknowledgement analysis: An enhanced tool to investigate \nresearch sponsorship impacts: The case of nanotechnology. Scientometrics, 87(3), 563–586. https://  \ndoi. org/ 10. 1007/ s11192- 011- 0362-5\nYamada, I., Asai, A., Shindo, H., Takeda, H., & Matsumoto, Y. (2020). LUKE: Deep contextualized \nentity representations with entity-aware self-attention. In Proceedings of the 2020 Conference on \nEmpirical Methods in Natural Language Processing (EMNLP) (pp. 6442–6454). Association for \nComputational Linguistics.Scientometrics \n1 3\nYu, J., Bohnet, B., & Poesio, M. (2020). Named entity recognition as dependency parsing. 10.48550/\nARXIV.2005.07150.\nZhang, C., Mayr, P., Lu, W., & Zhang, Y. (2023). Guest editorial: Extraction and evaluation of knowledge \nentities in the age of artificial intelligence. Aslib Journal of Information Management, 75, 433–437. \nhttps:// doi. org/ 10. 1108/ AJIM- 05- 2023- 507",
        "type": "direct"
    },
    {
        "citation": "Computational Linguistics 2017",
        "content": ".\nKayal et  al.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2021",
        "content": "introduced a method for extraction of funding organizations and \ngrants from acknowledgment texts using a combination of sequential learning models: con-\nditional random fields (CRF), hidden markov models (HMM), and maximum entropy mod-\nels (MaxEnt). The final model contained pooled outputs from the models used.\nAlexandera and Vries",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": "proposed AckNER, a tool for extracting financial informa-\ntion from the funding or acknowledgment section of a research article. AckNER works \nwith the use of dependency parse trees and regular expressions and is able to extract names \nof the organisations, projects, programs, and funds, as also numbers of contracts and \ngrants7.\nFollowing, Borst et  al.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2023",
        "content": "applied a question-answering (QA) based approach to \nidentify funding information in acknowledgments texts. This approach performs similarly \nto AckNER and requires a smaller set of training and test data.\nTable  1 shows an overview of works on NER in scientometrics. Overall, previous works \non the extraction of named entities from acknowledgements texts were mostly concerned \nwith the extraction of funding information, i.e., only names of funding bodies and grant \nnumbers, or extraction and linking of individuals. The special issue by Zhang et al.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2004",
        "content": "provided a recent overview of current works in the extraction of knowledge entities.\nTo the best of our knowledge the work of Giles and Councill",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2021",
        "content": "is the only \nattempt to extract and categorise multiple acknowledged entities. Nevertheless, entities \nwere extracted using the SVM algorithm but the classification of entities themselves \nwas produced manually, which limited the number of acknowledgement texts to be ana-\nlysed. Furthermore, as far as we know, there was no research done concerning the eval-\nuation of embedding models for extraction of information from acknowledgement texts \nand no tool for automatic extraction of different kinds of acknowledged entities was \ndeveloped.\n7 AckNER showed better performance as Flair, but is specifically designed to recognize two types of \nacknowledged entities",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2004",
        "content": ", which was insufficient for the present project.Scientometrics \n1 3\nTable 1  Overview of works on NER in scientometrics\nPaper Area of application and aim of \nthe studyCorpus Entities Methods and tools\nGiles and Councill",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2014",
        "content": "Extraction of acknowledged enti-\nties form acknowledgementsCiteSeer Funding agencies, Companies, \nEducational Institutions, \nIndividualsSVM for extracting entities and \ntheir manual classification\nThomer and Weber",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2017",
        "content": "Using NER to improve classifica-\ntion of acknowledgementsPubMed Central’s Open Access Persons, locations, organizations, \nand miscellaneous4-class Stanford Entity Recognizer\nKayal et al.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2018",
        "content": "Extraction of funding information \nfrom acknowledgementsPubMed Central’s Open Access Funding bodies, grants CRF, HMM, MaxEnt\nKenekayoro",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2021",
        "content": "Extraction of biography informa-\ntion from academic biographiesORCID Award, Location, Organization, \nPerson, Position, Specializa-\ntion, OthersSVM\nAlexandera and Vries",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": "Extraction of funding information \nfrom acknowledgementsTU Delft’s institutional repository Funding bodies, grants SpaCy dependency parser + regu-\nlar expressions\nJiang et al.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": "Extraction of scientific software \nfrom scientific articles (full \ntexts) in bioinformaticsbioinformatics journals EnsembleSVMs-CRF\nBorst et al.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": "Extraction of funding information \nfrom acknowledgementsEconStor Funding bodies, grants Haystack\nKusumegi and Sano",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2004",
        "content": "Extraction and linking of \nacknowledged individuals from \nacknowledgementsPLOS Individuals Stanford CoreNLP NER tagger + \nMAG Scientometrics\n1 3\nMethod\nIn the present paper, different models for extraction and classification of acknowledged \nentities supported by the Flair NLP framework were evaluated. The choice of classification \nwas inspired by Giles and Councill’s",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2019",
        "content": "classification: funding agencies (FUND), cor -\nporations (COR), universities (UNI), and individuals (IND). For our project, this classifica-\ntion was enhanced with the miscellaneous (MISC) and grant numbers (GRNB) categories. \nThe GRNB category was adopted from WoS funding information indexing. The entities \nin the miscellaneous category could provide useful information, but cannot be ascribed to \nother categories, e.g., names of projects and names of conferences. Figure  1 demonstrates \nan example of acknowledged entities of different types. To the best of our knowledge, Giles \nand Councill’s classification is the only existing classification of acknowledged entities and \ntherefore can be applied to the NER task. Other works on acknowledgment analysis were \nfocused on the classification of acknowledgment texts.\nThe Flair NLP framework\nFlair is an open-sourced NLP framework built on PyTorch",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2019, p.  54",
        "content": ", which \nis an open-source machine learning library. “The core idea of the framework is to pre-\nsent a simple, unified interface for conceptually very different types of word and document \nembeddings” . Flair has three default training algorithms for \nNER which were used for the first experiment in the present research: a) NER Model with \nFlair Embeddings (later on Flair Embeddings) (Akbik et al., 2018), b) NER Model with \nTransformers (later on Transformers) (Schweter & Akbik, 2020), and c) Zero-shot NER \nwith TARS (later on TARS) (Halder et al., 2020) 8.\nThe Flair Embeddings model uses stacked embeddings, i.e., a combination of contex-\ntual string embeddings (Akbik et al., 2018) with a static embeddings model. This approach \nwill generate different embeddings for the same word depending on its context. Stacked \nembedding is an important Flair feature, as a combination of different embeddings might \nbring better results than their separate uses (Akbik et al., 2019).\nThe Transformers model or FLERT-extension (document-level features for NER) is a \nset of settings to perform a NER on the document level using fine-tuning and feature-based \nFig. 1  An example of acknowledged entities. Each entity type is marked with a distinct color\n8 New transformer models as SciBERT or SsciBERT were not evaluated in this study, as the objective of \nthe study is to evaluate the performance of the Flair default models.Scientometrics \n1 3\nLSTM-CRF with the multilingual XML-RoBERTa transformer model (Schweter & Akbik, \n2020).\nThe TARS (task-aware representation of sentences) is a transformer-based model, \nwhich allows performing training without any training data (zero-shot learning) or with a \nsmall dataset (few-short learning) (Halder et al., 2020). The TARS approach differs from \nthe traditional transfer learning approach in the way that the TARS model also considers \nsemantic information captured in the class labels themselves. For example, for analyzing \nacknowledgments, class labels like funding organization or university already carry seman-\ntic information.\nTraining data\nThe Web of Science (WoS) database was used to harvest the training data (funding \nacknowledgments).9 From 2008 on, WoS started indexing information about funders and \ngrants. WoS uses information from different funding reporting systems such as Research-\nfish,10 Medline11 and others. As WoS contains millions of metadata records (Singh et al., \n2021), the data chosen for the present study was restricted by year and scientific domain \n(for the corpora Nos. 1, 2, and 3) or additionally by the affiliation country (for corpus No.4). \nTo construct corpora Nos. 1-3 records from four different scientific domains published \nfrom 2014 to 2019 were considered: two domains from the social sciences (sociology and \neconomics) and oceanography and computer science. Different scientific domains were Table 2  Number of sentences/texts in the training corpora\nCorpus No. Training set (train) Test set (test) Validation set (dev) Total\n1 29/27 10/10 10/10 49/47\n2 339/282 165/150 150/136 654/441\n3 784/657 165/150 150/136 1099/816\n4 1148/885 165/150 150/136 1463/1044\nTable 3  Number of sentences/texts from each scientific domain in the training corpora\nCorpus No. Oceanography Economics Social Sciences Computer Science\n1 13/13 3/3 20/20 16/14\n2 127/75 92/58 351/234 173/129\n3 175/112 128/89 590/434 333/269\n9 The present research was conducted in scopes of two projects: MinAck (https:// kalaw inka. github. io/ \nminack/) and SEASON (https:// github. com/ kalaw inka/ season). Corpora Nos.1, 2, and 3 were created for the \nMinAck project and serve the purpose of a general evaluation of the impact of the size of the training cor -\npus on the model performance. Corpus No.4 was designed specifically for the SEASON project in the hope \nof improving the recognition of Indian funding information. The project SEASON aims to get insight into \nGerman-Indian scientific collaboration. Our other corpora mainly contain papers published by European \ninstitutions. That is why we enhance Corpus 4 with the papers published by Indian institutions.\n10 https:// resea rchfi sh. com/.\n11 https:// www. nlm. nih. gov/ bsd/ fundi ng_ suppo rt. html. Scientometrics\n1 3\nchosen since previous work on acknowledgment analysis revealed the relations between \nthe scientific domain and the types of acknowledged entities, i.e., acknowledged individu-\nals are more characteristic of theoretical and social-oriented domains. At the same time, \ninformation on technical and instrumental support is more common for the natural and \nlife sciences domains (Diaz-Faes & Bordons, 2017). Only the WoS record types “article” \nand “review” published in a scientific journal in English were selected; then 1000 distinct \nacknowledgments texts were randomly gathered from this sample for the training dataset. \nFurther different amounts of sentences containing acknowledged entities were distributed \ninto the differently-sized training corpora. Table  2 demonstrates the number of sentences \nin each set in the four corpora. We selected only sentences that contain an acknowledged \nentity, regardless of the scientific domain. Table  3 contains the number of sentences and \ntexts from each scientific domain in the training corpora.12 The same article can belong to \nseveral scientific domains, therefore, the number of sentences and texts in Tables  2 and 3 \ndoes not match. Corpus No.4 was designed in such a way that all the training data from the \nCorpus No.3 was enhanced with acknowledgments texts from the articles that have Indian \naffiliations regardless of scientific domain or publication date.\nPreliminary analysis of the WoS data showed that the indexing of WoS funding infor -\nmation has several issues. The WoS includes only acknowledgments containing funding \ninformation; therefore, not every WoS entry has an acknowledgment, individuals are not \nincluded, and indexed funding organizations are not divided into different entity types like \nuniversities, corporations, etc. Therefore, the existing indexing of funding organizations \nis incomplete. Furthermore, there is a disproportion between the occurrences of acknowl-\nedged entities of different types. Thus, the most frequent entity types in the dataset with \nthe training data are IND, FUND and GRNB, followed by UNI and MISC. COR is the \nFig. 2  The distribution of acknowledged entities in the training corpora\n12 Corpus No.4 is not in Table  2, because the corpus contains additional acknowledgment texts from arti-\ncles with Indian affiliations regardless of the scientific domain and therefore contains different scientific \ndomains.Scientometrics \n1 3\ncategory most underrepresented in the data set. Consequently, there are different amounts \nof entities of different types in the training corpora (as Fig.  2 demonstrates), which might \nhave influenced the training results. Training with the corpora Nos. 2, 3, and 4 was evalu-\nated on the same training and validation datasets to ensure plausible accuracy (Fig.  3-B). \nFig. 3  The distribution of acknowledged entities in the test and validation corpora\nFig. 4  Annotation flowchart Scientometrics\n1 3\nHowever, training with corpus No.1 was evaluated with the smaller test and validation sets, \nas corpus No.1 contains a smaller number of sentences (Fig. 3-A).\nData annotation\nThe training corpus was annotated with six types of entities. As WoS already contains \nsome indexed funding information, it was decided to develop a semi-automated approach \nfor data annotation (as Fig.  4 demonstrates) and use indexed information provided by WoS, \ntherefore, grant numbers were adopted from the WoS indexing unaltered.\nFlair has a pre-trained 4-class NER Flair model (CoNLL-03).13 The model can pre-\ndict four tags: PER (person name), LOC (location), ORG (organization name), and MISC \n(other names). As Flair showed adequate results in the extraction of names of individu-\nals, it was decided to apply the pre-trained 4-class CoNLL-03 Flair model to the training \ndataset. Entities that fell into the PER category were added as the IND annotation to the \ntraining corpus. Furthermore, we noticed that some funding information was partially cor -\nrectly extracted into the ORG and MISC categories. Therefore, WoS funding organization \nindexing and entities from the ORG and MISC categories were adopted and distinguished \nbetween three categories (FUND, COR, and UNI) using regular expressions. In addition, \nthe automatic classification of entities was manually examined and reviewed. Mismatched \ncategories, partially extracted entities, and not extracted entities were corrected. Acknowl-\nedged entities, which fall into the MISC category, were manually annotated by one annota-\ntor. In the miscellaneous category entities referring to names of the conferences and pro-\njects were included.\nExperiments\nIn the present paper, we evaluated three default Flair NER models with four differently-\nsized corpora. In total, we performed three experiments. In the first experiment, mod-\nels with the default parameter were evaluated using corpora Nos. 1 and 2. In the second \nexperiment, we evaluated Flair Embeddings and Transformers model with altered training \nparameters and corpus No.2. In the third experiment, the first experiment was replicated \nwith corpora Nos. 3 and 4.\nExperiment 1\nIn the first experiment, we tested the TARS model zero-shot and few-shot scenarios (with \ncorpus No. 1), as well as the performance of two default FLAIR models (Flair Embeddings \nand Transformers) with corpus No.2. Additionally, the performance of Flair Embeddings \nand Transformers models was tested with the corpus No.1 The training was conducted with \nthe recommended parameters for all algorithms, as Flair developers specifically ran vari-\nous tests to find the best hyperparameters for the default models. For the few-shot TARS, \nthe training was conducted with the small dataset (corpus No.1), and for Transformers and \nFlair Embeddings with a larger dataset (corpus No.2).\n13 https:// github. com/ flair NLP/ flairScientometrics \n1 3\nThe Flair Embeddings model was initiated as a combination of static and contextual \nstring embeddings. We applied GloVe (Pennington et  al., 2014) as a static word-level \nembedding model. Thus, in our case, stacked embeddings comprise GloVe embeddings, \nforward contextual string embeddings, and backward contextual string embeddings. The \nmodel was trained with the recommended parameters: the size of mini-batches was set to \n32 and the maximum number of epochs was set to 150.\nFor Transformers, training was initiated with the RoBERTa model (Liu et al., 2019). For \nthe present paper, a fine-tuning approach was used. The fine-tuning procedure consisted of \nadding a linear layer to a transformer and retraining the entire network with a small learn-\ning rate. We used a standard approach, where only a linear classifier layer was added on the \ntop of the transformer, as adding the additional CRF decoder between the transformer and \nlinear classifier did not increase accuracy compared with this standard approach (Schweter \n& Akbik, 2020). The chosen transformer model uses subword tokenization. We used the \nmean of embeddings of all subtokens and concatenation of all transformer layers to pro-\nduce embeddings. The context around the sentence was considered. The training was initi-\nated with a small learning rate using the Adam Optimisation Algorithm (Kingma & Ba, \n2014).\nThe TARS model requires labels to be defined in a natural language. Therefore, we \ntransformed our original coded labels into the natural language: FUND - “Funding \nAgency”, IND - “Person”, COR - “Corporation”, GRNB - “Grant Number\", UNI - “Uni-\nversity”, and MISC - “Miscellaneous”. The training for the few-shot approach was initiated \nwith the TARS NER model (Halder et al., 2020).\nResults\nOverall, the training demonstrated mixed results. Table  4 shows training results with cor -\npus No.1 and the TARS zero-shot approach. GRNB showed adequate results by training \nwith Flair Embeddings and TARSfew-shot models. IND was the best-recognized entity by \ntraining with Flair Embeddings and TARS (both zero- and few-shot) with an F1-score of \n0.8 (Flair Embeddings) and 0.86 (TARS) respectively. Training with Transformers was not \nsuccessful for IND with an F1-score of 0. In general, transformers were a less efficient \nalgorithm for training with a small dataset with an overall accuracy of 0.35. FUND dem-\nonstrated not satisfactory results with F1-score of less than 0.5 for all models. Entity types \nFig. 5  The training results with the training corpus No.2. A Comprises diagrams with the F1-scores of the \ntraining with three algorithms for each label class. B depicts the total accuracy of training algorithms Scientometrics\n1 3\nMISC, UNI, and COR showed the worst results with the F1-score equal to zero for all algo-\nrithms. The low accuracy for MISC, UNI, and COR resulted in low overall accuracy for \nall algorithms. Overall, training with corpus No.1 showed insufficient results for all algo-\nrithms. Flair Embeddings and TARS showed better accuracy compared to Transformers.\nFigure  5 shows the training results with corpus No.2. Similar to the training with corpus \nNo.1, IND and GRNB are the best-recognized categories. The best results for IND and \nGRNB demonstrated Flair embeddings with an F1-score of 0.98 (IND) and 0.96 (GRNB). \nTARS achieved the best results for FUND with an F1-score of 0.77 against 0.71 for Flair \nEmbeddings and 0.68 for Transformers. Miscellaneous demonstrated the worst accuracy \nfor Flair Embeddings (0.64) and Transformers (0.49), while for TARS the worst accuracy \nlies in the COR category with an F1-score of 0.54. The best result for UNI showed Flair \nEmbeddings with an F1-score over 0.7. The COR category showed a decent precision \nof 0.88 with Flair Embeddings but a low recall of 0.58 which resulted in a low F1-Score \n(0.7)14.\nTraining with corpus No.2 showed a significant improvement in training accuracy \n(Fig.  5B). Overall, Flair Embeddings was more accurate than other training algorithms, \nalthough training with TARS showed better results for the FUND category. The Trans-\nformers showed the worst results during training.\nAdditionally, a zero-shot approach was tested for the TARS model on corpus no.1. The \nmodel was able to successfully recognize individuals, but struggled with other categories, \nas Table 4 demonstrates. The total accuracy of the model comprises 0.23.\nExperiment 2\nOur first hypothesis to explain the pure model performance for the FUND, COR, MISC, \nand UNI categories is their semantic proximity that prevents successful recognition. \nEntities of these categories are often used in the same context. To examine this hypoth-\nesis, we conducted an experiment using Flair Embeddings with the dataset contain-\ning three types of entities: IND, GRNB, and ORG. The MISC category was excluded \nfrom the training, as one of the aims of the present research is to extract information \nabout acknowledged entities, and the MISC category contains only additional informa-\ntion. The new ORG category was established, which includes a combination of entities \nfrom the FUND, COR, and UNI categories. The training was performed with exactly \nthe same parameters as training with the Flair Embeddings model in Experiment 1 \n(Sect. 4.1).Table 4  F1-scores of the training with three algorithms for each label class with Corpus No. 1\nAlgorithm FUND GRNB IND UNI COR MISC accuracy\nTARS (zero-shot) 0.23 0.33 0.86 0 0 0 0.23\nTARS (few-shot) 0.32 0.76 0.86 0 0 0 0.35\nFlair embeddings 0.42 0.61 0.80 0 0 0 0.35\nTransformers 0.30 0.40 0 0 0 0 0.15\n14 Accuracy metrics by type of entity and total accuracy for all experiments can be found in Appendixes   A \nand BScientometrics \n1 3\nThe UNI and COR categories, though, have distinct patterns. In this case, the low \nperformance of the models for the COR and UNI categories could be explained by the \nsmall size of the training sample that contains these categories (see Fig.  2). Thus, the \nmodel was not able to identify patterns because of the lack of data.\nSecondly, low results for FUND, COR, MISC, and UNI categories might also lie in \nthe nature of the miscellaneous category, as some entities that fall into this category are \nsemantically very close to the FUND and COR categories. As a result, training without \na MISC category might potentially show better performance. To examine this hypoth-\nesis, we conducted training with Flair Embeddings with a dataset excluding the MISC \ncategory, i.e., with five entity types. Training results are shown in Fig. 6 A.\nAdditionally, the problem might lie in the nature of the training algorithms that \nwere used. On the one hand, Flair developers claimed Transformers to be the most effi-\ncient algorithm (Schweter & Akbik, 2020). On the other, the stacked embeddings are \nan important feature of the Flair tool, as a combination of different embeddings might \nbring better results than their separate uses (Akbik et  al., 2019). Thus, the combina-\ntion of the Transformer embeddings model with the contextual string embeddings might \nimprove the model performance. Thus, for the third additional training, we combined \ncontextual string embeddings with FLERT parameters.\nFig. 6  The results of Experiment 2. A–C comprise diagrams with the F1-scores of the training with three \nalgorithms for each label class. D Represents the total accuracy of the training algorithms Scientometrics\n1 3\nResults\nResults of the training are represented in Fig.  6. During the training with three types of \nentities (Fig.  6B) IND and GRNB still achieved high F1-scores of 0.96 (IND) and 0.95 \n(GRNB). Nevertheless, ORG gained only an F1-score of 0.64, which is worse than the \nprevious results with six entity types. The results of the training with five types of entities \nwere quite similar to those achieved during the training with six types of entities. FUND \nand UNI categories showed a small improvement in precision, recall, and F1 score com-\npared to training with 6 types of entities with Flair Embeddings. At the same time, the \nperformance of the COR category deteriorated noticeably (0.6 vs. the previous 0.7). The \nimprovement in overall accuracy (Fig.  6D) (0.80 vs. the previous 0.77) could be explained \nby the fact that the MISC category was not present in this training and could not affect \noverall accuracy with its low F1-score.\nAs Fig.  6C demonstrates, training with Flair Embeddings and RoBERTa showed no \nimprovements compared to the results of the primary training with Transformers and worse \nperformance compared with Flair Embeddings. As in Experiment 1, the COR category \nachieved high precision but low recall, resulting in a low F1-score (0.67). For some catego-\nries (COR and GRNB) Flair Embeddings combined with RoBERTa performed better than \nTransformers but still worse than Flair Embeddings.\nExperiment 3\nThe results of experiment 2 showed that altering the training parameters and decreasing the \nnumber of entity classes does not improve the model accuracy. We assume that increasing \nthe size of the training corpus would improve the performance of entities with low recogni-\ntion accuracy. Therefore, for this experiment, we designed two corpora with an increased \nnumber of acknowledged entities.\nAs the Flair Embeddings algorithm trained with Corpus No.2 showed the best perfor -\nmance, it was of interest if the increased training data will outperform its accuracy score. \nTraining in Experiments 1 and 2 was carried out using Flair version 0.9. As Flair recently \nupdated to version 0.11, we used this newest version for the following training. The training \nwas carried out with exactly the same parameters as the training with the Flair Embeddings \nFig. 7  The results of Experiment 3. A Comprises diagrams with the F1-scores of training with three cor -\npora for each label class. B Represents the total accuracy of the trainingScientometrics \n1 3\nmodel in Experiment 1 (Sect. 3.1). To achieve comparable results we also retrained, for \nnow, the best model (Flair Embeddings with Corpus No.2) with the Flair 0.11.\nResults\nResults of the training are represented in Fig.  7. Retraining of the original model with the \nFlair 0.11 Fig.  7-B showed slightly better performance (0.79 vs. 0.77) than training with \nversion 0.9. In general, no huge differences in accuracy were found during training with \nextended corpora.\nOverall, the best F1-Score for the FUND category (0.77) was reached with the TARS \nalgorithm and corpus No.2. COR gained the best accuracy (0.7) with Flair Embeddings \nand corpus No.2 using Flair version 0.9. The GRNB category showed the best perfor -\nmance (0.96) with Flair Embeddings trained on the corpus with five types of entities \n(Flair Embeddings 5 Ent). The best F1-Score of the IND category was achieved with \nFlair Embeddings trained on corpus No.2 with Flair version 0.11. MISC performed the \nbest (0.66) with Flair Embeddings trained on Corpus No.4 with Flair version 0.11. The \nbest accuracy of the UNI category was achieved with Flair Embeddings trained on corpus \nNo.3 with Flair version 0.11. In general, the best overall accuracy of 0.79 (for six entity \ntypes) had the Flair Embeddings model trained on corpus No.2 with Flair version 0.11.\nDiscussion\nAs expected, Experiment 1 showed a large improvement in accuracy for all algorithms \nwhen the size of a training corpus was increased from 49 to 654 sentences. However, fur -\nther enlargement of the corpus (in Experiment 3) did not make any progress. Some types \nof entity, such as IND and GRNB, showed great performance (GRNB with an F1-Score of \n0.96 or IND with 0.98) with the small training samples, i.e., 354 entities from the GRNB \ncategory or 439 entities from the IND category. At the same time, training with a sample of \n1322 labelled funding organisations achieved an F1-Score of only 0.75.\nThe TARS model is designed to perform NER with small or no training data. In \nexperiment 1, TARS without training data was able to extract individuals with quite high \naccuracy (F-1 score of 0.86). TARS trained with the small corpus (No. 1) did not show \nimprovement in the F-1 score of individuals, but greatly improved the F-1 score of the \nGRNB category. For other entity types, this model showed extremely weak results. It was \nexpected that training with Flair Embeddings and Transformers will not bring high recog-\nnition accuracy with corpus No.1, however, interesting results can be observed. Thus, Flair \nEmbeddings showed decent accuracy of 0.8 for individuals with the small training dataset.\nThe imbalance in the performance of different types of entities can be explained by the \nnature of the data, on which the original models were trained. Thus, Flair Embeddings were \ntrained on the 1-billion words English corpus (Chelba et al., 2013). RoBERTa was pre-trained \non the combination of five datasets containing news articles, blog entries, books, and Wikipe-\ndia articles. TARS was mainly pre-trained on datasets for text classification. Thus, the models \nused were not trained on domain-specific data. This can also explain the pure Transformers \nand TARS performance. The higher accuracy for the individuals category in the training with \nTARS can be explained by the fact, that the word ’person’ is semantically more straightforward \nthan other categories. The same could be applied to grant numbers. Furthermore, grant num-\nbers generally have similar patterns, which can be applied to all entities of this type, that can  Scientometrics\n1 3\nexplain a rapid improvement in F-1 score between zero-shot and few-shot models. Moreover, \nIND and GRNB categories showed better performance for other algorithms too, which could \nlie in the structure of these entities: names of individuals and grant numbers usually have undi-\nversified patterns and in acknowledgement texts are used in a small variety of contexts. At the \nsame time, other entity types, such as funding organisations and universities could have similar \npatterns and could be used in the same context. In some cases, even for human annotators, it \nis impossible to distinguish between university, funding body and corporation without back -\nground knowledge about the entity.\nPrevious works showed improvements in downstream tasks using embedding models \nfine-tuned for the domain used (Shen et al., 2022; Beltagy et al.., 2019). Therefore, fine-\ntuning the general language model on the sample of acknowledgment texts could improve \nthe performance of the NER model for acknowledgment texts. We are planning to fine-tune \nBERT and Flair Embeddings (contextual string embeddings) on a sample of approx. 5 mil-\nlion acknowledgment texts from WoS and evaluate the performance of the NER models.\nThe results of Experiment 2 generally did not show an improvement in accuracy. On the \ncontrary, training with the three entity types deteriorated the model performance. Train-\ning without the MISC category did not show significant performance progress either. \nMoreover, further analysis of acknowledged entities showed that the miscellaneous cate-\ngory contained very inhomogeneous and partly irrelevant data, making the analysis more \ncomplicated (Smirnova & Mayr, 2023). Therefore, we assume that the model would make \nbetter predictions if the number of entity types is expanded and miscellaneous categories \nexcluded, i.e., the MISC category could be split into the following categories: names of \nprojects, names of conferences, names of software and dataset. Different subcategories \ncould also be distinguished in the FUND category.\nCorpora No.2 and No.3 contain the same number of MISC and COR entities15, while \nin corpus 4 number of occurrences of MISC and COR entities is higher. For MISC and \nCOR, accuracy slightly increased with corpus 4, therefore we assume that the extraction \naccuracy for these entities will increase with the increase of the training data. The situa-\ntion is different for funding organizations and universities. The number of UNI and FUND \nentities increased evenly from corpus No.1 to corpus No.4. Nevertheless, the best result for \nthe UNI category was achieved with corpus No.3. The poor performance of corpus No.4 \ncould be explained by the inclusion of Indian funders. Thus, the names of many Indian \nfunders are very similar to the entities which usually fall into the UNI category, e.g., the \nDepartment of Science and Technology or the Department of Biotechnology. This pattern \nis more common to the entities which fall into the UNI category. Therefore, that might \nmake the exact extraction of UNI and FUND entities more confusing. Moreover, many \nIndian Universities contain the name of individuals, e.g., Rajiv Gandhi University, which \ncan cause confusion of the UNI category with the IND category. Generally, no improve-\nment in increasing the size of the corpus for the FUND category can be explained by the \nambiguous nature of the entities which fall into the FUND category and their semantical \nproximity with other types of entities. Analysis of the extracted entities showed that many \nentities were extracted correctly, but were assigned to the wrong category (Smirnova & \nMayr, 2023). Therefore, an additional classification algorithm applied to extracted entities \ncould improve the model’s performance.\n15 These differences in entity distribution are caused by the peculiarities of acknowledgement information \nstored in WoS. As only acknowledgements with indexed funding information are stored in the database, it \nwas difficult to find an adequate number of acknowledged entities of other typesScientometrics \n1 3\nConclusion\nIn this paper, we evaluated different embedding models for the task of automatic extraction \nand classification of acknowledged entities from acknowledgment texts16. The annotation \nof the training corpora was the most challenging and time-consuming task of all data prep-\naration procedures. Therefore, a semi-automated approach was used to help significantly \naccelerate the procedure.\nThe study’s main limitations were its small size and just one annotator of the training \ncorpora. Additionally, we used acknowledgments texts collected in WoS. WoS only stores \nacknowledgments containing funding information, therefore there was a lack of other types \nof entities, such as corporations or universities in the training data.\nIn the present paper, we aimed to answer three questions. Thus, regarding research ques-\ntion 1, the few-shot and zero-shot models showed very low total recognition accuracy. At \nthe same time, it was observed that some entities performed better than others with all \nalgorithms and training corpora. Thus, individuals gained a good F1-score over 0.8 with \nzero-shot and few-shot models, as also with Flair embeddings trained with the smallest \ncorpus. With the enlargement of the training corpora, the performance of the IND category \nalso increased and achieved an F1-score over 0.9. The GRNB category showed an adequate \nF-1 score of 0.76 with the few-shot algorithm trained with the smallest corpus, following \ntraining with corpus No.2 boosts the F-1 score to over 0.9. Therefore, few-shot and zero-\nshot approaches were not able to identify all the defined acknowledged entity classes.\nWith respect to research question 2, Flair Embeddings showed the best accuracy in \ntraining with corpus No.2 (and version 0.11) and the fastest training time compared to the \nother models; thus, it is recommended to further use the Flair Embeddings model for the \nrecognition of acknowledged entities.\nExploring research question 3 we observed, that the expansion of the size of a training \ncorpus from very small (corpus No.1) to medium size (corpus No.2) massively increased the \naccuracy of all training algorithms. The best-performing model (Flair Embedding) was further \nretrained with the two bigger corpora, but the following expansion of the training corpus did \nnot bring further improvement. Moreover, the performance of the model slightly deteriorated.\nAcknowledgements The original work was funded by the German Center for Higher Education Research \nand Science Studies (DZHW) via the project “Mining Acknowledgement Texts in Web of Science \n(MinAck)”17. Access to the WoS data was granted via the Competence Centre for Bibliometrics18. Data \naccess was funded by BMBF (Federal Ministry of Education and Research, Germany) under grant number \n01PQ17001. Nina Smirnova received funding from the German Research Foundation (DFG) via the project \n“POLLUX”19. The present paper is an extended version of the paper “Evaluation of Embedding Models for \nAutomatic Extraction and Classification of Acknowledged Entities in Scientific Documents” (Smirnova & \nMayr, 2022) presented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Sci-\nentific Documents (EEKE2022).\nAppendix A: Accuracy metrics by type of entity (label) for all \nexperiments\nSee Table 5.\n16 The best model can be tested  at https:// huggi ngface. co/ kalaw inka/ flair- ner- ackno  wledg ments\n17 https:// kalaw inka. github. io/ minack/.\n18 https:// www. bibli ometr  ie. info/ en/ index. php? id= home.\n19 https:// www. pollux- fid. de/ about. Scientometrics\n1 3\nTable 5  Accuracy metrics by type of entity (label) for all experiments\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings No.1 9 IND 0.7692 0.8333 0.8000 12 1\nFlair embeddings No.1 9 GRNB 0.5385 0.7000 0.6087 10 1\nFlair embeddings No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nFlair embeddings No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nFlair embeddings No.1 9 COR 0.0000 0.0000 0.0000 1 1\nFlair embeddings No.1 9 FUND 0.4000 0.4444 0.4211 18 1\nFlair embeddings No.2 9 FUND 0.6524 0.7771 0.7093 157 1\nFlair embeddings No.2 9 IND 0.9764 0.9831 0.9797 295 1\nFlair embeddings No.2 9 GRNB 0.9398 0.9750 0.9571 160 1\nFlair embeddings No.2 9 UNI 0.7527 0.7071 0.7292 99 1\nFlair embeddings No.2 9 MISC 0.6420 0.6341 0.6380 82 1\nFlair embeddings No.2 9 COR 0.8750 0.5833 0.7000 12 1\nTARS (pretrained) No.1 9 IND 1.0000 0.7500 0.8571 12 1\nTARS (pretrained) No.1 9 GRNB 0.7273 0.8000 0.7619 10 1\nTARS (pretrained) No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTARS (pretrained) No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTARS (pretrained) No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTARS (pretrained) No.1 9 FUND 0.3158 0.3333 0.3243 18 1\nTARS (pretrained) No.2 9 FUND 0.7257 0.8089 0.7651 157 1\nTARS (pretrained) No.2 9 IND 0.9281 0.8746 0.9005 295 1\nTARS (pretrained) No.2 9 GRNB 0.8895 0.9563 0.9217 160 1\nTARS (pretrained) No.2 9 UNI 0.7407 0.6061 0.6667 99 1\nTARS (pretrained) No.2 9 MISC 0.6719 0.5244 0.5890 82 1\nTARS (pretrained) No.2 9 COR 0.5000 0.5833 0.5385 12 1\nTransformers No.1 9 GRNB 0.3000 0.6000 0.4000 10 1\nTransformers No.1 9 IND 0.0000 0.0000 0.0000 12 1\nTransformers No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTransformers No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTransformers No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTransformers No.1 9 FUND 0.2414 0.3889 0.2979 18 1\nTransformers No.2 9 FUND 0.6211 0.7516 0.6801 157 1\nTransformers No.2 9 IND 0.9346 0.9695 0.9517 295 1\nTransformers No.2 9 GRNB 0.8704 0.8812 0.8758 160 1\nTransformers No.2 9 UNI 0.6476 0.6869 0.6667 99 1\nTransformers No.2 9 MISC 0.4767 0.5000 0.4881 82 1\nTransformers No.2 9 COR 0.7500 0.5000 0.6000 12 1\nFlair embeddings (3 Ent) No.2 9 IND 0.9577 0.9703 0.9639 303 2\nFlair embeddings (3 Ent) No.2 9 ORG 0.6400 0.6154 0.6275 208 2\nFlair embeddings (3 Ent) No.2 9 GRNB 0.9286 0.9750 0.9512 160 2\nFlair embeddings (5 Ent) No.2 9 IND 0.9764 0.9797 0.9780 295 2\nFlair embeddings (5 Ent) No.2 9 GRNB 0.9345 0.9812 0.9573 160 2\nFlair embeddings (5 Ent) No.2 9 UNI 0.7802 0.7172 0.7474 99 2\nFlair embeddings (5 Ent) No.2 9 COR 0.7500 0.5000 0.6000 12 2\nFlair embeddings (5 Ent) No.2 9 FUND 0.6722 0.7707 0.7181 157 2Scientometrics \n1 3\nAppendix B: Overall accuracy for all experiments\nSee Table 6.Rows are sorted by experiment number and algorithmTable 5  (continued)\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings (RoB-\nERTa)No.2 9 IND 0.9206 0.9831 0.9508 295 2\nFlair embeddings (RoB-\nERTa)No.2 9 GRNB 0.8896 0.9062 0.8978 160 2\nFlair embeddings (RoB-\nERTa)No.2 9 UNI 0.5963 0.6566 0.6250 99 2\nFlair embeddings (RoB-\nERTa)No.2 9 MISC 0.4135 0.5244 0.4624 82 2\nFlair embeddings (RoB-\nERTa)No.2 9 COR 1.0000 0.5000 0.6667 12 2\nFlair embeddings (RoB-\nERTa)No.2 9 FUND 0.6096 0.7261 0.6628 157 2\nFlair embeddings No.2 11 GRNB 0.9345 0.9812 0.9573 160 3\nFlair embeddings No.2 11 IND 0.9797 0.9831 0.9814 295 3\nFlair embeddings No.2 11 FUND 0.7027 0.8280 0.7602 157 3\nFlair embeddings No.2 11 UNI 0.7684 0.7374 0.7526 99 3\nFlair embeddings No.2 11 MISC 0.6543 0.6463 0.6503 82 3\nFlair embeddings No.2 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 UNI 0.8000 0.7273 0.7619 99 3\nFlair embeddings No.3 11 IND 0.9731 0.9797 0.9764 295 3\nFlair embeddings No.3 11 GRNB 0.9281 0.9688 0.9480 160 3\nFlair embeddings No.3 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 MISC 0.6571 0.5610 0.6053 82 3\nFlair embeddings No.3 11 FUND 0.6757 0.7962 0.7310 157 3\nFlair embeddings No.4 11 MISC 0.7424 0.5976 0.6622 82 3\nFlair embeddings No.4 11 COR 0.8571 0.5000 0.6316 12 3\nFlair embeddings No.4 11 UNI 0.7753 0.6970 0.7340 99 3\nFlair embeddings No.4 11 IND 0.9698 0.9797 0.9747 295 3\nFlair embeddings No.4 11 FUND 0.6823 0.8344 0.7507 157 3\nFlair embeddings No.4 11 GRNB 0.9162 0.9563 0.9358 160 3 Scientometrics\n1 3\nFunding Open access funding enabled and organized by Projekt DEAL.\nDeclarations \n Conflict of interest Philipp Mayr, the co-author of this paper, has a conflict of interest because he serves on \nthe editorial board of the journal Scientometrics. In addition, he is a co-guest editor of the special issue on \n\"Extraction and Evaluation of Knowledge Entities from Scientific Documents\". He declares that he has noth-\ning to do with the decision about this paper submission.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAkbik, A., Bergmann, T., Blythe, D., Rasul, K., Schweter, S., & Vollgraf, R. 2019. FLAIR: An Easy-to-Use \nFramework for State-of-the-Art NLP. Minneapolis, Minnesota (pp. 54–59). Association for Computa-\ntional Linguistics.\nAkbik, A., Blythe, D., & Vollgraf, R. (2018). Contextual string embeddings for sequence labeling. In 2018, \n27th International Conference on Computational Linguistics (pp. 1638–1649).\nAlexandera, D. & Vries, A. P. (2021). This research is funded by...”: Named Entity Recognition of financial \ninformation in research papers. In BIR 2021: 11th International Workshop on Bibliometric-enhanced \nInformation Retrieval at ECIR (pp. 102–110).\nBeltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. \nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing \nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)  \n(pp. 3613–3618). Association for Computational Linguistics.Table 6  Overall accuracy for all experiments\nRows are sorted by experiment number and algorithmAlgorithm Corpus Version Accuracy Experiment\nFlair embeddings No.2 9 0.7702 1\nFlair embeddings No.1 9 0.3472 1\nTARS (pretrained) No.2 9 0.7113 1\nTARS (pretrained) No.1 9 0.3485 1\nTransformers No.2 9 0.6783 1\nTransformers No.1 9 0.1477 1\nFlair Embeddings (3 Entity Types) No.2 9 0.7536 2\nFlair embeddings (5 Entity Types) No.2 9 0.7990 2\nFlair embeddings + RoBERTa No.2 9 0.6697 2\nFlair Embeddings No.2 11 0.7869 3\nFlair embeddings No.4 11 0.7814 3\nFlair embeddings No.3 11 0.7691 3Scientometrics \n1 3\nBorst, T., Mielck, J., Nannt, M., & Riese, W. (2022). Extracting funder information from scien-\ntific papers—Experiences with question answering. In Silvello, G., O.  Corcho, P.  Manghi, G.M. \nDi Nunzio, K. Golub, N. Ferro, and A. Poggi (Eds.),Linking theory and practice of digital libraries  \n(Vol. 13541, pp. 289–296). Springer International Publishing. Series Title: Lecture Notes in Com-\nputer Science. https:// doi. org/ 10. 1007/ 978-3- 031- 16802-4_ 24.\nChelba, C., T.  Mikolov, M.  Schuster, Q.  Ge, T.  Brants, P.  Koehn, & Robinson, T. (2013). One Bil-\nlion Word Benchmark for Measuring Progress in Statistical Language Modeling. 10.48550/\nARXIV.1312.3005 .\nChen, H., Song, X., Jin, Q., & Wang, X. (2022). Network dynamics in university-industry collaboration: \nA collaboration-knowledge dual-layer network perspective. Scientometrics, 127(11), 6637–6660. \nhttps:// doi. org/ 10. 1007/ s11192- 022- 04330-9\nCronin, B. (1995). The Scholar’s courtesy: The role of acknowledgement in the primary communication \nprocess. Taylor Graham.\nCronin, B., & Weaver, S. (1995). The praxis of acknowledgement: From bibliometrics to influmetrics. \nRevista Española de Documentación Científica, 18(2), 172.\nDevlin, J., Chang, M.W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional \ntransformers for language understanding. 10.48550/ARXIV.1810.04805 .\nDiaz-Faes, A. A., & Bordons, M. (2017). Making visible the invisible through the analysis of acknowl-\nedgements in the humanities. Aslib Journal of Information Management, 69(5), 576–590. https://  \ndoi. org/ 10. 1108/ AJIM- 01- 2017- 0008\nDoehne, M., & Herfeld, C. (2023). How academic opinion leaders shape scientific ideas: an acknowl-\nedgment analysis., 128(4), 2507–2533. https:// doi. org/ 10. 1007/ s11192- 022- 04623-z\nDzieżyc, M., & Kazienko, P. (2022). Effectiveness of research grants funded by European research coun-\ncil and polish national science centre. Journal of Informetrics, 16(1), 101243. https:// doi. org/ 10.  \n1016/j. joi. 2021. 101243\nEftimov, T., Koroušić Seljak, B., & Korošec, P. (2017). A rule-based named-entity recognition \nmethod for knowledge extraction of evidence-based dietary recommendations. PLoS ONE, 12(6), \ne0179488. https:// doi. org/ 10. 1371/ journ al. pone. 01794 88\nEtzioni, O., Cafarella, M., Downey, D., Popescu, A. M., Shaked, T., Soderland, S., Weld, D. S., & Yates, \nA. (2005). Unsupervised named-entity extraction from the web: An experimental study. Artificial \nIntelligence, 165(1), 91–134. https:// doi. org/ 10. 1016/j. artint. 2005. 03. 001\nFinkel, J.R., Grenager, T., & Manning, C. (2005). Incorporating non-local information into information \nextraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL’05), Ann Arbor, Michigan (pp. 363–370). Association for \nComputational Linguistics.\nGiles, C. L., & Councill, I. G. (2004). Who gets acknowledged: Measuring scientific contributions \nthrough automatic acknowledgment indexing. Proceedings of the National Academy of Sciences,  \n101(51), 17599–17604. https:// doi. org/ 10. 1073/ pnas. 04077 43101\nHalder, K., Akbik, A., Krapac, J., & Vollgraf, R. (2020). Task-Aware Representation of Sentences for \nGeneric Text Classification. In Proceedings of the 28th International Conference on Computational \nLinguistics, Barcelona, Spain (Online) (pp. 3202–3213). International Committee on Computa-\ntional Linguistics.\nHubbard, D., Laddusaw, S., Tan, Q., & Hu, X. (2022). Analysis of acknowledgments of libraries in the \njournal literature using machine learning. Proceedings of the Association for Information Science \nand Technology, 59(1), 709–711. https:// doi. org/ 10. 1002/ pra2. 698\nIovine, A., Fang, A., Fetahu, B., Rokhlenko, O., & Malmasi, S. (2022). CycleNER: An unsupervised \ntraining approach for named entity recognition. In Proceedings of the ACM Web Conference 2022  \n(pp. 2916–2924). ACM.\nJiang, L., Kang, X., Huang, S., & Yang, B. (2022). A refinement strategy for identification of scientific \nsoftware from bioinformatics publications. Scientometrics, 127(6), 3293–3316. https:// doi. org/ 10.  \n1007/ s11192- 022- 04381-y\nKassirer, J. P., & Angell, M. (1991). On authorship and acknowledgments. The New England Journal of \nMedicine, 325(21), 1510–1512. https:// doi. org/ 10. 1056/ NEJM1 99111 21325 2112\nKayal, S., Afzal, Z., Tsatsaronis, G., Katrenko, S., Coupet, P., Doornenbal, M., & Gregory, M. (2017). \nTagging funding agencies and grants in scientific articles using sequential learning models. In \nBioNLP 2017, Vancouver, Canada (pp. 216–221). Association for Computational Linguistics.\nKenekayoro, P. (2018). Identifying named entities in academic biographies with supervised learning. \nScientometrics, 116(2), 751–765. https:// doi. org/ 10. 1007/ s11192- 018- 2797-4\nKingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. 10.48550/\nARXIV.1412.6980 . Scientometrics\n1 3\nKusumegi, K., & Sano, Y. (2022). Dataset of identified scholars mentioned in acknowledgement state-\nments. Scientific Data, 9(1), 461. https:// doi. org/ 10. 1038/ s41597- 022- 01585-y\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoy -\nanov, V. (2019). RoBERTa: A robustly optimized BERT pretraining approach. arXiv: 1907. 11692  \n[cs] .\nMackintosh, K. (1972). Acknowledgements patterns in sociology. Ph. D. thesis, University of Oregon.\nMccain, K. (2017). Beyond Garfield’s citation index: An assessment of some issues in building a per -\nsonal name acknowledgments index. Scientometrics. https:// doi. org/ 10. 1007/ s11192- 017- 2598-1\nMcCain, K. W. (1991). Communication, competition, and secrecy: The production and dissemination of \nresearch-related information in genetics. Science, Technology, & Human Values, 16(4), 491–516. \nhttps:// doi. org/ 10. 1177/ 01622 43991 01600 404\nMejia, C., & Kajikawa, Y. (2018). Using acknowledgement data to characterize funding organizations \nby the types of research sponsored: the case of robotics research. Scientometrics, 114(3), 883–904. \nhttps:// doi. org/ 10. 1007/ s11192- 017- 2617-2\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, \nN., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkur -\nthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: An imperative style, high-\nperformance deep learning library. arXiv: 1912. 01703 [cs, stat].\nPaul-Hus, A., & Desrochers, N. (2019). Acknowledgements are not just thank you notes: A qualitative \nanalysis of acknowledgements content in scientific articles and reviews published in 2015. PLoS \nONE, 14, e0226727. https:// doi. org/ 10. 1371/ journ al. pone. 02267 27\nPaul-Hus, A., Díaz-Faes, A., Sainte-Marie, M., Desrochers, N., Costas, R., & Larivière, V. (2017). \nBeyond funding: Acknowledgement patterns in biomedical, natural and social sciences. PLoS ONE,  \n12, e0185578. https:// doi. org/ 10. 1371/ journ al. pone. 01855 78\nPennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors for Word Representation. \nIn Empirical Methods in Natural Language Processing (EMNLP) (pp. 1532–1543).\nPustejovsky, J., & Stubbs, A. (2012). Natural language annotation for machine learning. O’Reilly \nMedia Inc.\nRose, M., & Georg, C. P. (2021). What 5,000 acknowledgements tell us about informal collaboration \nin financial economics. Research Policy, 50, 104236. https:// doi. org/ 10. 1016/j. respol. 2021. 104236\nSang, T. K., & E. F., & De Meulder, F. (2003). Introduction to the CoNLL-2003 shared task: Language-\nindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Lan-\nguage Learning at HLT-NAACL (pp. 142–147).\nSchweter, S., & Akbik, A. (2020). FLERT: Document-level features for named entity recognition. ArXiv. \n10.48550/arXiv.2011.06993 .\nShen, S., Liu, J., Lin, L., Huang, Y., Zhang, L., Liu, C., Feng, Y., & Wang, D. (2022). SsciBERT: A \npre-trained language model for social science texts. Scientometrics. https:// doi. org/ 10. 1007/  \ns11192- 022- 04602-4\nSingh, V. K., Singh, P., Karmakar, M., Leta, J., & Mayr, P. (2021). The journal coverage of web of sci-\nence, scopus and dimensions: A comparative analysis. Scientometrics, 126(6), 5113–5142. https://  \ndoi. org/ 10. 1007/ s11192- 021- 03948-5\nSmirnova, N., & Mayr, P. (2022). Evaluation of embedding models for automatic extraction and classifi-\ncation of acknowledged entities in scientific documents. In 3rd Workshop on Extraction and Evalu-\nation of Knowledge Entities from Scientific Documents 2022 (EEKE 2022) (pp. 48–55). CEUR-WS.\norg.\nSmirnova, N., & Mayr, P. (2023). A comprehensive analysis of acknowledgement texts in web of sci-\nence: A case study on four scientific domains. Scientometrics, 1(128), 709–734. https:// doi. org/ 10.  \n1007/ s11192- 022- 04554-9\nSong, M., Kang, K. Y., Timakum, T., & Zhang, X. (2020). Examining influential factors for acknowl-\nedgements classification using supervised learning. PLoS ONE. https:// doi. org/ 10. 1371/ journ al.  \npone. 02289 28\nThomer, A. K., & Weber, N. M. (2014). Using named entity recognition as a classification heuristic. In \niConference 2014 Proceedings (pp. 1133 – 1138). iSchools.\nWang, J., & Shapira, P. (2011). Funding acknowledgement analysis: An enhanced tool to investigate \nresearch sponsorship impacts: The case of nanotechnology. Scientometrics, 87(3), 563–586. https://  \ndoi. org/ 10. 1007/ s11192- 011- 0362-5\nYamada, I., Asai, A., Shindo, H., Takeda, H., & Matsumoto, Y. (2020). LUKE: Deep contextualized \nentity representations with entity-aware self-attention. In Proceedings of the 2020 Conference on \nEmpirical Methods in Natural Language Processing (EMNLP) (pp. 6442–6454). Association for \nComputational Linguistics.Scientometrics \n1 3\nYu, J., Bohnet, B., & Poesio, M. (2020). Named entity recognition as dependency parsing. 10.48550/\nARXIV.2005.07150.\nZhang, C., Mayr, P., Lu, W., & Zhang, Y. (2023). Guest editorial: Extraction and evaluation of knowledge \nentities in the age of artificial intelligence. Aslib Journal of Information Management, 75, 433–437. \nhttps:// doi. org/ 10. 1108/ AJIM- 05- 2023- 507",
        "type": "direct"
    },
    {
        "citation": "Computational Linguistics, 2018",
        "content": ". Flair has three default training algorithms for \nNER which were used for the first experiment in the present research: a) NER Model with \nFlair Embeddings (later on Flair Embeddings)",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2020",
        "content": ", b) NER Model with \nTransformers (later on Transformers)",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2020",
        "content": ", and c) Zero-shot NER \nwith TARS (later on TARS)",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2018",
        "content": "8.\nThe Flair Embeddings model uses stacked embeddings, i.e., a combination of contex-\ntual string embeddings",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2019",
        "content": "with a static embeddings model. This approach \nwill generate different embeddings for the same word depending on its context. Stacked \nembedding is an important Flair feature, as a combination of different embeddings might \nbring better results than their separate uses",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2020",
        "content": ".\nThe Transformers model or FLERT-extension (document-level features for NER) is a \nset of settings to perform a NER on the document level using fine-tuning and feature-based \nFig. 1  An example of acknowledged entities. Each entity type is marked with a distinct color\n8 New transformer models as SciBERT or SsciBERT were not evaluated in this study, as the objective of \nthe study is to evaluate the performance of the Flair default models.Scientometrics \n1 3\nLSTM-CRF with the multilingual XML-RoBERTa transformer model (Schweter & Akbik, \n2020).\nThe TARS (task-aware representation of sentences) is a transformer-based model, \nwhich allows performing training without any training data (zero-shot learning) or with a \nsmall dataset (few-short learning)",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2017",
        "content": ". The TARS approach differs from \nthe traditional transfer learning approach in the way that the TARS model also considers \nsemantic information captured in the class labels themselves. For example, for analyzing \nacknowledgments, class labels like funding organization or university already carry seman-\ntic information.\nTraining data\nThe Web of Science (WoS) database was used to harvest the training data (funding \nacknowledgments).9 From 2008 on, WoS started indexing information about funders and \ngrants. WoS uses information from different funding reporting systems such as Research-\nfish,10 Medline11 and others. As WoS contains millions of metadata records (Singh et al., \n2021), the data chosen for the present study was restricted by year and scientific domain \n(for the corpora Nos. 1, 2, and 3) or additionally by the affiliation country (for corpus No.4). \nTo construct corpora Nos. 1-3 records from four different scientific domains published \nfrom 2014 to 2019 were considered: two domains from the social sciences (sociology and \neconomics) and oceanography and computer science. Different scientific domains were Table 2  Number of sentences/texts in the training corpora\nCorpus No. Training set (train) Test set (test) Validation set (dev) Total\n1 29/27 10/10 10/10 49/47\n2 339/282 165/150 150/136 654/441\n3 784/657 165/150 150/136 1099/816\n4 1148/885 165/150 150/136 1463/1044\nTable 3  Number of sentences/texts from each scientific domain in the training corpora\nCorpus No. Oceanography Economics Social Sciences Computer Science\n1 13/13 3/3 20/20 16/14\n2 127/75 92/58 351/234 173/129\n3 175/112 128/89 590/434 333/269\n9 The present research was conducted in scopes of two projects: MinAck (https:// kalaw inka. github. io/ \nminack/) and SEASON (https:// github. com/ kalaw inka/ season). Corpora Nos.1, 2, and 3 were created for the \nMinAck project and serve the purpose of a general evaluation of the impact of the size of the training cor -\npus on the model performance. Corpus No.4 was designed specifically for the SEASON project in the hope \nof improving the recognition of Indian funding information. The project SEASON aims to get insight into \nGerman-Indian scientific collaboration. Our other corpora mainly contain papers published by European \ninstitutions. That is why we enhance Corpus 4 with the papers published by Indian institutions.\n10 https:// resea rchfi sh. com/.\n11 https:// www. nlm. nih. gov/ bsd/ fundi ng_ suppo rt. html. Scientometrics\n1 3\nchosen since previous work on acknowledgment analysis revealed the relations between \nthe scientific domain and the types of acknowledged entities, i.e., acknowledged individu-\nals are more characteristic of theoretical and social-oriented domains. At the same time, \ninformation on technical and instrumental support is more common for the natural and \nlife sciences domains",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2014",
        "content": ". Only the WoS record types “article” \nand “review” published in a scientific journal in English were selected; then 1000 distinct \nacknowledgments texts were randomly gathered from this sample for the training dataset. \nFurther different amounts of sentences containing acknowledged entities were distributed \ninto the differently-sized training corpora. Table  2 demonstrates the number of sentences \nin each set in the four corpora. We selected only sentences that contain an acknowledged \nentity, regardless of the scientific domain. Table  3 contains the number of sentences and \ntexts from each scientific domain in the training corpora.12 The same article can belong to \nseveral scientific domains, therefore, the number of sentences and texts in Tables  2 and 3 \ndoes not match. Corpus No.4 was designed in such a way that all the training data from the \nCorpus No.3 was enhanced with acknowledgments texts from the articles that have Indian \naffiliations regardless of scientific domain or publication date.\nPreliminary analysis of the WoS data showed that the indexing of WoS funding infor -\nmation has several issues. The WoS includes only acknowledgments containing funding \ninformation; therefore, not every WoS entry has an acknowledgment, individuals are not \nincluded, and indexed funding organizations are not divided into different entity types like \nuniversities, corporations, etc. Therefore, the existing indexing of funding organizations \nis incomplete. Furthermore, there is a disproportion between the occurrences of acknowl-\nedged entities of different types. Thus, the most frequent entity types in the dataset with \nthe training data are IND, FUND and GRNB, followed by UNI and MISC. COR is the \nFig. 2  The distribution of acknowledged entities in the training corpora\n12 Corpus No.4 is not in Table  2, because the corpus contains additional acknowledgment texts from arti-\ncles with Indian affiliations regardless of the scientific domain and therefore contains different scientific \ndomains.Scientometrics \n1 3\ncategory most underrepresented in the data set. Consequently, there are different amounts \nof entities of different types in the training corpora (as Fig.  2 demonstrates), which might \nhave influenced the training results. Training with the corpora Nos. 2, 3, and 4 was evalu-\nated on the same training and validation datasets to ensure plausible accuracy (Fig.  3-B). \nFig. 3  The distribution of acknowledged entities in the test and validation corpora\nFig. 4  Annotation flowchart Scientometrics\n1 3\nHowever, training with corpus No.1 was evaluated with the smaller test and validation sets, \nas corpus No.1 contains a smaller number of sentences (Fig. 3-A).\nData annotation\nThe training corpus was annotated with six types of entities. As WoS already contains \nsome indexed funding information, it was decided to develop a semi-automated approach \nfor data annotation (as Fig.  4 demonstrates) and use indexed information provided by WoS, \ntherefore, grant numbers were adopted from the WoS indexing unaltered.\nFlair has a pre-trained 4-class NER Flair model (CoNLL-03).13 The model can pre-\ndict four tags: PER (person name), LOC (location), ORG (organization name), and MISC \n(other names). As Flair showed adequate results in the extraction of names of individu-\nals, it was decided to apply the pre-trained 4-class CoNLL-03 Flair model to the training \ndataset. Entities that fell into the PER category were added as the IND annotation to the \ntraining corpus. Furthermore, we noticed that some funding information was partially cor -\nrectly extracted into the ORG and MISC categories. Therefore, WoS funding organization \nindexing and entities from the ORG and MISC categories were adopted and distinguished \nbetween three categories (FUND, COR, and UNI) using regular expressions. In addition, \nthe automatic classification of entities was manually examined and reviewed. Mismatched \ncategories, partially extracted entities, and not extracted entities were corrected. Acknowl-\nedged entities, which fall into the MISC category, were manually annotated by one annota-\ntor. In the miscellaneous category entities referring to names of the conferences and pro-\njects were included.\nExperiments\nIn the present paper, we evaluated three default Flair NER models with four differently-\nsized corpora. In total, we performed three experiments. In the first experiment, mod-\nels with the default parameter were evaluated using corpora Nos. 1 and 2. In the second \nexperiment, we evaluated Flair Embeddings and Transformers model with altered training \nparameters and corpus No.2. In the third experiment, the first experiment was replicated \nwith corpora Nos. 3 and 4.\nExperiment 1\nIn the first experiment, we tested the TARS model zero-shot and few-shot scenarios (with \ncorpus No. 1), as well as the performance of two default FLAIR models (Flair Embeddings \nand Transformers) with corpus No.2. Additionally, the performance of Flair Embeddings \nand Transformers models was tested with the corpus No.1 The training was conducted with \nthe recommended parameters for all algorithms, as Flair developers specifically ran vari-\nous tests to find the best hyperparameters for the default models. For the few-shot TARS, \nthe training was conducted with the small dataset (corpus No.1), and for Transformers and \nFlair Embeddings with a larger dataset (corpus No.2).\n13 https:// github. com/ flair NLP/ flairScientometrics \n1 3\nThe Flair Embeddings model was initiated as a combination of static and contextual \nstring embeddings. We applied GloVe",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2019",
        "content": "as a static word-level \nembedding model. Thus, in our case, stacked embeddings comprise GloVe embeddings, \nforward contextual string embeddings, and backward contextual string embeddings. The \nmodel was trained with the recommended parameters: the size of mini-batches was set to \n32 and the maximum number of epochs was set to 150.\nFor Transformers, training was initiated with the RoBERTa model",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2020",
        "content": ". For \nthe present paper, a fine-tuning approach was used. The fine-tuning procedure consisted of \nadding a linear layer to a transformer and retraining the entire network with a small learn-\ning rate. We used a standard approach, where only a linear classifier layer was added on the \ntop of the transformer, as adding the additional CRF decoder between the transformer and \nlinear classifier did not increase accuracy compared with this standard approach (Schweter \n& Akbik, 2020). The chosen transformer model uses subword tokenization. We used the \nmean of embeddings of all subtokens and concatenation of all transformer layers to pro-\nduce embeddings. The context around the sentence was considered. The training was initi-\nated with a small learning rate using the Adam Optimisation Algorithm (Kingma & Ba, \n2014).\nThe TARS model requires labels to be defined in a natural language. Therefore, we \ntransformed our original coded labels into the natural language: FUND - “Funding \nAgency”, IND - “Person”, COR - “Corporation”, GRNB - “Grant Number\", UNI - “Uni-\nversity”, and MISC - “Miscellaneous”. The training for the few-shot approach was initiated \nwith the TARS NER model",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2020",
        "content": ".\nResults\nOverall, the training demonstrated mixed results. Table  4 shows training results with cor -\npus No.1 and the TARS zero-shot approach. GRNB showed adequate results by training \nwith Flair Embeddings and TARSfew-shot models. IND was the best-recognized entity by \ntraining with Flair Embeddings and TARS (both zero- and few-shot) with an F1-score of \n0.8 (Flair Embeddings) and 0.86 (TARS) respectively. Training with Transformers was not \nsuccessful for IND with an F1-score of 0. In general, transformers were a less efficient \nalgorithm for training with a small dataset with an overall accuracy of 0.35. FUND dem-\nonstrated not satisfactory results with F1-score of less than 0.5 for all models. Entity types \nFig. 5  The training results with the training corpus No.2. A Comprises diagrams with the F1-scores of the \ntraining with three algorithms for each label class. B depicts the total accuracy of training algorithms Scientometrics\n1 3\nMISC, UNI, and COR showed the worst results with the F1-score equal to zero for all algo-\nrithms. The low accuracy for MISC, UNI, and COR resulted in low overall accuracy for \nall algorithms. Overall, training with corpus No.1 showed insufficient results for all algo-\nrithms. Flair Embeddings and TARS showed better accuracy compared to Transformers.\nFigure  5 shows the training results with corpus No.2. Similar to the training with corpus \nNo.1, IND and GRNB are the best-recognized categories. The best results for IND and \nGRNB demonstrated Flair embeddings with an F1-score of 0.98 (IND) and 0.96 (GRNB). \nTARS achieved the best results for FUND with an F1-score of 0.77 against 0.71 for Flair \nEmbeddings and 0.68 for Transformers. Miscellaneous demonstrated the worst accuracy \nfor Flair Embeddings (0.64) and Transformers (0.49), while for TARS the worst accuracy \nlies in the COR category with an F1-score of 0.54. The best result for UNI showed Flair \nEmbeddings with an F1-score over 0.7. The COR category showed a decent precision \nof 0.88 with Flair Embeddings but a low recall of 0.58 which resulted in a low F1-Score \n(0.7)14.\nTraining with corpus No.2 showed a significant improvement in training accuracy \n(Fig.  5B). Overall, Flair Embeddings was more accurate than other training algorithms, \nalthough training with TARS showed better results for the FUND category. The Trans-\nformers showed the worst results during training.\nAdditionally, a zero-shot approach was tested for the TARS model on corpus no.1. The \nmodel was able to successfully recognize individuals, but struggled with other categories, \nas Table 4 demonstrates. The total accuracy of the model comprises 0.23.\nExperiment 2\nOur first hypothesis to explain the pure model performance for the FUND, COR, MISC, \nand UNI categories is their semantic proximity that prevents successful recognition. \nEntities of these categories are often used in the same context. To examine this hypoth-\nesis, we conducted an experiment using Flair Embeddings with the dataset contain-\ning three types of entities: IND, GRNB, and ORG. The MISC category was excluded \nfrom the training, as one of the aims of the present research is to extract information \nabout acknowledged entities, and the MISC category contains only additional informa-\ntion. The new ORG category was established, which includes a combination of entities \nfrom the FUND, COR, and UNI categories. The training was performed with exactly \nthe same parameters as training with the Flair Embeddings model in Experiment 1 \n(Sect. 4.1).Table 4  F1-scores of the training with three algorithms for each label class with Corpus No. 1\nAlgorithm FUND GRNB IND UNI COR MISC accuracy\nTARS (zero-shot) 0.23 0.33 0.86 0 0 0 0.23\nTARS (few-shot) 0.32 0.76 0.86 0 0 0 0.35\nFlair embeddings 0.42 0.61 0.80 0 0 0 0.35\nTransformers 0.30 0.40 0 0 0 0 0.15\n14 Accuracy metrics by type of entity and total accuracy for all experiments can be found in Appendixes   A \nand BScientometrics \n1 3\nThe UNI and COR categories, though, have distinct patterns. In this case, the low \nperformance of the models for the COR and UNI categories could be explained by the \nsmall size of the training sample that contains these categories (see Fig.  2). Thus, the \nmodel was not able to identify patterns because of the lack of data.\nSecondly, low results for FUND, COR, MISC, and UNI categories might also lie in \nthe nature of the miscellaneous category, as some entities that fall into this category are \nsemantically very close to the FUND and COR categories. As a result, training without \na MISC category might potentially show better performance. To examine this hypoth-\nesis, we conducted training with Flair Embeddings with a dataset excluding the MISC \ncategory, i.e., with five entity types. Training results are shown in Fig. 6 A.\nAdditionally, the problem might lie in the nature of the training algorithms that \nwere used. On the one hand, Flair developers claimed Transformers to be the most effi-\ncient algorithm",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2019",
        "content": ". On the other, the stacked embeddings are \nan important feature of the Flair tool, as a combination of different embeddings might \nbring better results than their separate uses",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2013",
        "content": ". Thus, the combina-\ntion of the Transformer embeddings model with the contextual string embeddings might \nimprove the model performance. Thus, for the third additional training, we combined \ncontextual string embeddings with FLERT parameters.\nFig. 6  The results of Experiment 2. A–C comprise diagrams with the F1-scores of the training with three \nalgorithms for each label class. D Represents the total accuracy of the training algorithms Scientometrics\n1 3\nResults\nResults of the training are represented in Fig.  6. During the training with three types of \nentities (Fig.  6B) IND and GRNB still achieved high F1-scores of 0.96 (IND) and 0.95 \n(GRNB). Nevertheless, ORG gained only an F1-score of 0.64, which is worse than the \nprevious results with six entity types. The results of the training with five types of entities \nwere quite similar to those achieved during the training with six types of entities. FUND \nand UNI categories showed a small improvement in precision, recall, and F1 score com-\npared to training with 6 types of entities with Flair Embeddings. At the same time, the \nperformance of the COR category deteriorated noticeably (0.6 vs. the previous 0.7). The \nimprovement in overall accuracy (Fig.  6D) (0.80 vs. the previous 0.77) could be explained \nby the fact that the MISC category was not present in this training and could not affect \noverall accuracy with its low F1-score.\nAs Fig.  6C demonstrates, training with Flair Embeddings and RoBERTa showed no \nimprovements compared to the results of the primary training with Transformers and worse \nperformance compared with Flair Embeddings. As in Experiment 1, the COR category \nachieved high precision but low recall, resulting in a low F1-score (0.67). For some catego-\nries (COR and GRNB) Flair Embeddings combined with RoBERTa performed better than \nTransformers but still worse than Flair Embeddings.\nExperiment 3\nThe results of experiment 2 showed that altering the training parameters and decreasing the \nnumber of entity classes does not improve the model accuracy. We assume that increasing \nthe size of the training corpus would improve the performance of entities with low recogni-\ntion accuracy. Therefore, for this experiment, we designed two corpora with an increased \nnumber of acknowledged entities.\nAs the Flair Embeddings algorithm trained with Corpus No.2 showed the best perfor -\nmance, it was of interest if the increased training data will outperform its accuracy score. \nTraining in Experiments 1 and 2 was carried out using Flair version 0.9. As Flair recently \nupdated to version 0.11, we used this newest version for the following training. The training \nwas carried out with exactly the same parameters as the training with the Flair Embeddings \nFig. 7  The results of Experiment 3. A Comprises diagrams with the F1-scores of training with three cor -\npora for each label class. B Represents the total accuracy of the trainingScientometrics \n1 3\nmodel in Experiment 1 (Sect. 3.1). To achieve comparable results we also retrained, for \nnow, the best model (Flair Embeddings with Corpus No.2) with the Flair 0.11.\nResults\nResults of the training are represented in Fig.  7. Retraining of the original model with the \nFlair 0.11 Fig.  7-B showed slightly better performance (0.79 vs. 0.77) than training with \nversion 0.9. In general, no huge differences in accuracy were found during training with \nextended corpora.\nOverall, the best F1-Score for the FUND category (0.77) was reached with the TARS \nalgorithm and corpus No.2. COR gained the best accuracy (0.7) with Flair Embeddings \nand corpus No.2 using Flair version 0.9. The GRNB category showed the best perfor -\nmance (0.96) with Flair Embeddings trained on the corpus with five types of entities \n(Flair Embeddings 5 Ent). The best F1-Score of the IND category was achieved with \nFlair Embeddings trained on corpus No.2 with Flair version 0.11. MISC performed the \nbest (0.66) with Flair Embeddings trained on Corpus No.4 with Flair version 0.11. The \nbest accuracy of the UNI category was achieved with Flair Embeddings trained on corpus \nNo.3 with Flair version 0.11. In general, the best overall accuracy of 0.79 (for six entity \ntypes) had the Flair Embeddings model trained on corpus No.2 with Flair version 0.11.\nDiscussion\nAs expected, Experiment 1 showed a large improvement in accuracy for all algorithms \nwhen the size of a training corpus was increased from 49 to 654 sentences. However, fur -\nther enlargement of the corpus (in Experiment 3) did not make any progress. Some types \nof entity, such as IND and GRNB, showed great performance (GRNB with an F1-Score of \n0.96 or IND with 0.98) with the small training samples, i.e., 354 entities from the GRNB \ncategory or 439 entities from the IND category. At the same time, training with a sample of \n1322 labelled funding organisations achieved an F1-Score of only 0.75.\nThe TARS model is designed to perform NER with small or no training data. In \nexperiment 1, TARS without training data was able to extract individuals with quite high \naccuracy (F-1 score of 0.86). TARS trained with the small corpus (No. 1) did not show \nimprovement in the F-1 score of individuals, but greatly improved the F-1 score of the \nGRNB category. For other entity types, this model showed extremely weak results. It was \nexpected that training with Flair Embeddings and Transformers will not bring high recog-\nnition accuracy with corpus No.1, however, interesting results can be observed. Thus, Flair \nEmbeddings showed decent accuracy of 0.8 for individuals with the small training dataset.\nThe imbalance in the performance of different types of entities can be explained by the \nnature of the data, on which the original models were trained. Thus, Flair Embeddings were \ntrained on the 1-billion words English corpus",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics, 2022; Beltagy et al.., 2019",
        "content": ". RoBERTa was pre-trained \non the combination of five datasets containing news articles, blog entries, books, and Wikipe-\ndia articles. TARS was mainly pre-trained on datasets for text classification. Thus, the models \nused were not trained on domain-specific data. This can also explain the pure Transformers \nand TARS performance. The higher accuracy for the individuals category in the training with \nTARS can be explained by the fact, that the word ’person’ is semantically more straightforward \nthan other categories. The same could be applied to grant numbers. Furthermore, grant num-\nbers generally have similar patterns, which can be applied to all entities of this type, that can  Scientometrics\n1 3\nexplain a rapid improvement in F-1 score between zero-shot and few-shot models. Moreover, \nIND and GRNB categories showed better performance for other algorithms too, which could \nlie in the structure of these entities: names of individuals and grant numbers usually have undi-\nversified patterns and in acknowledgement texts are used in a small variety of contexts. At the \nsame time, other entity types, such as funding organisations and universities could have similar \npatterns and could be used in the same context. In some cases, even for human annotators, it \nis impossible to distinguish between university, funding body and corporation without back -\nground knowledge about the entity.\nPrevious works showed improvements in downstream tasks using embedding models \nfine-tuned for the domain used . Therefore, fine-\ntuning the general language model on the sample of acknowledgment texts could improve \nthe performance of the NER model for acknowledgment texts. We are planning to fine-tune \nBERT and Flair Embeddings (contextual string embeddings) on a sample of approx. 5 mil-\nlion acknowledgment texts from WoS and evaluate the performance of the NER models.\nThe results of Experiment 2 generally did not show an improvement in accuracy. On the \ncontrary, training with the three entity types deteriorated the model performance. Train-\ning without the MISC category did not show significant performance progress either. \nMoreover, further analysis of acknowledged entities showed that the miscellaneous cate-\ngory contained very inhomogeneous and partly irrelevant data, making the analysis more \ncomplicated (Smirnova & Mayr, 2023). Therefore, we assume that the model would make \nbetter predictions if the number of entity types is expanded and miscellaneous categories \nexcluded, i.e., the MISC category could be split into the following categories: names of \nprojects, names of conferences, names of software and dataset. Different subcategories \ncould also be distinguished in the FUND category.\nCorpora No.2 and No.3 contain the same number of MISC and COR entities15, while \nin corpus 4 number of occurrences of MISC and COR entities is higher. For MISC and \nCOR, accuracy slightly increased with corpus 4, therefore we assume that the extraction \naccuracy for these entities will increase with the increase of the training data. The situa-\ntion is different for funding organizations and universities. The number of UNI and FUND \nentities increased evenly from corpus No.1 to corpus No.4. Nevertheless, the best result for \nthe UNI category was achieved with corpus No.3. The poor performance of corpus No.4 \ncould be explained by the inclusion of Indian funders. Thus, the names of many Indian \nfunders are very similar to the entities which usually fall into the UNI category, e.g., the \nDepartment of Science and Technology or the Department of Biotechnology. This pattern \nis more common to the entities which fall into the UNI category. Therefore, that might \nmake the exact extraction of UNI and FUND entities more confusing. Moreover, many \nIndian Universities contain the name of individuals, e.g., Rajiv Gandhi University, which \ncan cause confusion of the UNI category with the IND category. Generally, no improve-\nment in increasing the size of the corpus for the FUND category can be explained by the \nambiguous nature of the entities which fall into the FUND category and their semantical \nproximity with other types of entities. Analysis of the extracted entities showed that many \nentities were extracted correctly, but were assigned to the wrong category (Smirnova & \nMayr, 2023). Therefore, an additional classification algorithm applied to extracted entities \ncould improve the model’s performance.\n15 These differences in entity distribution are caused by the peculiarities of acknowledgement information \nstored in WoS. As only acknowledgements with indexed funding information are stored in the database, it \nwas difficult to find an adequate number of acknowledged entities of other typesScientometrics \n1 3\nConclusion\nIn this paper, we evaluated different embedding models for the task of automatic extraction \nand classification of acknowledged entities from acknowledgment texts16. The annotation \nof the training corpora was the most challenging and time-consuming task of all data prep-\naration procedures. Therefore, a semi-automated approach was used to help significantly \naccelerate the procedure.\nThe study’s main limitations were its small size and just one annotator of the training \ncorpora. Additionally, we used acknowledgments texts collected in WoS. WoS only stores \nacknowledgments containing funding information, therefore there was a lack of other types \nof entities, such as corporations or universities in the training data.\nIn the present paper, we aimed to answer three questions. Thus, regarding research ques-\ntion 1, the few-shot and zero-shot models showed very low total recognition accuracy. At \nthe same time, it was observed that some entities performed better than others with all \nalgorithms and training corpora. Thus, individuals gained a good F1-score over 0.8 with \nzero-shot and few-shot models, as also with Flair embeddings trained with the smallest \ncorpus. With the enlargement of the training corpora, the performance of the IND category \nalso increased and achieved an F1-score over 0.9. The GRNB category showed an adequate \nF-1 score of 0.76 with the few-shot algorithm trained with the smallest corpus, following \ntraining with corpus No.2 boosts the F-1 score to over 0.9. Therefore, few-shot and zero-\nshot approaches were not able to identify all the defined acknowledged entity classes.\nWith respect to research question 2, Flair Embeddings showed the best accuracy in \ntraining with corpus No.2 (and version 0.11) and the fastest training time compared to the \nother models; thus, it is recommended to further use the Flair Embeddings model for the \nrecognition of acknowledged entities.\nExploring research question 3 we observed, that the expansion of the size of a training \ncorpus from very small (corpus No.1) to medium size (corpus No.2) massively increased the \naccuracy of all training algorithms. The best-performing model (Flair Embedding) was further \nretrained with the two bigger corpora, but the following expansion of the training corpus did \nnot bring further improvement. Moreover, the performance of the model slightly deteriorated.\nAcknowledgements The original work was funded by the German Center for Higher Education Research \nand Science Studies (DZHW) via the project “Mining Acknowledgement Texts in Web of Science \n(MinAck)”17. Access to the WoS data was granted via the Competence Centre for Bibliometrics18. Data \naccess was funded by BMBF (Federal Ministry of Education and Research, Germany) under grant number \n01PQ17001. Nina Smirnova received funding from the German Research Foundation (DFG) via the project \n“POLLUX”19. The present paper is an extended version of the paper “Evaluation of Embedding Models for \nAutomatic Extraction and Classification of Acknowledged Entities in Scientific Documents” (Smirnova & \nMayr, 2022) presented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Sci-\nentific Documents (EEKE2022).\nAppendix A: Accuracy metrics by type of entity (label) for all \nexperiments\nSee Table 5.\n16 The best model can be tested  at https:// huggi ngface. co/ kalaw inka/ flair- ner- ackno  wledg ments\n17 https:// kalaw inka. github. io/ minack/.\n18 https:// www. bibli ometr  ie. info/ en/ index. php? id= home.\n19 https:// www. pollux- fid. de/ about. Scientometrics\n1 3\nTable 5  Accuracy metrics by type of entity (label) for all experiments\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings No.1 9 IND 0.7692 0.8333 0.8000 12 1\nFlair embeddings No.1 9 GRNB 0.5385 0.7000 0.6087 10 1\nFlair embeddings No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nFlair embeddings No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nFlair embeddings No.1 9 COR 0.0000 0.0000 0.0000 1 1\nFlair embeddings No.1 9 FUND 0.4000 0.4444 0.4211 18 1\nFlair embeddings No.2 9 FUND 0.6524 0.7771 0.7093 157 1\nFlair embeddings No.2 9 IND 0.9764 0.9831 0.9797 295 1\nFlair embeddings No.2 9 GRNB 0.9398 0.9750 0.9571 160 1\nFlair embeddings No.2 9 UNI 0.7527 0.7071 0.7292 99 1\nFlair embeddings No.2 9 MISC 0.6420 0.6341 0.6380 82 1\nFlair embeddings No.2 9 COR 0.8750 0.5833 0.7000 12 1\nTARS (pretrained) No.1 9 IND 1.0000 0.7500 0.8571 12 1\nTARS (pretrained) No.1 9 GRNB 0.7273 0.8000 0.7619 10 1\nTARS (pretrained) No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTARS (pretrained) No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTARS (pretrained) No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTARS (pretrained) No.1 9 FUND 0.3158 0.3333 0.3243 18 1\nTARS (pretrained) No.2 9 FUND 0.7257 0.8089 0.7651 157 1\nTARS (pretrained) No.2 9 IND 0.9281 0.8746 0.9005 295 1\nTARS (pretrained) No.2 9 GRNB 0.8895 0.9563 0.9217 160 1\nTARS (pretrained) No.2 9 UNI 0.7407 0.6061 0.6667 99 1\nTARS (pretrained) No.2 9 MISC 0.6719 0.5244 0.5890 82 1\nTARS (pretrained) No.2 9 COR 0.5000 0.5833 0.5385 12 1\nTransformers No.1 9 GRNB 0.3000 0.6000 0.4000 10 1\nTransformers No.1 9 IND 0.0000 0.0000 0.0000 12 1\nTransformers No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTransformers No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTransformers No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTransformers No.1 9 FUND 0.2414 0.3889 0.2979 18 1\nTransformers No.2 9 FUND 0.6211 0.7516 0.6801 157 1\nTransformers No.2 9 IND 0.9346 0.9695 0.9517 295 1\nTransformers No.2 9 GRNB 0.8704 0.8812 0.8758 160 1\nTransformers No.2 9 UNI 0.6476 0.6869 0.6667 99 1\nTransformers No.2 9 MISC 0.4767 0.5000 0.4881 82 1\nTransformers No.2 9 COR 0.7500 0.5000 0.6000 12 1\nFlair embeddings (3 Ent) No.2 9 IND 0.9577 0.9703 0.9639 303 2\nFlair embeddings (3 Ent) No.2 9 ORG 0.6400 0.6154 0.6275 208 2\nFlair embeddings (3 Ent) No.2 9 GRNB 0.9286 0.9750 0.9512 160 2\nFlair embeddings (5 Ent) No.2 9 IND 0.9764 0.9797 0.9780 295 2\nFlair embeddings (5 Ent) No.2 9 GRNB 0.9345 0.9812 0.9573 160 2\nFlair embeddings (5 Ent) No.2 9 UNI 0.7802 0.7172 0.7474 99 2\nFlair embeddings (5 Ent) No.2 9 COR 0.7500 0.5000 0.6000 12 2\nFlair embeddings (5 Ent) No.2 9 FUND 0.6722 0.7707 0.7181 157 2Scientometrics \n1 3\nAppendix B: Overall accuracy for all experiments\nSee Table 6.Rows are sorted by experiment number and algorithmTable 5  (continued)\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings (RoB-\nERTa)No.2 9 IND 0.9206 0.9831 0.9508 295 2\nFlair embeddings (RoB-\nERTa)No.2 9 GRNB 0.8896 0.9062 0.8978 160 2\nFlair embeddings (RoB-\nERTa)No.2 9 UNI 0.5963 0.6566 0.6250 99 2\nFlair embeddings (RoB-\nERTa)No.2 9 MISC 0.4135 0.5244 0.4624 82 2\nFlair embeddings (RoB-\nERTa)No.2 9 COR 1.0000 0.5000 0.6667 12 2\nFlair embeddings (RoB-\nERTa)No.2 9 FUND 0.6096 0.7261 0.6628 157 2\nFlair embeddings No.2 11 GRNB 0.9345 0.9812 0.9573 160 3\nFlair embeddings No.2 11 IND 0.9797 0.9831 0.9814 295 3\nFlair embeddings No.2 11 FUND 0.7027 0.8280 0.7602 157 3\nFlair embeddings No.2 11 UNI 0.7684 0.7374 0.7526 99 3\nFlair embeddings No.2 11 MISC 0.6543 0.6463 0.6503 82 3\nFlair embeddings No.2 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 UNI 0.8000 0.7273 0.7619 99 3\nFlair embeddings No.3 11 IND 0.9731 0.9797 0.9764 295 3\nFlair embeddings No.3 11 GRNB 0.9281 0.9688 0.9480 160 3\nFlair embeddings No.3 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 MISC 0.6571 0.5610 0.6053 82 3\nFlair embeddings No.3 11 FUND 0.6757 0.7962 0.7310 157 3\nFlair embeddings No.4 11 MISC 0.7424 0.5976 0.6622 82 3\nFlair embeddings No.4 11 COR 0.8571 0.5000 0.6316 12 3\nFlair embeddings No.4 11 UNI 0.7753 0.6970 0.7340 99 3\nFlair embeddings No.4 11 IND 0.9698 0.9797 0.9747 295 3\nFlair embeddings No.4 11 FUND 0.6823 0.8344 0.7507 157 3\nFlair embeddings No.4 11 GRNB 0.9162 0.9563 0.9358 160 3 Scientometrics\n1 3\nFunding Open access funding enabled and organized by Projekt DEAL.\nDeclarations \n Conflict of interest Philipp Mayr, the co-author of this paper, has a conflict of interest because he serves on \nthe editorial board of the journal Scientometrics. In addition, he is a co-guest editor of the special issue on \n\"Extraction and Evaluation of Knowledge Entities from Scientific Documents\". He declares that he has noth-\ning to do with the decision about this paper submission.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAkbik, A., Bergmann, T., Blythe, D., Rasul, K., Schweter, S., & Vollgraf, R. 2019. FLAIR: An Easy-to-Use \nFramework for State-of-the-Art NLP. Minneapolis, Minnesota (pp. 54–59). Association for Computa-\ntional Linguistics.\nAkbik, A., Blythe, D., & Vollgraf, R. (2018). Contextual string embeddings for sequence labeling. In 2018, \n27th International Conference on Computational Linguistics (pp. 1638–1649).\nAlexandera, D. & Vries, A. P. (2021). This research is funded by...”: Named Entity Recognition of financial \ninformation in research papers. In BIR 2021: 11th International Workshop on Bibliometric-enhanced \nInformation Retrieval at ECIR (pp. 102–110).\nBeltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. \nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing \nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)  \n(pp. 3613–3618). Association for Computational Linguistics.Table 6  Overall accuracy for all experiments\nRows are sorted by experiment number and algorithmAlgorithm Corpus Version Accuracy Experiment\nFlair embeddings No.2 9 0.7702 1\nFlair embeddings No.1 9 0.3472 1\nTARS (pretrained) No.2 9 0.7113 1\nTARS (pretrained) No.1 9 0.3485 1\nTransformers No.2 9 0.6783 1\nTransformers No.1 9 0.1477 1\nFlair Embeddings (3 Entity Types) No.2 9 0.7536 2\nFlair embeddings (5 Entity Types) No.2 9 0.7990 2\nFlair embeddings + RoBERTa No.2 9 0.6697 2\nFlair Embeddings No.2 11 0.7869 3\nFlair embeddings No.4 11 0.7814 3\nFlair embeddings No.3 11 0.7691 3Scientometrics \n1 3\nBorst, T., Mielck, J., Nannt, M., & Riese, W. (2022). Extracting funder information from scien-\ntific papers—Experiences with question answering. In Silvello, G., O.  Corcho, P.  Manghi, G.M. \nDi Nunzio, K. Golub, N. Ferro, and A. Poggi (Eds.),Linking theory and practice of digital libraries  \n(Vol. 13541, pp. 289–296). Springer International Publishing. Series Title: Lecture Notes in Com-\nputer Science. https:// doi. org/ 10. 1007/ 978-3- 031- 16802-4_ 24.\nChelba, C., T.  Mikolov, M.  Schuster, Q.  Ge, T.  Brants, P.  Koehn, & Robinson, T. (2013). One Bil-\nlion Word Benchmark for Measuring Progress in Statistical Language Modeling. 10.48550/\nARXIV.1312.3005 .\nChen, H., Song, X., Jin, Q., & Wang, X. (2022). Network dynamics in university-industry collaboration: \nA collaboration-knowledge dual-layer network perspective. Scientometrics, 127(11), 6637–6660. \nhttps:// doi. org/ 10. 1007/ s11192- 022- 04330-9\nCronin, B. (1995). The Scholar’s courtesy: The role of acknowledgement in the primary communication \nprocess. Taylor Graham.\nCronin, B., & Weaver, S. (1995). The praxis of acknowledgement: From bibliometrics to influmetrics. \nRevista Española de Documentación Científica, 18(2), 172.\nDevlin, J., Chang, M.W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional \ntransformers for language understanding. 10.48550/ARXIV.1810.04805 .\nDiaz-Faes, A. A., & Bordons, M. (2017). Making visible the invisible through the analysis of acknowl-\nedgements in the humanities. Aslib Journal of Information Management, 69(5), 576–590. https://  \ndoi. org/ 10. 1108/ AJIM- 01- 2017- 0008\nDoehne, M., & Herfeld, C. (2023). How academic opinion leaders shape scientific ideas: an acknowl-\nedgment analysis., 128(4), 2507–2533. https:// doi. org/ 10. 1007/ s11192- 022- 04623-z\nDzieżyc, M., & Kazienko, P. (2022). Effectiveness of research grants funded by European research coun-\ncil and polish national science centre. Journal of Informetrics, 16(1), 101243. https:// doi. org/ 10.  \n1016/j. joi. 2021. 101243\nEftimov, T., Koroušić Seljak, B., & Korošec, P. (2017). A rule-based named-entity recognition \nmethod for knowledge extraction of evidence-based dietary recommendations. PLoS ONE, 12(6), \ne0179488. https:// doi. org/ 10. 1371/ journ al. pone. 01794 88\nEtzioni, O., Cafarella, M., Downey, D., Popescu, A. M., Shaked, T., Soderland, S., Weld, D. S., & Yates, \nA. (2005). Unsupervised named-entity extraction from the web: An experimental study. Artificial \nIntelligence, 165(1), 91–134. https:// doi. org/ 10. 1016/j. artint. 2005. 03. 001\nFinkel, J.R., Grenager, T., & Manning, C. (2005). Incorporating non-local information into information \nextraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL’05), Ann Arbor, Michigan (pp. 363–370). Association for \nComputational Linguistics.\nGiles, C. L., & Councill, I. G. (2004). Who gets acknowledged: Measuring scientific contributions \nthrough automatic acknowledgment indexing. Proceedings of the National Academy of Sciences,  \n101(51), 17599–17604. https:// doi. org/ 10. 1073/ pnas. 04077 43101\nHalder, K., Akbik, A., Krapac, J., & Vollgraf, R. (2020). Task-Aware Representation of Sentences for \nGeneric Text Classification. In Proceedings of the 28th International Conference on Computational \nLinguistics, Barcelona, Spain (Online) (pp. 3202–3213). International Committee on Computa-\ntional Linguistics.\nHubbard, D., Laddusaw, S., Tan, Q., & Hu, X. (2022). Analysis of acknowledgments of libraries in the \njournal literature using machine learning. Proceedings of the Association for Information Science \nand Technology, 59(1), 709–711. https:// doi. org/ 10. 1002/ pra2. 698\nIovine, A., Fang, A., Fetahu, B., Rokhlenko, O., & Malmasi, S. (2022). CycleNER: An unsupervised \ntraining approach for named entity recognition. In Proceedings of the ACM Web Conference 2022  \n(pp. 2916–2924). ACM.\nJiang, L., Kang, X., Huang, S., & Yang, B. (2022). A refinement strategy for identification of scientific \nsoftware from bioinformatics publications. Scientometrics, 127(6), 3293–3316. https:// doi. org/ 10.  \n1007/ s11192- 022- 04381-y\nKassirer, J. P., & Angell, M. (1991). On authorship and acknowledgments. The New England Journal of \nMedicine, 325(21), 1510–1512. https:// doi. org/ 10. 1056/ NEJM1 99111 21325 2112\nKayal, S., Afzal, Z., Tsatsaronis, G., Katrenko, S., Coupet, P., Doornenbal, M., & Gregory, M. (2017). \nTagging funding agencies and grants in scientific articles using sequential learning models. In \nBioNLP 2017, Vancouver, Canada (pp. 216–221). Association for Computational Linguistics.\nKenekayoro, P. (2018). Identifying named entities in academic biographies with supervised learning. \nScientometrics, 116(2), 751–765. https:// doi. org/ 10. 1007/ s11192- 018- 2797-4\nKingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. 10.48550/\nARXIV.1412.6980 . Scientometrics\n1 3\nKusumegi, K., & Sano, Y. (2022). Dataset of identified scholars mentioned in acknowledgement state-\nments. Scientific Data, 9(1), 461. https:// doi. org/ 10. 1038/ s41597- 022- 01585-y\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoy -\nanov, V. (2019). RoBERTa: A robustly optimized BERT pretraining approach. arXiv: 1907. 11692  \n[cs] .\nMackintosh, K. (1972). Acknowledgements patterns in sociology. Ph. D. thesis, University of Oregon.\nMccain, K. (2017). Beyond Garfield’s citation index: An assessment of some issues in building a per -\nsonal name acknowledgments index. Scientometrics. https:// doi. org/ 10. 1007/ s11192- 017- 2598-1\nMcCain, K. W. (1991). Communication, competition, and secrecy: The production and dissemination of \nresearch-related information in genetics. Science, Technology, & Human Values, 16(4), 491–516. \nhttps:// doi. org/ 10. 1177/ 01622 43991 01600 404\nMejia, C., & Kajikawa, Y. (2018). Using acknowledgement data to characterize funding organizations \nby the types of research sponsored: the case of robotics research. Scientometrics, 114(3), 883–904. \nhttps:// doi. org/ 10. 1007/ s11192- 017- 2617-2\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, \nN., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkur -\nthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S. (2019). PyTorch: An imperative style, high-\nperformance deep learning library. arXiv: 1912. 01703 [cs, stat].\nPaul-Hus, A., & Desrochers, N. (2019). Acknowledgements are not just thank you notes: A qualitative \nanalysis of acknowledgements content in scientific articles and reviews published in 2015. PLoS \nONE, 14, e0226727. https:// doi. org/ 10. 1371/ journ al. pone. 02267 27\nPaul-Hus, A., Díaz-Faes, A., Sainte-Marie, M., Desrochers, N., Costas, R., & Larivière, V. (2017). \nBeyond funding: Acknowledgement patterns in biomedical, natural and social sciences. PLoS ONE,  \n12, e0185578. https:// doi. org/ 10. 1371/ journ al. pone. 01855 78\nPennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global Vectors for Word Representation. \nIn Empirical Methods in Natural Language Processing (EMNLP) (pp. 1532–1543).\nPustejovsky, J., & Stubbs, A. (2012). Natural language annotation for machine learning. O’Reilly \nMedia Inc.\nRose, M., & Georg, C. P. (2021). What 5,000 acknowledgements tell us about informal collaboration \nin financial economics. Research Policy, 50, 104236. https:// doi. org/ 10. 1016/j. respol. 2021. 104236\nSang, T. K., & E. F., & De Meulder, F. (2003). Introduction to the CoNLL-2003 shared task: Language-\nindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Lan-\nguage Learning at HLT-NAACL (pp. 142–147).\nSchweter, S., & Akbik, A. (2020). FLERT: Document-level features for named entity recognition. ArXiv. \n10.48550/arXiv.2011.06993 .\nShen, S., Liu, J., Lin, L., Huang, Y., Zhang, L., Liu, C., Feng, Y., & Wang, D. (2022). SsciBERT: A \npre-trained language model for social science texts. Scientometrics. https:// doi. org/ 10. 1007/  \ns11192- 022- 04602-4\nSingh, V. K., Singh, P., Karmakar, M., Leta, J., & Mayr, P. (2021). The journal coverage of web of sci-\nence, scopus and dimensions: A comparative analysis. Scientometrics, 126(6), 5113–5142. https://  \ndoi. org/ 10. 1007/ s11192- 021- 03948-5\nSmirnova, N., & Mayr, P. (2022). Evaluation of embedding models for automatic extraction and classifi-\ncation of acknowledged entities in scientific documents. In 3rd Workshop on Extraction and Evalu-\nation of Knowledge Entities from Scientific Documents 2022 (EEKE 2022) (pp. 48–55). CEUR-WS.\norg.\nSmirnova, N., & Mayr, P. (2023). A comprehensive analysis of acknowledgement texts in web of sci-\nence: A case study on four scientific domains. Scientometrics, 1(128), 709–734. https:// doi. org/ 10.  \n1007/ s11192- 022- 04554-9\nSong, M., Kang, K. Y., Timakum, T., & Zhang, X. (2020). Examining influential factors for acknowl-\nedgements classification using supervised learning. PLoS ONE. https:// doi. org/ 10. 1371/ journ al.  \npone. 02289 28\nThomer, A. K., & Weber, N. M. (2014). Using named entity recognition as a classification heuristic. In \niConference 2014 Proceedings (pp. 1133 – 1138). iSchools.\nWang, J., & Shapira, P. (2011). Funding acknowledgement analysis: An enhanced tool to investigate \nresearch sponsorship impacts: The case of nanotechnology. Scientometrics, 87(3), 563–586. https://  \ndoi. org/ 10. 1007/ s11192- 011- 0362-5\nYamada, I., Asai, A., Shindo, H., Takeda, H., & Matsumoto, Y. (2020). LUKE: Deep contextualized \nentity representations with entity-aware self-attention. In Proceedings of the 2020 Conference on \nEmpirical Methods in Natural Language Processing (EMNLP) (pp. 6442–6454). Association for \nComputational Linguistics.Scientometrics \n1 3\nYu, J., Bohnet, B., & Poesio, M. (2020). Named entity recognition as dependency parsing. 10.48550/\nARXIV.2005.07150.\nZhang, C., Mayr, P., Lu, W., & Zhang, Y. (2023). Guest editorial: Extraction and evaluation of knowledge \nentities in the age of artificial intelligence. Aslib Journal of Information Management, 75, 433–437. \nhttps:// doi. org/ 10. 1108/ AJIM- 05- 2023- 507",
        "type": "direct"
    },
    {
        "citation": "Computational Linguistics, 2023",
        "content": ". Therefore, fine-\ntuning the general language model on the sample of acknowledgment texts could improve \nthe performance of the NER model for acknowledgment texts. We are planning to fine-tune \nBERT and Flair Embeddings (contextual string embeddings) on a sample of approx. 5 mil-\nlion acknowledgment texts from WoS and evaluate the performance of the NER models.\nThe results of Experiment 2 generally did not show an improvement in accuracy. On the \ncontrary, training with the three entity types deteriorated the model performance. Train-\ning without the MISC category did not show significant performance progress either. \nMoreover, further analysis of acknowledged entities showed that the miscellaneous cate-\ngory contained very inhomogeneous and partly irrelevant data, making the analysis more \ncomplicated",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2018",
        "content": ". Therefore, we assume that the model would make \nbetter predictions if the number of entity types is expanded and miscellaneous categories \nexcluded, i.e., the MISC category could be split into the following categories: names of \nprojects, names of conferences, names of software and dataset. Different subcategories \ncould also be distinguished in the FUND category.\nCorpora No.2 and No.3 contain the same number of MISC and COR entities15, while \nin corpus 4 number of occurrences of MISC and COR entities is higher. For MISC and \nCOR, accuracy slightly increased with corpus 4, therefore we assume that the extraction \naccuracy for these entities will increase with the increase of the training data. The situa-\ntion is different for funding organizations and universities. The number of UNI and FUND \nentities increased evenly from corpus No.1 to corpus No.4. Nevertheless, the best result for \nthe UNI category was achieved with corpus No.3. The poor performance of corpus No.4 \ncould be explained by the inclusion of Indian funders. Thus, the names of many Indian \nfunders are very similar to the entities which usually fall into the UNI category, e.g., the \nDepartment of Science and Technology or the Department of Biotechnology. This pattern \nis more common to the entities which fall into the UNI category. Therefore, that might \nmake the exact extraction of UNI and FUND entities more confusing. Moreover, many \nIndian Universities contain the name of individuals, e.g., Rajiv Gandhi University, which \ncan cause confusion of the UNI category with the IND category. Generally, no improve-\nment in increasing the size of the corpus for the FUND category can be explained by the \nambiguous nature of the entities which fall into the FUND category and their semantical \nproximity with other types of entities. Analysis of the extracted entities showed that many \nentities were extracted correctly, but were assigned to the wrong category (Smirnova & \nMayr, 2023). Therefore, an additional classification algorithm applied to extracted entities \ncould improve the model’s performance.\n15 These differences in entity distribution are caused by the peculiarities of acknowledgement information \nstored in WoS. As only acknowledgements with indexed funding information are stored in the database, it \nwas difficult to find an adequate number of acknowledged entities of other typesScientometrics \n1 3\nConclusion\nIn this paper, we evaluated different embedding models for the task of automatic extraction \nand classification of acknowledged entities from acknowledgment texts16. The annotation \nof the training corpora was the most challenging and time-consuming task of all data prep-\naration procedures. Therefore, a semi-automated approach was used to help significantly \naccelerate the procedure.\nThe study’s main limitations were its small size and just one annotator of the training \ncorpora. Additionally, we used acknowledgments texts collected in WoS. WoS only stores \nacknowledgments containing funding information, therefore there was a lack of other types \nof entities, such as corporations or universities in the training data.\nIn the present paper, we aimed to answer three questions. Thus, regarding research ques-\ntion 1, the few-shot and zero-shot models showed very low total recognition accuracy. At \nthe same time, it was observed that some entities performed better than others with all \nalgorithms and training corpora. Thus, individuals gained a good F1-score over 0.8 with \nzero-shot and few-shot models, as also with Flair embeddings trained with the smallest \ncorpus. With the enlargement of the training corpora, the performance of the IND category \nalso increased and achieved an F1-score over 0.9. The GRNB category showed an adequate \nF-1 score of 0.76 with the few-shot algorithm trained with the smallest corpus, following \ntraining with corpus No.2 boosts the F-1 score to over 0.9. Therefore, few-shot and zero-\nshot approaches were not able to identify all the defined acknowledged entity classes.\nWith respect to research question 2, Flair Embeddings showed the best accuracy in \ntraining with corpus No.2 (and version 0.11) and the fastest training time compared to the \nother models; thus, it is recommended to further use the Flair Embeddings model for the \nrecognition of acknowledged entities.\nExploring research question 3 we observed, that the expansion of the size of a training \ncorpus from very small (corpus No.1) to medium size (corpus No.2) massively increased the \naccuracy of all training algorithms. The best-performing model (Flair Embedding) was further \nretrained with the two bigger corpora, but the following expansion of the training corpus did \nnot bring further improvement. Moreover, the performance of the model slightly deteriorated.\nAcknowledgements The original work was funded by the German Center for Higher Education Research \nand Science Studies (DZHW) via the project “Mining Acknowledgement Texts in Web of Science \n(MinAck)”17. Access to the WoS data was granted via the Competence Centre for Bibliometrics18. Data \naccess was funded by BMBF (Federal Ministry of Education and Research, Germany) under grant number \n01PQ17001. Nina Smirnova received funding from the German Research Foundation (DFG) via the project \n“POLLUX”19. The present paper is an extended version of the paper “Evaluation of Embedding Models for \nAutomatic Extraction and Classification of Acknowledged Entities in Scientific Documents” (Smirnova & \nMayr, 2022) presented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Sci-\nentific Documents (EEKE2022).\nAppendix A: Accuracy metrics by type of entity (label) for all \nexperiments\nSee Table 5.\n16 The best model can be tested  at https:// huggi ngface. co/ kalaw inka/ flair- ner- ackno  wledg ments\n17 https:// kalaw inka. github. io/ minack/.\n18 https:// www. bibli ometr  ie. info/ en/ index. php? id= home.\n19 https:// www. pollux- fid. de/ about. Scientometrics\n1 3\nTable 5  Accuracy metrics by type of entity (label) for all experiments\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings No.1 9 IND 0.7692 0.8333 0.8000 12 1\nFlair embeddings No.1 9 GRNB 0.5385 0.7000 0.6087 10 1\nFlair embeddings No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nFlair embeddings No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nFlair embeddings No.1 9 COR 0.0000 0.0000 0.0000 1 1\nFlair embeddings No.1 9 FUND 0.4000 0.4444 0.4211 18 1\nFlair embeddings No.2 9 FUND 0.6524 0.7771 0.7093 157 1\nFlair embeddings No.2 9 IND 0.9764 0.9831 0.9797 295 1\nFlair embeddings No.2 9 GRNB 0.9398 0.9750 0.9571 160 1\nFlair embeddings No.2 9 UNI 0.7527 0.7071 0.7292 99 1\nFlair embeddings No.2 9 MISC 0.6420 0.6341 0.6380 82 1\nFlair embeddings No.2 9 COR 0.8750 0.5833 0.7000 12 1\nTARS (pretrained) No.1 9 IND 1.0000 0.7500 0.8571 12 1\nTARS (pretrained) No.1 9 GRNB 0.7273 0.8000 0.7619 10 1\nTARS (pretrained) No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTARS (pretrained) No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTARS (pretrained) No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTARS (pretrained) No.1 9 FUND 0.3158 0.3333 0.3243 18 1\nTARS (pretrained) No.2 9 FUND 0.7257 0.8089 0.7651 157 1\nTARS (pretrained) No.2 9 IND 0.9281 0.8746 0.9005 295 1\nTARS (pretrained) No.2 9 GRNB 0.8895 0.9563 0.9217 160 1\nTARS (pretrained) No.2 9 UNI 0.7407 0.6061 0.6667 99 1\nTARS (pretrained) No.2 9 MISC 0.6719 0.5244 0.5890 82 1\nTARS (pretrained) No.2 9 COR 0.5000 0.5833 0.5385 12 1\nTransformers No.1 9 GRNB 0.3000 0.6000 0.4000 10 1\nTransformers No.1 9 IND 0.0000 0.0000 0.0000 12 1\nTransformers No.1 9 MISC 0.0000 0.0000 0.0000 6 1\nTransformers No.1 9 UNI 0.0000 0.0000 0.0000 3 1\nTransformers No.1 9 COR 0.0000 0.0000 0.0000 1 1\nTransformers No.1 9 FUND 0.2414 0.3889 0.2979 18 1\nTransformers No.2 9 FUND 0.6211 0.7516 0.6801 157 1\nTransformers No.2 9 IND 0.9346 0.9695 0.9517 295 1\nTransformers No.2 9 GRNB 0.8704 0.8812 0.8758 160 1\nTransformers No.2 9 UNI 0.6476 0.6869 0.6667 99 1\nTransformers No.2 9 MISC 0.4767 0.5000 0.4881 82 1\nTransformers No.2 9 COR 0.7500 0.5000 0.6000 12 1\nFlair embeddings (3 Ent) No.2 9 IND 0.9577 0.9703 0.9639 303 2\nFlair embeddings (3 Ent) No.2 9 ORG 0.6400 0.6154 0.6275 208 2\nFlair embeddings (3 Ent) No.2 9 GRNB 0.9286 0.9750 0.9512 160 2\nFlair embeddings (5 Ent) No.2 9 IND 0.9764 0.9797 0.9780 295 2\nFlair embeddings (5 Ent) No.2 9 GRNB 0.9345 0.9812 0.9573 160 2\nFlair embeddings (5 Ent) No.2 9 UNI 0.7802 0.7172 0.7474 99 2\nFlair embeddings (5 Ent) No.2 9 COR 0.7500 0.5000 0.6000 12 2\nFlair embeddings (5 Ent) No.2 9 FUND 0.6722 0.7707 0.7181 157 2Scientometrics \n1 3\nAppendix B: Overall accuracy for all experiments\nSee Table 6.Rows are sorted by experiment number and algorithmTable 5  (continued)\nAlgorithm Corpus Version Label Precision Recall F1-score Support Experiment\nFlair embeddings (RoB-\nERTa)No.2 9 IND 0.9206 0.9831 0.9508 295 2\nFlair embeddings (RoB-\nERTa)No.2 9 GRNB 0.8896 0.9062 0.8978 160 2\nFlair embeddings (RoB-\nERTa)No.2 9 UNI 0.5963 0.6566 0.6250 99 2\nFlair embeddings (RoB-\nERTa)No.2 9 MISC 0.4135 0.5244 0.4624 82 2\nFlair embeddings (RoB-\nERTa)No.2 9 COR 1.0000 0.5000 0.6667 12 2\nFlair embeddings (RoB-\nERTa)No.2 9 FUND 0.6096 0.7261 0.6628 157 2\nFlair embeddings No.2 11 GRNB 0.9345 0.9812 0.9573 160 3\nFlair embeddings No.2 11 IND 0.9797 0.9831 0.9814 295 3\nFlair embeddings No.2 11 FUND 0.7027 0.8280 0.7602 157 3\nFlair embeddings No.2 11 UNI 0.7684 0.7374 0.7526 99 3\nFlair embeddings No.2 11 MISC 0.6543 0.6463 0.6503 82 3\nFlair embeddings No.2 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 UNI 0.8000 0.7273 0.7619 99 3\nFlair embeddings No.3 11 IND 0.9731 0.9797 0.9764 295 3\nFlair embeddings No.3 11 GRNB 0.9281 0.9688 0.9480 160 3\nFlair embeddings No.3 11 COR 0.7500 0.5000 0.6000 12 3\nFlair embeddings No.3 11 MISC 0.6571 0.5610 0.6053 82 3\nFlair embeddings No.3 11 FUND 0.6757 0.7962 0.7310 157 3\nFlair embeddings No.4 11 MISC 0.7424 0.5976 0.6622 82 3\nFlair embeddings No.4 11 COR 0.8571 0.5000 0.6316 12 3\nFlair embeddings No.4 11 UNI 0.7753 0.6970 0.7340 99 3\nFlair embeddings No.4 11 IND 0.9698 0.9797 0.9747 295 3\nFlair embeddings No.4 11 FUND 0.6823 0.8344 0.7507 157 3\nFlair embeddings No.4 11 GRNB 0.9162 0.9563 0.9358 160 3 Scientometrics\n1 3\nFunding Open access funding enabled and organized by Projekt DEAL.\nDeclarations \n Conflict of interest Philipp Mayr, the co-author of this paper, has a conflict of interest because he serves on \nthe editorial board of the journal Scientometrics. In addition, he is a co-guest editor of the special issue on \n\"Extraction and Evaluation of Knowledge Entities from Scientific Documents\". He declares that he has noth-\ning to do with the decision about this paper submission.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAkbik, A., Bergmann, T., Blythe, D., Rasul, K., Schweter, S., & Vollgraf, R. 2019. FLAIR: An Easy-to-Use \nFramework for State-of-the-Art NLP. Minneapolis, Minnesota (pp. 54–59). Association for Computa-\ntional Linguistics.\nAkbik, A., Blythe, D., & Vollgraf, R.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics pp. 1638–1649",
        "content": ". Contextual string embeddings for sequence labeling. In 2018, \n27th International Conference on Computational Linguistics",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2021",
        "content": ".\nAlexandera, D. & Vries, A. P.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2019",
        "content": ". This research is funded by...”: Named Entity Recognition of financial \ninformation in research papers. In BIR 2021: 11th International Workshop on Bibliometric-enhanced \nInformation Retrieval at ECIR (pp. 102–110).\nBeltagy, I., Lo, K., & Cohan, A.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics pp. 3613–3618",
        "content": ". SciBERT: A pretrained language model for scientific text. \nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing \nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": ". Association for Computational Linguistics.Table 6  Overall accuracy for all experiments\nRows are sorted by experiment number and algorithmAlgorithm Corpus Version Accuracy Experiment\nFlair embeddings No.2 9 0.7702 1\nFlair embeddings No.1 9 0.3472 1\nTARS (pretrained) No.2 9 0.7113 1\nTARS (pretrained) No.1 9 0.3485 1\nTransformers No.2 9 0.6783 1\nTransformers No.1 9 0.1477 1\nFlair Embeddings (3 Entity Types) No.2 9 0.7536 2\nFlair embeddings (5 Entity Types) No.2 9 0.7990 2\nFlair embeddings + RoBERTa No.2 9 0.6697 2\nFlair Embeddings No.2 11 0.7869 3\nFlair embeddings No.4 11 0.7814 3\nFlair embeddings No.3 11 0.7691 3Scientometrics \n1 3\nBorst, T., Mielck, J., Nannt, M., & Riese, W.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2013",
        "content": ". Extracting funder information from scien-\ntific papers—Experiences with question answering. In Silvello, G., O.  Corcho, P.  Manghi, G.M. \nDi Nunzio, K. Golub, N. Ferro, and A. Poggi (Eds.),Linking theory and practice of digital libraries  \n(Vol. 13541, pp. 289–296). Springer International Publishing. Series Title: Lecture Notes in Com-\nputer Science. https:// doi. org/ 10. 1007/ 978-3- 031- 16802-4_ 24.\nChelba, C., T.  Mikolov, M.  Schuster, Q.  Ge, T.  Brants, P.  Koehn, & Robinson, T.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": ". One Bil-\nlion Word Benchmark for Measuring Progress in Statistical Language Modeling. 10.48550/\nARXIV.1312.3005 .\nChen, H., Song, X., Jin, Q., & Wang, X.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 1995",
        "content": ". Network dynamics in university-industry collaboration: \nA collaboration-knowledge dual-layer network perspective. Scientometrics, 127(11), 6637–6660. \nhttps:// doi. org/ 10. 1007/ s11192- 022- 04330-9\nCronin, B.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 1995",
        "content": ". The Scholar’s courtesy: The role of acknowledgement in the primary communication \nprocess. Taylor Graham.\nCronin, B., & Weaver, S.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2018",
        "content": ". The praxis of acknowledgement: From bibliometrics to influmetrics. \nRevista Española de Documentación Científica, 18(2), 172.\nDevlin, J., Chang, M.W., Lee, K., & Toutanova, K.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2017",
        "content": ". BERT: Pre-training of deep bidirectional \ntransformers for language understanding. 10.48550/ARXIV.1810.04805 .\nDiaz-Faes, A. A., & Bordons, M.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2023",
        "content": ". Making visible the invisible through the analysis of acknowl-\nedgements in the humanities. Aslib Journal of Information Management, 69(5), 576–590. https://  \ndoi. org/ 10. 1108/ AJIM- 01- 2017- 0008\nDoehne, M., & Herfeld, C.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": ". How academic opinion leaders shape scientific ideas: an acknowl-\nedgment analysis., 128(4), 2507–2533. https:// doi. org/ 10. 1007/ s11192- 022- 04623-z\nDzieżyc, M., & Kazienko, P.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2017",
        "content": ". Effectiveness of research grants funded by European research coun-\ncil and polish national science centre. Journal of Informetrics, 16(1), 101243. https:// doi. org/ 10.  \n1016/j. joi. 2021. 101243\nEftimov, T., Koroušić Seljak, B., & Korošec, P.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2005",
        "content": ". A rule-based named-entity recognition \nmethod for knowledge extraction of evidence-based dietary recommendations. PLoS ONE, 12(6), \ne0179488. https:// doi. org/ 10. 1371/ journ al. pone. 01794 88\nEtzioni, O., Cafarella, M., Downey, D., Popescu, A. M., Shaked, T., Soderland, S., Weld, D. S., & Yates, \nA.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2005",
        "content": ". Unsupervised named-entity extraction from the web: An experimental study. Artificial \nIntelligence, 165(1), 91–134. https:// doi. org/ 10. 1016/j. artint. 2005. 03. 001\nFinkel, J.R., Grenager, T., & Manning, C.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2004",
        "content": ". Incorporating non-local information into information \nextraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL’05), Ann Arbor, Michigan (pp. 363–370). Association for \nComputational Linguistics.\nGiles, C. L., & Councill, I. G.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2020",
        "content": ". Who gets acknowledged: Measuring scientific contributions \nthrough automatic acknowledgment indexing. Proceedings of the National Academy of Sciences,  \n101(51), 17599–17604. https:// doi. org/ 10. 1073/ pnas. 04077 43101\nHalder, K., Akbik, A., Krapac, J., & Vollgraf, R.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics pp. 3202–3213",
        "content": ". Task-Aware Representation of Sentences for \nGeneric Text Classification. In Proceedings of the 28th International Conference on Computational \nLinguistics, Barcelona, Spain (Online)",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": ". International Committee on Computa-\ntional Linguistics.\nHubbard, D., Laddusaw, S., Tan, Q., & Hu, X.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": ". Analysis of acknowledgments of libraries in the \njournal literature using machine learning. Proceedings of the Association for Information Science \nand Technology, 59(1), 709–711. https:// doi. org/ 10. 1002/ pra2. 698\nIovine, A., Fang, A., Fetahu, B., Rokhlenko, O., & Malmasi, S.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics pp. 2916–2924",
        "content": ". CycleNER: An unsupervised \ntraining approach for named entity recognition. In Proceedings of the ACM Web Conference 2022",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": ". ACM.\nJiang, L., Kang, X., Huang, S., & Yang, B.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 1991",
        "content": ". A refinement strategy for identification of scientific \nsoftware from bioinformatics publications. Scientometrics, 127(6), 3293–3316. https:// doi. org/ 10.  \n1007/ s11192- 022- 04381-y\nKassirer, J. P., & Angell, M.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2017",
        "content": ". On authorship and acknowledgments. The New England Journal of \nMedicine, 325(21), 1510–1512. https:// doi. org/ 10. 1056/ NEJM1 99111 21325 2112\nKayal, S., Afzal, Z., Tsatsaronis, G., Katrenko, S., Coupet, P., Doornenbal, M., & Gregory, M.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2018",
        "content": ". \nTagging funding agencies and grants in scientific articles using sequential learning models. In \nBioNLP 2017, Vancouver, Canada (pp. 216–221). Association for Computational Linguistics.\nKenekayoro, P.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2014",
        "content": ". Identifying named entities in academic biographies with supervised learning. \nScientometrics, 116(2), 751–765. https:// doi. org/ 10. 1007/ s11192- 018- 2797-4\nKingma, D. P., & Ba, J.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": ". Adam: A method for stochastic optimization. 10.48550/\nARXIV.1412.6980 . Scientometrics\n1 3\nKusumegi, K., & Sano, Y.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2019",
        "content": ". Dataset of identified scholars mentioned in acknowledgement state-\nments. Scientific Data, 9(1), 461. https:// doi. org/ 10. 1038/ s41597- 022- 01585-y\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoy -\nanov, V.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 1972",
        "content": ". RoBERTa: A robustly optimized BERT pretraining approach. arXiv: 1907. 11692  \n[cs] .\nMackintosh, K.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2017",
        "content": ". Acknowledgements patterns in sociology. Ph. D. thesis, University of Oregon.\nMccain, K.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 1991",
        "content": ". Beyond Garfield’s citation index: An assessment of some issues in building a per -\nsonal name acknowledgments index. Scientometrics. https:// doi. org/ 10. 1007/ s11192- 017- 2598-1\nMcCain, K. W.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2018",
        "content": ". Communication, competition, and secrecy: The production and dissemination of \nresearch-related information in genetics. Science, Technology, & Human Values, 16(4), 491–516. \nhttps:// doi. org/ 10. 1177/ 01622 43991 01600 404\nMejia, C., & Kajikawa, Y.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2019",
        "content": ". Using acknowledgement data to characterize funding organizations \nby the types of research sponsored: the case of robotics research. Scientometrics, 114(3), 883–904. \nhttps:// doi. org/ 10. 1007/ s11192- 017- 2617-2\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, \nN., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkur -\nthy, S., Steiner, B., Fang, L., Bai, J., & Chintala, S.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2019",
        "content": ". PyTorch: An imperative style, high-\nperformance deep learning library. arXiv: 1912. 01703 [cs, stat].\nPaul-Hus, A., & Desrochers, N.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2017",
        "content": ". Acknowledgements are not just thank you notes: A qualitative \nanalysis of acknowledgements content in scientific articles and reviews published in 2015. PLoS \nONE, 14, e0226727. https:// doi. org/ 10. 1371/ journ al. pone. 02267 27\nPaul-Hus, A., Díaz-Faes, A., Sainte-Marie, M., Desrochers, N., Costas, R., & Larivière, V.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2014",
        "content": ". \nBeyond funding: Acknowledgement patterns in biomedical, natural and social sciences. PLoS ONE,  \n12, e0185578. https:// doi. org/ 10. 1371/ journ al. pone. 01855 78\nPennington, J., Socher, R., & Manning, C. D.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics pp. 1532–1543",
        "content": ". GloVe: Global Vectors for Word Representation. \nIn Empirical Methods in Natural Language Processing (EMNLP)",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2012",
        "content": ".\nPustejovsky, J., & Stubbs, A.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2021",
        "content": ". Natural language annotation for machine learning. O’Reilly \nMedia Inc.\nRose, M., & Georg, C. P.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2003",
        "content": ". What 5,000 acknowledgements tell us about informal collaboration \nin financial economics. Research Policy, 50, 104236. https:// doi. org/ 10. 1016/j. respol. 2021. 104236\nSang, T. K., & E. F., & De Meulder, F.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2020",
        "content": ". Introduction to the CoNLL-2003 shared task: Language-\nindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Lan-\nguage Learning at HLT-NAACL (pp. 142–147).\nSchweter, S., & Akbik, A.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": ". FLERT: Document-level features for named entity recognition. ArXiv. \n10.48550/arXiv.2011.06993 .\nShen, S., Liu, J., Lin, L., Huang, Y., Zhang, L., Liu, C., Feng, Y., & Wang, D.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2021",
        "content": ". SsciBERT: A \npre-trained language model for social science texts. Scientometrics. https:// doi. org/ 10. 1007/  \ns11192- 022- 04602-4\nSingh, V. K., Singh, P., Karmakar, M., Leta, J., & Mayr, P.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2022",
        "content": ". The journal coverage of web of sci-\nence, scopus and dimensions: A comparative analysis. Scientometrics, 126(6), 5113–5142. https://  \ndoi. org/ 10. 1007/ s11192- 021- 03948-5\nSmirnova, N., & Mayr, P.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics EEKE 2022",
        "content": ". Evaluation of embedding models for automatic extraction and classifi-\ncation of acknowledged entities in scientific documents. In 3rd Workshop on Extraction and Evalu-\nation of Knowledge Entities from Scientific Documents 2022",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2023",
        "content": "(pp. 48–55). CEUR-WS.\norg.\nSmirnova, N., & Mayr, P.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2020",
        "content": ". A comprehensive analysis of acknowledgement texts in web of sci-\nence: A case study on four scientific domains. Scientometrics, 1(128), 709–734. https:// doi. org/ 10.  \n1007/ s11192- 022- 04554-9\nSong, M., Kang, K. Y., Timakum, T., & Zhang, X.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2014",
        "content": ". Examining influential factors for acknowl-\nedgements classification using supervised learning. PLoS ONE. https:// doi. org/ 10. 1371/ journ al.  \npone. 02289 28\nThomer, A. K., & Weber, N. M.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics pp. 1133 – 1138",
        "content": ". Using named entity recognition as a classification heuristic. In \niConference 2014 Proceedings",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2011",
        "content": ". iSchools.\nWang, J., & Shapira, P.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2020",
        "content": ". Funding acknowledgement analysis: An enhanced tool to investigate \nresearch sponsorship impacts: The case of nanotechnology. Scientometrics, 87(3), 563–586. https://  \ndoi. org/ 10. 1007/ s11192- 011- 0362-5\nYamada, I., Asai, A., Shindo, H., Takeda, H., & Matsumoto, Y.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics pp. 6442–6454",
        "content": ". LUKE: Deep contextualized \nentity representations with entity-aware self-attention. In Proceedings of the 2020 Conference on \nEmpirical Methods in Natural Language Processing (EMNLP)",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2020",
        "content": ". Association for \nComputational Linguistics.Scientometrics \n1 3\nYu, J., Bohnet, B., & Poesio, M.",
        "type": "regular"
    },
    {
        "citation": "Computational Linguistics 2023",
        "content": ". Named entity recognition as dependency parsing. 10.48550/\nARXIV.2005.07150.\nZhang, C., Mayr, P., Lu, W., & Zhang, Y.",
        "type": "regular"
    }
]
{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 14658262,
     "sourceType": "datasetVersion",
     "datasetId": 9364102
    },
    {
     "sourceId": 294555354,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Task 3: Citation Span Extraction - Inference\n\n**Model:** QA model trained with positions (BERT/RoBERTa/SciBERT)\n\n**Task:** Extract text span that each citation supports\n\n**Metrics:** F1 Score + Exact Match\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import transformers, datasets\nprint(f\"‚úÖ transformers: {transformers.__version__}\")\nprint(f\"‚úÖ datasets: {datasets.__version__}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-29T04:03:13.033634Z",
     "iopub.execute_input": "2026-01-29T04:03:13.033923Z",
     "iopub.status.idle": "2026-01-29T04:03:13.038445Z",
     "shell.execute_reply.started": "2026-01-29T04:03:13.033899Z",
     "shell.execute_reply": "2026-01-29T04:03:13.037692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "‚úÖ transformers: 4.57.1\n‚úÖ datasets: 4.4.2\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": "# Configuration\nMODEL_PATH = '/kaggle/input/task3-bert-training-withpositions/models/task3_bert_with_positions_final'\nTEST_DIR = '/kaggle/input/thesis-data-task3-with-positions-test-gold-500/test_gold_500'\nOUTPUT_DIR = '/kaggle/working/predictions'\nEVAL_OUTPUT = '/kaggle/working/evaluation_results.json'\n\nprint(f\"üìÇ Model: {MODEL_PATH}\")\nprint(f\"üìÇ Test data: {TEST_DIR}\")\nprint(f\"üìÇ Output: {OUTPUT_DIR}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-29T04:03:13.039655Z",
     "iopub.execute_input": "2026-01-29T04:03:13.039861Z",
     "iopub.status.idle": "2026-01-29T04:03:13.053031Z",
     "shell.execute_reply.started": "2026-01-29T04:03:13.039841Z",
     "shell.execute_reply": "2026-01-29T04:03:13.052500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üìÇ Model: /kaggle/input/task3-bert-training-withpositions/models/task3_bert_with_positions_final\nüìÇ Test data: /kaggle/input/thesis-data-task3-with-positions-test-gold-500/test_gold_500\nüìÇ Output: /kaggle/working/predictions\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": "# Load model\nimport torch\nfrom transformers import pipeline\n\ndevice = 0 if torch.cuda.is_available() else -1\nprint(f\"Device: {'GPU' if device == 0 else 'CPU'}\")\n\nqa_pipeline = pipeline(\n    'question-answering',\n    model=MODEL_PATH,\n    tokenizer=MODEL_PATH,\n    device=device\n)\n\nprint(\"‚úÖ Model loaded successfully\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-29T04:03:13.054444Z",
     "iopub.execute_input": "2026-01-29T04:03:13.054755Z",
     "iopub.status.idle": "2026-01-29T04:03:13.370336Z",
     "shell.execute_reply.started": "2026-01-29T04:03:13.054732Z",
     "shell.execute_reply": "2026-01-29T04:03:13.369739Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Device set to use cuda:0\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Device: GPU\n‚úÖ Model loaded successfully\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": "# Inference function\ndef extract_citation_span(text: str, citation_id: str):\n    \"\"\"Extract span using QA model.\"\"\"\n    question = f\"What does citation {citation_id} support?\"\n    \n    try:\n        result = qa_pipeline(\n            question=question,\n            context=text,\n            max_seq_len=512,\n            handle_impossible_answer=False\n        )\n        \n        return {\n            'span_text': result['answer'],\n            'score': result['score'],\n            'start': result['start'],\n            'end': result['end']\n        }\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Error: {e}\")\n        return {\n            'span_text': '',\n            'score': 0.0,\n            'start': -1,\n            'end': -1\n        }\n\nprint(\"‚úÖ Inference function defined\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-29T04:03:13.371167Z",
     "iopub.execute_input": "2026-01-29T04:03:13.371452Z",
     "iopub.status.idle": "2026-01-29T04:03:13.377332Z",
     "shell.execute_reply.started": "2026-01-29T04:03:13.371421Z",
     "shell.execute_reply": "2026-01-29T04:03:13.376558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "‚úÖ Inference function defined\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": "# Run inference\nimport json\nfrom pathlib import Path\nfrom tqdm import tqdm\n\ntest_path = Path(TEST_DIR)\noutput_path = Path(OUTPUT_DIR)\noutput_path.mkdir(parents=True, exist_ok=True)\n\nlabel_files = sorted(test_path.glob(\"*.label\"))\nprint(f\"üìä Found {len(label_files)} files\")\nprint(\"=\" * 60)\n\nstats = {\n    'total_files': 0,\n    'total_citations': 0,\n    'successful': 0,\n    'failed': 0\n}\n\nfor label_file in tqdm(label_files):\n    try:\n        # Read file\n        with open(label_file) as f:\n            label_data = json.load(f)\n        \n        text = label_data.get('text', '')\n        if not text:\n            stats['failed'] += 1\n            continue\n        \n        # Get citations\n        citation_ids = list(label_data.get('correct_citation', {}).keys())\n        \n        # Extract spans - format y chang nh∆∞ file .label g·ªëc\n        citation_spans = []\n        for citation_id in citation_ids:\n            result = extract_citation_span(text, citation_id)\n            \n            citation_spans.append({\n                'citation_id': citation_id,\n                'span_text': result['span_text'],\n                's_span': result['start'],\n                'e_span': result['end']\n            })\n            \n            if result['score'] > 0:\n                stats['successful'] += 1\n            else:\n                stats['failed'] += 1\n            \n            stats['total_citations'] += 1\n        \n        # Save predictions - structure y chang file .label\n        output_data = {\n            'doc_id': label_data.get('doc_id', label_file.stem),\n            'text': text,\n            'correct_citation': label_data.get('correct_citation', {}),\n            'citation_spans': citation_spans,  # Y chang t√™n field trong .label\n            'bib_entries': label_data.get('bib_entries', {}),  # Gi·ªØ nguy√™n bib_entries\n            'generator': 'qa_model_inference'  # ƒê√°nh d·∫•u l√† model prediction\n        }\n        \n        output_file = output_path / label_file.name\n        with open(output_file, 'w') as f:\n            json.dump(output_data, f, indent=2, ensure_ascii=False)\n        \n        stats['total_files'] += 1\n        \n    except Exception as e:\n        print(f\"\\n‚ùå Error processing {label_file.name}: {e}\")\n        stats['failed'] += 1\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üìä INFERENCE RESULTS\")\nprint(\"=\" * 60)\nprint(f\"Files processed: {stats['total_files']}\")\nprint(f\"Total citations: {stats['total_citations']}\")\nprint(f\"‚úÖ Successful: {stats['successful']} ({stats['successful']/max(stats['total_citations'],1)*100:.1f}%)\")\nprint(f\"‚ùå Failed: {stats['failed']} ({stats['failed']/max(stats['total_citations'],1)*100:.1f}%)\")\nprint(\"=\" * 60)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-29T04:03:13.378846Z",
     "iopub.execute_input": "2026-01-29T04:03:13.379096Z",
     "iopub.status.idle": "2026-01-29T04:03:38.743225Z",
     "shell.execute_reply.started": "2026-01-29T04:03:13.379075Z",
     "shell.execute_reply": "2026-01-29T04:03:38.742529Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Evaluation - Calculate F1 and Exact Match\nimport numpy as np\n\ndef calculate_f1_em(pred_start, pred_end, true_start, true_end):\n    \"\"\"Calculate F1 score and Exact Match for character-level spans.\"\"\"\n    # Exact Match\n    exact_match = 1 if (pred_start == true_start and pred_end == true_end) else 0\n    \n    # F1 Score\n    if pred_start == -1 or pred_end == -1:\n        return 0.0, exact_match\n    \n    if pred_end < pred_start:\n        pred_end = pred_start\n    \n    # Calculate overlap\n    overlap_start = max(pred_start, true_start)\n    overlap_end = min(pred_end, true_end)\n    overlap = max(0, overlap_end - overlap_start)\n    \n    if overlap == 0:\n        return 0.0, exact_match\n    \n    pred_length = pred_end - pred_start\n    true_length = true_end - true_start\n    \n    precision = overlap / pred_length if pred_length > 0 else 0\n    recall = overlap / true_length if true_length > 0 else 0\n    \n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * precision * recall / (precision + recall)\n    \n    return f1, exact_match\n\nprint(\"‚úÖ Evaluation function defined\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-29T04:03:38.744161Z",
     "iopub.execute_input": "2026-01-29T04:03:38.744444Z",
     "iopub.status.idle": "2026-01-29T04:03:38.751076Z",
     "shell.execute_reply.started": "2026-01-29T04:03:38.744419Z",
     "shell.execute_reply": "2026-01-29T04:03:38.750416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "‚úÖ Evaluation function defined\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": "# Evaluate all predictions\nprediction_files = sorted(output_path.glob(\"*.label\"))\n\nall_f1_scores = []\nall_exact_matches = []\nfile_results = []\n\nfor pred_file in tqdm(prediction_files, desc=\"Evaluating\"):\n    with open(pred_file) as f:\n        data = json.load(f)\n    \n    # ƒê·ªçc ground truth t·ª´ test data g·ªëc\n    gt_file = test_path / pred_file.name\n    with open(gt_file) as f:\n        gt_data = json.load(f)\n    \n    if 'citation_spans' not in gt_data:\n        continue\n    \n    # Ground truth spans\n    ground_truth = {\n        span['citation_id']: span\n        for span in gt_data['citation_spans']\n    }\n    \n    # Predicted spans\n    predictions = {\n        span['citation_id']: span\n        for span in data['citation_spans']\n    }\n    \n    file_f1_scores = []\n    file_exact_matches = []\n    \n    for citation_id, gt_span in ground_truth.items():\n        if citation_id not in predictions:\n            file_f1_scores.append(0.0)\n            file_exact_matches.append(0)\n            continue\n        \n        pred = predictions[citation_id]\n        \n        true_start = gt_span.get('s_span', -1)\n        true_end = gt_span.get('e_span', -1)\n        pred_start = pred.get('s_span', -1)\n        pred_end = pred.get('e_span', -1)\n        \n        if true_start == -1 or true_end == -1:\n            continue\n        \n        f1, em = calculate_f1_em(pred_start, pred_end, true_start, true_end)\n        \n        file_f1_scores.append(f1)\n        file_exact_matches.append(em)\n    \n    all_f1_scores.extend(file_f1_scores)\n    all_exact_matches.extend(file_exact_matches)\n    \n    file_results.append({\n        'file': pred_file.name,\n        'num_citations': len(file_f1_scores),\n        'avg_f1': np.mean(file_f1_scores) if file_f1_scores else 0,\n        'avg_em': np.mean(file_exact_matches) if file_exact_matches else 0\n    })\n\n# Overall metrics\noverall_metrics = {\n    'total_files': len(file_results),\n    'total_citations': len(all_f1_scores),\n    'f1_score': np.mean(all_f1_scores) if all_f1_scores else 0,\n    'exact_match': np.mean(all_exact_matches) if all_exact_matches else 0,\n    'file_results': file_results\n}\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üìä EVALUATION RESULTS\")\nprint(\"=\" * 60)\nprint(f\"Files evaluated: {overall_metrics['total_files']}\")\nprint(f\"Total citations: {overall_metrics['total_citations']}\")\nprint(f\"F1 Score: {overall_metrics['f1_score']:.4f} ({overall_metrics['f1_score']*100:.2f}%)\")\nprint(f\"Exact Match: {overall_metrics['exact_match']:.4f} ({overall_metrics['exact_match']*100:.2f}%)\")\nprint(\"=\" * 60)\n\n# Save evaluation results\nwith open(EVAL_OUTPUT, 'w') as f:\n    json.dump(overall_metrics, f, indent=2, ensure_ascii=False)\n\nprint(f\"\\n‚úÖ Evaluation results saved to: {EVAL_OUTPUT}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-29T04:03:38.751836Z",
     "iopub.execute_input": "2026-01-29T04:03:38.752303Z",
     "iopub.status.idle": "2026-01-29T04:03:38.805643Z",
     "shell.execute_reply.started": "2026-01-29T04:03:38.752278Z",
     "shell.execute_reply": "2026-01-29T04:03:38.804997Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Sample prediction\nsample_file = sorted(output_path.glob(\"*.label\"))[0]\nwith open(sample_file) as f:\n    sample = json.load(f)\n\nprint(f\"üìã Sample: {sample['doc_id']}\")\nprint(f\"\\nText: {sample['text'][:200]}...\")\nprint(f\"\\nCorrect Citations: {sample['correct_citation']}\")\nprint(f\"\\nPredicted Citation Spans:\")\nfor span in sample['citation_spans']:\n    print(f\"\\n{span['citation_id']}:\")\n    print(f\"  span_text: {span['span_text'][:100]}...\")\n    print(f\"  s_span: {span['s_span']}\")\n    print(f\"  e_span: {span['e_span']}\")\n    \nprint(f\"\\n\\nüìÑ Full structure (same as .label file):\")\nprint(json.dumps(sample, indent=2, ensure_ascii=False)[:500] + \"...\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-29T04:03:38.806389Z",
     "iopub.execute_input": "2026-01-29T04:03:38.806668Z",
     "iopub.status.idle": "2026-01-29T04:03:38.816299Z",
     "shell.execute_reply.started": "2026-01-29T04:03:38.806647Z",
     "shell.execute_reply": "2026-01-29T04:03:38.815723Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
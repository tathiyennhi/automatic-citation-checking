{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Citation Span Extraction - Inference\n",
    "\n",
    "**Model:** QA model trained with positions (BERT/RoBERTa/SciBERT)\n",
    "\n",
    "**Task:** Extract text span that each citation supports\n",
    "\n",
    "**Metrics:** F1 Score + Exact Match\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, datasets\n",
    "print(f\"‚úÖ transformers: {transformers.__version__}\")\n",
    "print(f\"‚úÖ datasets: {datasets.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = '/kaggle/input/your-trained-model/task3_scibert_with_positions_final'\n",
    "TEST_DIR = '/kaggle/input/thesis-data-task3-with-positions/test_gold_500'\n",
    "OUTPUT_DIR = '/kaggle/working/predictions'\n",
    "EVAL_OUTPUT = '/kaggle/working/evaluation_results.json'\n",
    "\n",
    "print(f\"üìÇ Model: {MODEL_PATH}\")\n",
    "print(f\"üìÇ Test data: {TEST_DIR}\")\n",
    "print(f\"üìÇ Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    'question-answering',\n",
    "    model=MODEL_PATH,\n",
    "    tokenizer=MODEL_PATH,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def extract_citation_span(text: str, citation_id: str):\n",
    "    \"\"\"Extract span using QA model.\"\"\"\n",
    "    question = f\"What does citation {citation_id} support?\"\n",
    "    \n",
    "    try:\n",
    "        result = qa_pipeline(\n",
    "            question=question,\n",
    "            context=text,\n",
    "            max_seq_len=512,\n",
    "            handle_impossible_answer=False\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'span_text': result['answer'],\n",
    "            'score': result['score'],\n",
    "            'start': result['start'],\n",
    "            'end': result['end']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error: {e}\")\n",
    "        return {\n",
    "            'span_text': '',\n",
    "            'score': 0.0,\n",
    "            'start': -1,\n",
    "            'end': -1\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Inference function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_path = Path(TEST_DIR)\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "label_files = sorted(test_path.glob(\"*.label\"))\n",
    "print(f\"üìä Found {len(label_files)} files\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stats = {\n",
    "    'total_files': 0,\n",
    "    'total_citations': 0,\n",
    "    'successful': 0,\n",
    "    'failed': 0\n",
    "}\n",
    "\n",
    "for label_file in tqdm(label_files):\n",
    "    try:\n",
    "        # Read file\n",
    "        with open(label_file) as f:\n",
    "            label_data = json.load(f)\n",
    "        \n",
    "        text = label_data.get('text', '')\n",
    "        if not text:\n",
    "            stats['failed'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Get citations\n",
    "        citation_ids = list(label_data.get('correct_citation', {}).keys())\n",
    "        \n",
    "        # Extract spans\n",
    "        predictions = []\n",
    "        for citation_id in citation_ids:\n",
    "            result = extract_citation_span(text, citation_id)\n",
    "            \n",
    "            predictions.append({\n",
    "                'citation_id': citation_id,\n",
    "                'predicted_span': result['span_text'],\n",
    "                'confidence': float(result['score']),\n",
    "                'start_char': result['start'],\n",
    "                'end_char': result['end']\n",
    "            })\n",
    "            \n",
    "            if result['score'] > 0:\n",
    "                stats['successful'] += 1\n",
    "            else:\n",
    "                stats['failed'] += 1\n",
    "            \n",
    "            stats['total_citations'] += 1\n",
    "        \n",
    "        # Save predictions\n",
    "        output_data = {\n",
    "            'doc_id': label_data.get('doc_id', label_file.stem),\n",
    "            'text': text,\n",
    "            'correct_citation': label_data.get('correct_citation', {}),\n",
    "            'predicted_spans': predictions,\n",
    "            'model': MODEL_PATH\n",
    "        }\n",
    "        \n",
    "        if 'citation_spans' in label_data:\n",
    "            output_data['ground_truth_spans'] = label_data['citation_spans']\n",
    "        \n",
    "        output_file = output_path / label_file.name\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        stats['total_files'] += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error processing {label_file.name}: {e}\")\n",
    "        stats['failed'] += 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä INFERENCE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Files processed: {stats['total_files']}\")\n",
    "print(f\"Total citations: {stats['total_citations']}\")\n",
    "print(f\"‚úÖ Successful: {stats['successful']} ({stats['successful']/max(stats['total_citations'],1)*100:.1f}%)\")\n",
    "print(f\"‚ùå Failed: {stats['failed']} ({stats['failed']/max(stats['total_citations'],1)*100:.1f}%)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation - Calculate F1 and Exact Match\n",
    "import numpy as np\n",
    "\n",
    "def calculate_f1_em(pred_start, pred_end, true_start, true_end):\n",
    "    \"\"\"Calculate F1 score and Exact Match for character-level spans.\"\"\"\n",
    "    # Exact Match\n",
    "    exact_match = 1 if (pred_start == true_start and pred_end == true_end) else 0\n",
    "    \n",
    "    # F1 Score\n",
    "    if pred_start == -1 or pred_end == -1:\n",
    "        return 0.0, exact_match\n",
    "    \n",
    "    if pred_end < pred_start:\n",
    "        pred_end = pred_start\n",
    "    \n",
    "    # Calculate overlap\n",
    "    overlap_start = max(pred_start, true_start)\n",
    "    overlap_end = min(pred_end, true_end)\n",
    "    overlap = max(0, overlap_end - overlap_start)\n",
    "    \n",
    "    if overlap == 0:\n",
    "        return 0.0, exact_match\n",
    "    \n",
    "    pred_length = pred_end - pred_start\n",
    "    true_length = true_end - true_start\n",
    "    \n",
    "    precision = overlap / pred_length if pred_length > 0 else 0\n",
    "    recall = overlap / true_length if true_length > 0 else 0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return f1, exact_match\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all predictions\n",
    "prediction_files = sorted(output_path.glob(\"*.label\"))\n",
    "\n",
    "all_f1_scores = []\n",
    "all_exact_matches = []\n",
    "file_results = []\n",
    "\n",
    "for pred_file in tqdm(prediction_files, desc=\"Evaluating\"):\n",
    "    with open(pred_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if 'ground_truth_spans' not in data:\n",
    "        continue\n",
    "    \n",
    "    ground_truth = {\n",
    "        span['citation_id']: span\n",
    "        for span in data['ground_truth_spans']\n",
    "    }\n",
    "    \n",
    "    predictions = {\n",
    "        pred['citation_id']: pred\n",
    "        for pred in data['predicted_spans']\n",
    "    }\n",
    "    \n",
    "    file_f1_scores = []\n",
    "    file_exact_matches = []\n",
    "    \n",
    "    for citation_id, gt_span in ground_truth.items():\n",
    "        if citation_id not in predictions:\n",
    "            file_f1_scores.append(0.0)\n",
    "            file_exact_matches.append(0)\n",
    "            continue\n",
    "        \n",
    "        pred = predictions[citation_id]\n",
    "        \n",
    "        true_start = gt_span.get('s_span', -1)\n",
    "        true_end = gt_span.get('e_span', -1)\n",
    "        pred_start = pred.get('start_char', -1)\n",
    "        pred_end = pred.get('end_char', -1)\n",
    "        \n",
    "        if true_start == -1 or true_end == -1:\n",
    "            continue\n",
    "        \n",
    "        f1, em = calculate_f1_em(pred_start, pred_end, true_start, true_end)\n",
    "        \n",
    "        file_f1_scores.append(f1)\n",
    "        file_exact_matches.append(em)\n",
    "    \n",
    "    all_f1_scores.extend(file_f1_scores)\n",
    "    all_exact_matches.extend(file_exact_matches)\n",
    "    \n",
    "    file_results.append({\n",
    "        'file': pred_file.name,\n",
    "        'num_citations': len(file_f1_scores),\n",
    "        'avg_f1': np.mean(file_f1_scores) if file_f1_scores else 0,\n",
    "        'avg_em': np.mean(file_exact_matches) if file_exact_matches else 0\n",
    "    })\n",
    "\n",
    "# Overall metrics\n",
    "overall_metrics = {\n",
    "    'total_files': len(file_results),\n",
    "    'total_citations': len(all_f1_scores),\n",
    "    'f1_score': np.mean(all_f1_scores) if all_f1_scores else 0,\n",
    "    'exact_match': np.mean(all_exact_matches) if all_exact_matches else 0,\n",
    "    'file_results': file_results\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Files evaluated: {overall_metrics['total_files']}\")\n",
    "print(f\"Total citations: {overall_metrics['total_citations']}\")\n",
    "print(f\"F1 Score: {overall_metrics['f1_score']:.4f} ({overall_metrics['f1_score']*100:.2f}%)\")\n",
    "print(f\"Exact Match: {overall_metrics['exact_match']:.4f} ({overall_metrics['exact_match']*100:.2f}%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save evaluation results\n",
    "with open(EVAL_OUTPUT, 'w') as f:\n",
    "    json.dump(overall_metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation results saved to: {EVAL_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prediction\n",
    "sample_file = sorted(output_path.glob(\"*.label\"))[0]\n",
    "with open(sample_file) as f:\n",
    "    sample = json.load(f)\n",
    "\n",
    "print(f\"üìã Sample: {sample['doc_id']}\")\n",
    "print(f\"\\nText: {sample['text'][:200]}...\")\n",
    "print(f\"\\nPredictions:\")\n",
    "for pred in sample['predicted_spans']:\n",
    "    print(f\"\\n{pred['citation_id']}:\")\n",
    "    print(f\"  Predicted: {pred['predicted_span'][:100]}...\")\n",
    "    print(f\"  Confidence: {pred['confidence']:.4f}\")\n",
    "    print(f\"  Position: [{pred['start_char']}, {pred['end_char']}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

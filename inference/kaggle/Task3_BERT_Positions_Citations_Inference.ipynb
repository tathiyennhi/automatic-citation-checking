{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14661098,"sourceType":"datasetVersion","datasetId":9366061},{"sourceId":294733078,"sourceType":"kernelVersion"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task 3: Citation Span Extraction - Inference\n\n**Model:** QA model trained with positions (BERT/RoBERTa/SciBERT)\n\n**Task:** Extract text span that each citation supports\n\n**Metrics:** F1 Score + Exact Match\n\n---","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import transformers, datasets\nprint(f\"‚úÖ transformers: {transformers.__version__}\")\nprint(f\"‚úÖ datasets: {datasets.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-30T08:27:12.307984Z","iopub.execute_input":"2026-01-30T08:27:12.308624Z","iopub.status.idle":"2026-01-30T08:27:24.078145Z","shell.execute_reply.started":"2026-01-30T08:27:12.308587Z","shell.execute_reply":"2026-01-30T08:27:24.077030Z"},"trusted":true},"outputs":[{"name":"stdout","text":"‚úÖ transformers: 4.57.1\n‚úÖ datasets: 4.4.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Configuration\nMODEL_PATH = '/kaggle/input/task3-bert-training-positions-citations/models/task3_bert_with_positions_final'\nTEST_DIR = '/kaggle/input/thesis-data-task3-positions-citations/data/test_gold_500'\nOUTPUT_DIR = '/kaggle/working/predictions'\nEVAL_OUTPUT = '/kaggle/working/evaluation_results.json'\n\nprint(f\"üìÇ Model: {MODEL_PATH}\")\nprint(f\"üìÇ Test data: {TEST_DIR}\")\nprint(f\"üìÇ Output: {OUTPUT_DIR}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-30T08:27:24.080688Z","iopub.execute_input":"2026-01-30T08:27:24.081307Z","iopub.status.idle":"2026-01-30T08:27:24.088141Z","shell.execute_reply.started":"2026-01-30T08:27:24.081275Z","shell.execute_reply":"2026-01-30T08:27:24.086849Z"},"trusted":true},"outputs":[{"name":"stdout","text":"üìÇ Model: /kaggle/input/task3-bert-training-positions-citations/models/task3_bert_with_positions_final\nüìÇ Test data: /kaggle/input/thesis-data-task3-positions-citations/data/test_gold_500\nüìÇ Output: /kaggle/working/predictions\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load model\nimport torch\nfrom transformers import pipeline\n\ndevice = 0 if torch.cuda.is_available() else -1\nprint(f\"Device: {'GPU' if device == 0 else 'CPU'}\")\n\nqa_pipeline = pipeline(\n    'question-answering',\n    model=MODEL_PATH,\n    tokenizer=MODEL_PATH,\n    device=device\n)\n\nprint(\"‚úÖ Model loaded successfully\")","metadata":{"execution":{"iopub.status.busy":"2026-01-30T08:27:24.089587Z","iopub.execute_input":"2026-01-30T08:27:24.089907Z","iopub.status.idle":"2026-01-30T08:27:56.455037Z","shell.execute_reply.started":"2026-01-30T08:27:24.089881Z","shell.execute_reply":"2026-01-30T08:27:56.454163Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2026-01-30 08:27:31.131966: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769761651.421076      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769761651.501431      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769761652.208962      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769761652.209018      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769761652.209022      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769761652.209025      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Device: CPU\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Model loaded successfully\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Inference function\ndef extract_citation_span(text: str, citation_id: str):\n    \"\"\"Extract span using QA model.\"\"\"\n    question = f\"What does citation {citation_id} support?\"\n    \n    try:\n        result = qa_pipeline(\n            question=question,\n            context=text,\n            max_seq_len=512,\n            handle_impossible_answer=False\n        )\n        \n        return {\n            'span_text': result['answer'],\n            'score': result['score'],\n            'start': result['start'],\n            'end': result['end']\n        }\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Error: {e}\")\n        return {\n            'span_text': '',\n            'score': 0.0,\n            'start': -1,\n            'end': -1\n        }\n\nprint(\"‚úÖ Inference function defined\")","metadata":{"execution":{"iopub.status.busy":"2026-01-30T08:27:56.455993Z","iopub.execute_input":"2026-01-30T08:27:56.456711Z","iopub.status.idle":"2026-01-30T08:27:56.464988Z","shell.execute_reply.started":"2026-01-30T08:27:56.456681Z","shell.execute_reply":"2026-01-30T08:27:56.463850Z"},"trusted":true},"outputs":[{"name":"stdout","text":"‚úÖ Inference function defined\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Run inference\nimport json\nfrom pathlib import Path\nfrom tqdm import tqdm\n\ntest_path = Path(TEST_DIR)\noutput_path = Path(OUTPUT_DIR)\noutput_path.mkdir(parents=True, exist_ok=True)\n\nlabel_files = sorted(test_path.glob(\"*.label\"))\nprint(f\"üìä Found {len(label_files)} files\")\nprint(\"=\" * 60)\n\nstats = {\n    'total_files': 0,\n    'total_citations': 0,\n    'successful': 0,\n    'failed': 0\n}\n\nfor label_file in tqdm(label_files):\n    try:\n        # Read file\n        with open(label_file) as f:\n            label_data = json.load(f)\n        \n        text = label_data.get('text', '')\n        if not text:\n            stats['failed'] += 1\n            continue\n        \n        # Get citations\n        citation_ids = list(label_data.get('correct_citation', {}).keys())\n        \n        # Extract spans - format y chang nh∆∞ file .label g·ªëc\n        citation_spans = []\n        for citation_id in citation_ids:\n            result = extract_citation_span(text, citation_id)\n            \n            citation_spans.append({\n                'citation_id': citation_id,\n                'span_text': result['span_text'],\n                's_span': result['start'],\n                'e_span': result['end']\n            })\n            \n            if result['score'] > 0:\n                stats['successful'] += 1\n            else:\n                stats['failed'] += 1\n            \n            stats['total_citations'] += 1\n        \n        # Save predictions - structure y chang file .label\n        output_data = {\n            'doc_id': label_data.get('doc_id', label_file.stem),\n            'text': text,\n            'correct_citation': label_data.get('correct_citation', {}),\n            'citation_spans': citation_spans,  # Y chang t√™n field trong .label\n            'bib_entries': label_data.get('bib_entries', {}),  # Gi·ªØ nguy√™n bib_entries\n            'generator': 'qa_model_inference'  # ƒê√°nh d·∫•u l√† model prediction\n        }\n        \n        output_file = output_path / label_file.name\n        with open(output_file, 'w') as f:\n            json.dump(output_data, f, indent=2, ensure_ascii=False)\n        \n        stats['total_files'] += 1\n        \n    except Exception as e:\n        print(f\"\\n‚ùå Error processing {label_file.name}: {e}\")\n        stats['failed'] += 1\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üìä INFERENCE RESULTS\")\nprint(\"=\" * 60)\nprint(f\"Files processed: {stats['total_files']}\")\nprint(f\"Total citations: {stats['total_citations']}\")\nprint(f\"‚úÖ Successful: {stats['successful']} ({stats['successful']/max(stats['total_citations'],1)*100:.1f}%)\")\nprint(f\"‚ùå Failed: {stats['failed']} ({stats['failed']/max(stats['total_citations'],1)*100:.1f}%)\")\nprint(\"=\" * 60)","metadata":{"execution":{"iopub.status.busy":"2026-01-30T08:27:56.466411Z","iopub.execute_input":"2026-01-30T08:27:56.466993Z","iopub.status.idle":"2026-01-30T08:39:48.363208Z","shell.execute_reply.started":"2026-01-30T08:27:56.466923Z","shell.execute_reply":"2026-01-30T08:39:48.362116Z"},"trusted":true},"outputs":[{"name":"stdout","text":"üìä Found 500 files\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [11:51<00:00,  1.42s/it]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüìä INFERENCE RESULTS\n============================================================\nFiles processed: 500\nTotal citations: 1272\n‚úÖ Successful: 1272 (100.0%)\n‚ùå Failed: 0 (0.0%)\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Evaluation - Calculate F1 and Exact Match\nimport numpy as np\n\ndef calculate_f1_em(pred_start, pred_end, true_start, true_end):\n    \"\"\"Calculate F1 score and Exact Match for character-level spans.\"\"\"\n    # Exact Match\n    exact_match = 1 if (pred_start == true_start and pred_end == true_end) else 0\n    \n    # F1 Score\n    if pred_start == -1 or pred_end == -1:\n        return 0.0, exact_match\n    \n    if pred_end < pred_start:\n        pred_end = pred_start\n    \n    # Calculate overlap\n    overlap_start = max(pred_start, true_start)\n    overlap_end = min(pred_end, true_end)\n    overlap = max(0, overlap_end - overlap_start)\n    \n    if overlap == 0:\n        return 0.0, exact_match\n    \n    pred_length = pred_end - pred_start\n    true_length = true_end - true_start\n    \n    precision = overlap / pred_length if pred_length > 0 else 0\n    recall = overlap / true_length if true_length > 0 else 0\n    \n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * precision * recall / (precision + recall)\n    \n    return f1, exact_match\n\nprint(\"‚úÖ Evaluation function defined\")","metadata":{"execution":{"iopub.status.busy":"2026-01-30T08:39:48.364909Z","iopub.execute_input":"2026-01-30T08:39:48.365404Z","iopub.status.idle":"2026-01-30T08:39:48.374222Z","shell.execute_reply.started":"2026-01-30T08:39:48.365372Z","shell.execute_reply":"2026-01-30T08:39:48.372976Z"},"trusted":true},"outputs":[{"name":"stdout","text":"‚úÖ Evaluation function defined\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Evaluate all predictions\nprediction_files = sorted(output_path.glob(\"*.label\"))\n\nall_f1_scores = []\nall_exact_matches = []\nfile_results = []\n\nfor pred_file in tqdm(prediction_files, desc=\"Evaluating\"):\n    with open(pred_file) as f:\n        data = json.load(f)\n    \n    # ƒê·ªçc ground truth t·ª´ test data g·ªëc\n    gt_file = test_path / pred_file.name\n    with open(gt_file) as f:\n        gt_data = json.load(f)\n    \n    if 'citation_spans' not in gt_data:\n        continue\n    \n    # Ground truth spans\n    ground_truth = {\n        span['citation_id']: span\n        for span in gt_data['citation_spans']\n    }\n    \n    # Predicted spans\n    predictions = {\n        span['citation_id']: span\n        for span in data['citation_spans']\n    }\n    \n    file_f1_scores = []\n    file_exact_matches = []\n    \n    for citation_id, gt_span in ground_truth.items():\n        if citation_id not in predictions:\n            file_f1_scores.append(0.0)\n            file_exact_matches.append(0)\n            continue\n        \n        pred = predictions[citation_id]\n        \n        true_start = gt_span.get('s_span', -1)\n        true_end = gt_span.get('e_span', -1)\n        pred_start = pred.get('s_span', -1)\n        pred_end = pred.get('e_span', -1)\n        \n        if true_start == -1 or true_end == -1:\n            continue\n        \n        f1, em = calculate_f1_em(pred_start, pred_end, true_start, true_end)\n        \n        file_f1_scores.append(f1)\n        file_exact_matches.append(em)\n    \n    all_f1_scores.extend(file_f1_scores)\n    all_exact_matches.extend(file_exact_matches)\n    \n    file_results.append({\n        'file': pred_file.name,\n        'num_citations': len(file_f1_scores),\n        'avg_f1': np.mean(file_f1_scores) if file_f1_scores else 0,\n        'avg_em': np.mean(file_exact_matches) if file_exact_matches else 0\n    })\n\n# Overall metrics\noverall_metrics = {\n    'total_files': len(file_results),\n    'total_citations': len(all_f1_scores),\n    'f1_score': np.mean(all_f1_scores) if all_f1_scores else 0,\n    'exact_match': np.mean(all_exact_matches) if all_exact_matches else 0,\n    'file_results': file_results\n}\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üìä EVALUATION RESULTS\")\nprint(\"=\" * 60)\nprint(f\"Files evaluated: {overall_metrics['total_files']}\")\nprint(f\"Total citations: {overall_metrics['total_citations']}\")\nprint(f\"F1 Score: {overall_metrics['f1_score']:.4f} ({overall_metrics['f1_score']*100:.2f}%)\")\nprint(f\"Exact Match: {overall_metrics['exact_match']:.4f} ({overall_metrics['exact_match']*100:.2f}%)\")\nprint(\"=\" * 60)\n\n# Save evaluation results\nwith open(EVAL_OUTPUT, 'w') as f:\n    json.dump(overall_metrics, f, indent=2, ensure_ascii=False)\n\nprint(f\"\\n‚úÖ Evaluation results saved to: {EVAL_OUTPUT}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-30T08:39:48.376581Z","iopub.execute_input":"2026-01-30T08:39:48.376840Z","iopub.status.idle":"2026-01-30T08:39:48.974084Z","shell.execute_reply.started":"2026-01-30T08:39:48.376815Z","shell.execute_reply":"2026-01-30T08:39:48.973117Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:00<00:00, 891.53it/s]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nüìä EVALUATION RESULTS\n============================================================\nFiles evaluated: 500\nTotal citations: 1271\nF1 Score: 0.0370 (3.70%)\nExact Match: 0.0126 (1.26%)\n============================================================\n\n‚úÖ Evaluation results saved to: /kaggle/working/evaluation_results.json\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Sample prediction\nsample_file = sorted(output_path.glob(\"*.label\"))[0]\nwith open(sample_file) as f:\n    sample = json.load(f)\n\nprint(f\"üìã Sample: {sample['doc_id']}\")\nprint(f\"\\nText: {sample['text'][:200]}...\")\nprint(f\"\\nCorrect Citations: {sample['correct_citation']}\")\nprint(f\"\\nPredicted Citation Spans:\")\nfor span in sample['citation_spans']:\n    print(f\"\\n{span['citation_id']}:\")\n    print(f\"  span_text: {span['span_text'][:100]}...\")\n    print(f\"  s_span: {span['s_span']}\")\n    print(f\"  e_span: {span['e_span']}\")\n    \nprint(f\"\\n\\nüìÑ Full structure (same as .label file):\")\nprint(json.dumps(sample, indent=2, ensure_ascii=False)[:500] + \"...\")","metadata":{"execution":{"iopub.status.busy":"2026-01-30T08:39:48.975364Z","iopub.execute_input":"2026-01-30T08:39:48.975745Z","iopub.status.idle":"2026-01-30T08:39:48.990750Z","shell.execute_reply.started":"2026-01-30T08:39:48.975717Z","shell.execute_reply":"2026-01-30T08:39:48.989758Z"},"trusted":true},"outputs":[{"name":"stdout","text":"üìã Sample: 10050\n\nText: The current findings unequivocally demonstrate that cardiomyocytes must express members of the Fermitin/Kindlin family in order to develop as a functional syncytium. When Drosophila cardiomyocytes fai...\n\nCorrect Citations: {'[CITATION_1]': '285225', '[CITATION_2]': '8007412'}\n\nPredicted Citation Spans:\n\n[CITATION_1]:\n  span_text: Studies...\n  s_span: 401\n  e_span: 408\n\n[CITATION_2]:\n  span_text: Studies...\n  s_span: 401\n  e_span: 408\n\n\nüìÑ Full structure (same as .label file):\n{\n  \"doc_id\": \"10050\",\n  \"text\": \"The current findings unequivocally demonstrate that cardiomyocytes must express members of the Fermitin/Kindlin family in order to develop as a functional syncytium. When Drosophila cardiomyocytes fail to couple together to form a cardiac syncytium, synchronous contractions and fractional shortening of the adult heart are significantly reduced, despite individual cardiomyocytes remaining myogenic. Studies of vertebrate hearts suggest a role for Kind2 in cardiac ...\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ZIP all outputs for easy download\nimport shutil\nimport os\n\n# Create ZIP file\noutput_zip = '/kaggle/working/task3_predictions.zip'\nshutil.make_archive(\n    output_zip.replace('.zip', ''),  # base name without .zip\n    'zip',  # format\n    OUTPUT_DIR  # directory to zip\n)\n\n# Get file size\nzip_size_mb = os.path.getsize(output_zip) / (1024 * 1024)\n\nprint(\"=\" * 60)\nprint(\"üì¶ OUTPUT FILES ZIPPED\")\nprint(\"=\" * 60)\nprint(f\"ZIP file: {output_zip}\")\nprint(f\"Size: {zip_size_mb:.2f} MB\")\nprint(f\"Contains: {stats['total_files']} prediction files\")\nprint(\"=\" * 60)\nprint(\"\\n‚úÖ Download the ZIP file from Kaggle Output tab\")\nprint(\"   It contains all 500 .label prediction files\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T08:39:48.992128Z","iopub.execute_input":"2026-01-30T08:39:48.992495Z","iopub.status.idle":"2026-01-30T08:39:49.155106Z","shell.execute_reply.started":"2026-01-30T08:39:48.992454Z","shell.execute_reply":"2026-01-30T08:39:49.154017Z"}},"outputs":[{"name":"stdout","text":"============================================================\nüì¶ OUTPUT FILES ZIPPED\n============================================================\nZIP file: /kaggle/working/task3_predictions.zip\nSize: 0.95 MB\nContains: 500 prediction files\n============================================================\n\n‚úÖ Download the ZIP file from Kaggle Output tab\n   It contains all 500 .label prediction files\n","output_type":"stream"}],"execution_count":9}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Citation Span Extraction - Inference\n",
    "\n",
    "**Model:** QA model trained with positions (BERT/RoBERTa/SciBERT)\n",
    "\n",
    "**Task:** Extract text span that each citation supports\n",
    "\n",
    "**Metrics:** F1 Score + Exact Match\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T04:03:13.033923Z",
     "iopub.status.busy": "2026-01-29T04:03:13.033634Z",
     "iopub.status.idle": "2026-01-29T04:03:13.038445Z",
     "shell.execute_reply": "2026-01-29T04:03:13.037692Z",
     "shell.execute_reply.started": "2026-01-29T04:03:13.033899Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ transformers: 4.57.1\n",
      "‚úÖ datasets: 4.4.2\n"
     ]
    }
   ],
   "source": [
    "import transformers, datasets\n",
    "print(f\"‚úÖ transformers: {transformers.__version__}\")\n",
    "print(f\"‚úÖ datasets: {datasets.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T04:03:13.039861Z",
     "iopub.status.busy": "2026-01-29T04:03:13.039655Z",
     "iopub.status.idle": "2026-01-29T04:03:13.053031Z",
     "shell.execute_reply": "2026-01-29T04:03:13.052500Z",
     "shell.execute_reply.started": "2026-01-29T04:03:13.039841Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Model: /kaggle/input/task3-bert-training-withpositions/models/task3_bert_with_positions_final\n",
      "üìÇ Test data: /kaggle/input/thesis-data-task3-with-positions-test-gold-500/test_gold_500\n",
      "üìÇ Output: /kaggle/working/predictions\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = '/kaggle/input/task3-bert-training-withpositions/models/task3_bert_with_positions_final'\n",
    "TEST_DIR = '/kaggle/input/thesis-data-task3-with-positions-test-gold-500/test_gold_500'\n",
    "OUTPUT_DIR = '/kaggle/working/predictions'\n",
    "EVAL_OUTPUT = '/kaggle/working/evaluation_results.json'\n",
    "\n",
    "print(f\"üìÇ Model: {MODEL_PATH}\")\n",
    "print(f\"üìÇ Test data: {TEST_DIR}\")\n",
    "print(f\"üìÇ Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T04:03:13.054755Z",
     "iopub.status.busy": "2026-01-29T04:03:13.054444Z",
     "iopub.status.idle": "2026-01-29T04:03:13.370336Z",
     "shell.execute_reply": "2026-01-29T04:03:13.369739Z",
     "shell.execute_reply.started": "2026-01-29T04:03:13.054732Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: GPU\n",
      "‚úÖ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    'question-answering',\n",
    "    model=MODEL_PATH,\n",
    "    tokenizer=MODEL_PATH,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T04:03:13.371452Z",
     "iopub.status.busy": "2026-01-29T04:03:13.371167Z",
     "iopub.status.idle": "2026-01-29T04:03:13.377332Z",
     "shell.execute_reply": "2026-01-29T04:03:13.376558Z",
     "shell.execute_reply.started": "2026-01-29T04:03:13.371421Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference function defined\n"
     ]
    }
   ],
   "source": [
    "# Inference function\n",
    "def extract_citation_span(text: str, citation_id: str):\n",
    "    \"\"\"Extract span using QA model.\"\"\"\n",
    "    question = f\"What does citation {citation_id} support?\"\n",
    "    \n",
    "    try:\n",
    "        result = qa_pipeline(\n",
    "            question=question,\n",
    "            context=text,\n",
    "            max_seq_len=512,\n",
    "            handle_impossible_answer=False\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'span_text': result['answer'],\n",
    "            'score': result['score'],\n",
    "            'start': result['start'],\n",
    "            'end': result['end']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error: {e}\")\n",
    "        return {\n",
    "            'span_text': '',\n",
    "            'score': 0.0,\n",
    "            'start': -1,\n",
    "            'end': -1\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Inference function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T04:03:13.379096Z",
     "iopub.status.busy": "2026-01-29T04:03:13.378846Z",
     "iopub.status.idle": "2026-01-29T04:03:38.743225Z",
     "shell.execute_reply": "2026-01-29T04:03:38.742529Z",
     "shell.execute_reply.started": "2026-01-29T04:03:13.379075Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run inference\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_path = Path(TEST_DIR)\n",
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "label_files = sorted(test_path.glob(\"*.label\"))\n",
    "print(f\"üìä Found {len(label_files)} files\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stats = {\n",
    "    'total_files': 0,\n",
    "    'total_citations': 0,\n",
    "    'successful': 0,\n",
    "    'failed': 0\n",
    "}\n",
    "\n",
    "for label_file in tqdm(label_files):\n",
    "    try:\n",
    "        # Read file\n",
    "        with open(label_file) as f:\n",
    "            label_data = json.load(f)\n",
    "        \n",
    "        text = label_data.get('text', '')\n",
    "        if not text:\n",
    "            stats['failed'] += 1\n",
    "            continue\n",
    "        \n",
    "        # Get citations\n",
    "        citation_ids = list(label_data.get('correct_citation', {}).keys())\n",
    "        \n",
    "        # Extract spans - format y chang nh∆∞ file .label g·ªëc\n",
    "        citation_spans = []\n",
    "        for citation_id in citation_ids:\n",
    "            result = extract_citation_span(text, citation_id)\n",
    "            \n",
    "            citation_spans.append({\n",
    "                'citation_id': citation_id,\n",
    "                'span_text': result['span_text'],\n",
    "                's_span': result['start'],\n",
    "                'e_span': result['end']\n",
    "            })\n",
    "            \n",
    "            if result['score'] > 0:\n",
    "                stats['successful'] += 1\n",
    "            else:\n",
    "                stats['failed'] += 1\n",
    "            \n",
    "            stats['total_citations'] += 1\n",
    "        \n",
    "        # Save predictions - structure y chang file .label\n",
    "        output_data = {\n",
    "            'doc_id': label_data.get('doc_id', label_file.stem),\n",
    "            'text': text,\n",
    "            'correct_citation': label_data.get('correct_citation', {}),\n",
    "            'citation_spans': citation_spans,  # Y chang t√™n field trong .label\n",
    "            'bib_entries': label_data.get('bib_entries', {}),  # Gi·ªØ nguy√™n bib_entries\n",
    "            'generator': 'qa_model_inference'  # ƒê√°nh d·∫•u l√† model prediction\n",
    "        }\n",
    "        \n",
    "        output_file = output_path / label_file.name\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        stats['total_files'] += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error processing {label_file.name}: {e}\")\n",
    "        stats['failed'] += 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä INFERENCE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Files processed: {stats['total_files']}\")\n",
    "print(f\"Total citations: {stats['total_citations']}\")\n",
    "print(f\"‚úÖ Successful: {stats['successful']} ({stats['successful']/max(stats['total_citations'],1)*100:.1f}%)\")\n",
    "print(f\"‚ùå Failed: {stats['failed']} ({stats['failed']/max(stats['total_citations'],1)*100:.1f}%)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T04:03:38.744444Z",
     "iopub.status.busy": "2026-01-29T04:03:38.744161Z",
     "iopub.status.idle": "2026-01-29T04:03:38.751076Z",
     "shell.execute_reply": "2026-01-29T04:03:38.750416Z",
     "shell.execute_reply.started": "2026-01-29T04:03:38.744419Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "# Evaluation - Calculate F1 and Exact Match\n",
    "import numpy as np\n",
    "\n",
    "def calculate_f1_em(pred_start, pred_end, true_start, true_end):\n",
    "    \"\"\"Calculate F1 score and Exact Match for character-level spans.\"\"\"\n",
    "    # Exact Match\n",
    "    exact_match = 1 if (pred_start == true_start and pred_end == true_end) else 0\n",
    "    \n",
    "    # F1 Score\n",
    "    if pred_start == -1 or pred_end == -1:\n",
    "        return 0.0, exact_match\n",
    "    \n",
    "    if pred_end < pred_start:\n",
    "        pred_end = pred_start\n",
    "    \n",
    "    # Calculate overlap\n",
    "    overlap_start = max(pred_start, true_start)\n",
    "    overlap_end = min(pred_end, true_end)\n",
    "    overlap = max(0, overlap_end - overlap_start)\n",
    "    \n",
    "    if overlap == 0:\n",
    "        return 0.0, exact_match\n",
    "    \n",
    "    pred_length = pred_end - pred_start\n",
    "    true_length = true_end - true_start\n",
    "    \n",
    "    precision = overlap / pred_length if pred_length > 0 else 0\n",
    "    recall = overlap / true_length if true_length > 0 else 0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return f1, exact_match\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T04:03:38.752303Z",
     "iopub.status.busy": "2026-01-29T04:03:38.751836Z",
     "iopub.status.idle": "2026-01-29T04:03:38.805643Z",
     "shell.execute_reply": "2026-01-29T04:03:38.804997Z",
     "shell.execute_reply.started": "2026-01-29T04:03:38.752278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluate all predictions\n",
    "prediction_files = sorted(output_path.glob(\"*.label\"))\n",
    "\n",
    "all_f1_scores = []\n",
    "all_exact_matches = []\n",
    "file_results = []\n",
    "\n",
    "for pred_file in tqdm(prediction_files, desc=\"Evaluating\"):\n",
    "    with open(pred_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # ƒê·ªçc ground truth t·ª´ test data g·ªëc\n",
    "    gt_file = test_path / pred_file.name\n",
    "    with open(gt_file) as f:\n",
    "        gt_data = json.load(f)\n",
    "    \n",
    "    if 'citation_spans' not in gt_data:\n",
    "        continue\n",
    "    \n",
    "    # Ground truth spans\n",
    "    ground_truth = {\n",
    "        span['citation_id']: span\n",
    "        for span in gt_data['citation_spans']\n",
    "    }\n",
    "    \n",
    "    # Predicted spans\n",
    "    predictions = {\n",
    "        span['citation_id']: span\n",
    "        for span in data['citation_spans']\n",
    "    }\n",
    "    \n",
    "    file_f1_scores = []\n",
    "    file_exact_matches = []\n",
    "    \n",
    "    for citation_id, gt_span in ground_truth.items():\n",
    "        if citation_id not in predictions:\n",
    "            file_f1_scores.append(0.0)\n",
    "            file_exact_matches.append(0)\n",
    "            continue\n",
    "        \n",
    "        pred = predictions[citation_id]\n",
    "        \n",
    "        true_start = gt_span.get('s_span', -1)\n",
    "        true_end = gt_span.get('e_span', -1)\n",
    "        pred_start = pred.get('s_span', -1)\n",
    "        pred_end = pred.get('e_span', -1)\n",
    "        \n",
    "        if true_start == -1 or true_end == -1:\n",
    "            continue\n",
    "        \n",
    "        f1, em = calculate_f1_em(pred_start, pred_end, true_start, true_end)\n",
    "        \n",
    "        file_f1_scores.append(f1)\n",
    "        file_exact_matches.append(em)\n",
    "    \n",
    "    all_f1_scores.extend(file_f1_scores)\n",
    "    all_exact_matches.extend(file_exact_matches)\n",
    "    \n",
    "    file_results.append({\n",
    "        'file': pred_file.name,\n",
    "        'num_citations': len(file_f1_scores),\n",
    "        'avg_f1': np.mean(file_f1_scores) if file_f1_scores else 0,\n",
    "        'avg_em': np.mean(file_exact_matches) if file_exact_matches else 0\n",
    "    })\n",
    "\n",
    "# Overall metrics\n",
    "overall_metrics = {\n",
    "    'total_files': len(file_results),\n",
    "    'total_citations': len(all_f1_scores),\n",
    "    'f1_score': np.mean(all_f1_scores) if all_f1_scores else 0,\n",
    "    'exact_match': np.mean(all_exact_matches) if all_exact_matches else 0,\n",
    "    'file_results': file_results\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Files evaluated: {overall_metrics['total_files']}\")\n",
    "print(f\"Total citations: {overall_metrics['total_citations']}\")\n",
    "print(f\"F1 Score: {overall_metrics['f1_score']:.4f} ({overall_metrics['f1_score']*100:.2f}%)\")\n",
    "print(f\"Exact Match: {overall_metrics['exact_match']:.4f} ({overall_metrics['exact_match']*100:.2f}%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save evaluation results\n",
    "with open(EVAL_OUTPUT, 'w') as f:\n",
    "    json.dump(overall_metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation results saved to: {EVAL_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T04:03:38.806668Z",
     "iopub.status.busy": "2026-01-29T04:03:38.806389Z",
     "iopub.status.idle": "2026-01-29T04:03:38.816299Z",
     "shell.execute_reply": "2026-01-29T04:03:38.815723Z",
     "shell.execute_reply.started": "2026-01-29T04:03:38.806647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sample prediction\n",
    "sample_file = sorted(output_path.glob(\"*.label\"))[0]\n",
    "with open(sample_file) as f:\n",
    "    sample = json.load(f)\n",
    "\n",
    "print(f\"üìã Sample: {sample['doc_id']}\")\n",
    "print(f\"\\nText: {sample['text'][:200]}...\")\n",
    "print(f\"\\nCorrect Citations: {sample['correct_citation']}\")\n",
    "print(f\"\\nPredicted Citation Spans:\")\n",
    "for span in sample['citation_spans']:\n",
    "    print(f\"\\n{span['citation_id']}:\")\n",
    "    print(f\"  span_text: {span['span_text'][:100]}...\")\n",
    "    print(f\"  s_span: {span['s_span']}\")\n",
    "    print(f\"  e_span: {span['e_span']}\")\n",
    "    \n",
    "print(f\"\\n\\nüìÑ Full structure (same as .label file):\")\n",
    "print(json.dumps(sample, indent=2, ensure_ascii=False)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZIP all outputs for easy download\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Create ZIP file\n",
    "output_zip = '/kaggle/working/task3_predictions.zip'\n",
    "shutil.make_archive(\n",
    "    output_zip.replace('.zip', ''),  # base name without .zip\n",
    "    'zip',  # format\n",
    "    OUTPUT_DIR  # directory to zip\n",
    ")\n",
    "\n",
    "# Get file size\n",
    "zip_size_mb = os.path.getsize(output_zip) / (1024 * 1024)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üì¶ OUTPUT FILES ZIPPED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ZIP file: {output_zip}\")\n",
    "print(f\"Size: {zip_size_mb:.2f} MB\")\n",
    "print(f\"Contains: {stats['total_files']} prediction files\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úÖ Download the ZIP file from Kaggle Output tab\")\n",
    "print(\"   It contains all 500 .label prediction files\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9364102,
     "sourceId": 14658262,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 294555354,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

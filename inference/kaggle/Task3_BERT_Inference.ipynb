{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3: BERT Inference (Kaggle)\n",
        "\n",
        "Attach these Kaggle inputs before running:\n",
        "- Dataset: `thesis-data-task3-test-gold-500`\n",
        "- Notebook Output: `task3-bert-training` (latest version)\n",
        "\n",
        "The notebook loads the trained model, runs QA-based span extraction across 500 test files, reports Exact Match/F1, shows sample predictions, and saves `task3_bert_predictions.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 1: Setup\n",
        "# ============================================================\n",
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "print(\"\u2705 Libraries imported\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 2: Load Model\n",
        "# ============================================================\n",
        "# Path to saved model from training notebook output\n",
        "model_path = '/kaggle/input/task3-bert-training/models/task3_bert_final'\n",
        "\n",
        "print(f\"\ud83d\udce6 Loading model from: {model_path}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"\u2705 Model loaded on {device}\")\n",
        "print(f\"\ud83d\udcca Model parameters: {model.num_parameters():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 3: Load Test Data\n",
        "# ============================================================\n",
        "test_path = Path('/kaggle/input/thesis-data-task3-test-gold-500/test_gold_500')\n",
        "test_files = sorted(test_path.glob(\"*.in\"))\n",
        "\n",
        "print(f\"\ud83d\udcc2 Test path: {test_path}\")\n",
        "print(f\"\ud83d\udcca Found {len(test_files)} test files\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 4: Inference Function\n",
        "# ============================================================\n",
        "def extract_citation_span(text, citation, model, tokenizer, device, max_length=512):\n",
        "    \"\"\"\n",
        "    Extract span for a given citation using Question Answering approach\n",
        "    \"\"\"\n",
        "    # Create question\n",
        "    question = f\"What does citation {citation} support?\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "        question,\n",
        "        text,\n",
        "        max_length=max_length,\n",
        "        truncation='only_second',\n",
        "        return_tensors='pt',\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    # Move to device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get start and end positions\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    start_idx = torch.argmax(start_logits, dim=1).item()\n",
        "    end_idx = torch.argmax(end_logits, dim=1).item()\n",
        "\n",
        "    # Decode answer\n",
        "    if start_idx <= end_idx and start_idx > 0:\n",
        "        answer_tokens = inputs['input_ids'][0][start_idx:end_idx+1]\n",
        "        predicted_span = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
        "    else:\n",
        "        predicted_span = \"\"\n",
        "\n",
        "    # Get confidence scores\n",
        "    start_prob = torch.softmax(start_logits, dim=1)[0][start_idx].item()\n",
        "    end_prob = torch.softmax(end_logits, dim=1)[0][end_idx].item()\n",
        "    confidence = (start_prob + end_prob) / 2\n",
        "\n",
        "    return {\n",
        "        'predicted_span': predicted_span,\n",
        "        'start_idx': start_idx,\n",
        "        'end_idx': end_idx,\n",
        "        'confidence': confidence\n",
        "    }\n",
        "\n",
        "print(\"\u2705 Inference function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 5: Run Inference on Test Set\n",
        "# ============================================================\n",
        "results = []\n",
        "errors = []\n",
        "\n",
        "print(\"\ud83d\ude80 Starting inference...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for test_file in tqdm(test_files, desc=\"Processing files\"):\n",
        "    try:\n",
        "        # Load input file\n",
        "        with open(test_file) as f:\n",
        "            in_data = json.load(f)\n",
        "\n",
        "        # Load label file (for comparison)\n",
        "        label_file = test_file.with_suffix('.label')\n",
        "        with open(label_file) as f:\n",
        "            label_data = json.load(f)\n",
        "\n",
        "        text = in_data['text']\n",
        "        citation_spans = label_data.get('citation_spans', [])\n",
        "\n",
        "        # Process each citation\n",
        "        for span_info in citation_spans:\n",
        "            citation_id = span_info['citation_id']\n",
        "            gold_span = span_info['span_text']\n",
        "\n",
        "            # Run inference\n",
        "            prediction = extract_citation_span(\n",
        "                text=text,\n",
        "                citation=citation_id,\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            results.append({\n",
        "                'file': test_file.stem,\n",
        "                'citation_id': citation_id,\n",
        "                'gold_span': gold_span,\n",
        "                'predicted_span': prediction['predicted_span'],\n",
        "                'confidence': prediction['confidence'],\n",
        "                'start_idx': prediction['start_idx'],\n",
        "                'end_idx': prediction['end_idx']\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        errors.append({\n",
        "            'file': test_file.stem,\n",
        "            'error': str(e)\n",
        "        })\n",
        "\n",
        "print(f\"\n",
        "\u2705 Inference complete!\")\n",
        "print(f\"\ud83d\udcca Processed: {len(results)} predictions\")\n",
        "print(f\"\u274c Errors: {len(errors)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 6: Calculate Metrics\n",
        "# ============================================================\n",
        "def calculate_exact_match(gold, pred):\n",
        "    \"\"\"Exact match: predicted == gold (after normalization)\"\"\"\n",
        "    return gold.strip().lower() == pred.strip().lower()\n",
        "\n",
        "\n",
        "def calculate_f1(gold, pred):\n",
        "    \"\"\"F1 score based on token overlap\"\"\"\n",
        "    gold_tokens = gold.strip().lower().split()\n",
        "    pred_tokens = pred.strip().lower().split()\n",
        "\n",
        "    if len(gold_tokens) == 0 or len(pred_tokens) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    common = set(gold_tokens) & set(pred_tokens)\n",
        "\n",
        "    if len(common) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(gold_tokens)\n",
        "\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "# Calculate metrics\n",
        "exact_matches = 0\n",
        "total_f1 = 0\n",
        "\n",
        "for result in results:\n",
        "    if calculate_exact_match(result['gold_span'], result['predicted_span']):\n",
        "        exact_matches += 1\n",
        "\n",
        "    total_f1 += calculate_f1(result['gold_span'], result['predicted_span'])\n",
        "\n",
        "exact_match_score = exact_matches / len(results) if results else 0\n",
        "avg_f1_score = total_f1 / len(results) if results else 0\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"\ud83d\udcca EVALUATION METRICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total predictions: {len(results)}\")\n",
        "print(f\"Exact Match: {exact_match_score:.4f} ({exact_matches}/{len(results)})\")\n",
        "print(f\"F1 Score: {avg_f1_score:.4f}\")\n",
        "print(f\"Average Confidence: {sum(r['confidence'] for r in results)/len(results):.4f}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 7: Show Examples\n",
        "# ============================================================\n",
        "print(\"\n",
        "\ud83d\udccb SAMPLE PREDICTIONS:\n",
        "\")\n",
        "\n",
        "# Show first 10 predictions\n",
        "for i, result in enumerate(results[:10]):\n",
        "    print(f\"\n",
        "{'='*60}\")\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"File: {result['file']}\")\n",
        "    print(f\"Citation: {result['citation_id']}\")\n",
        "    print(f\"Gold Span: {result['gold_span'][:100]}...\")\n",
        "    print(f\"Predicted: {result['predicted_span'][:100]}...\")\n",
        "    print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "    match = \"\u2705 MATCH\" if calculate_exact_match(result['gold_span'], result['predicted_span']) else \"\u274c NO MATCH\"\n",
        "    print(f\"Result: {match}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 8: Save Results\n",
        "# ============================================================\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Save to CSV\n",
        "output_file = 'task3_bert_predictions.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"\n",
        "\u2705 Results saved to: {output_file}\")\n",
        "print(f\"\ud83d\udcca Total rows: {len(df)}\")\n",
        "\n",
        "# Show errors if any\n",
        "if errors:\n",
        "    print(f\"\n",
        "\u26a0\ufe0f Errors encountered: {len(errors)}\")\n",
        "    for error in errors[:5]:\n",
        "        print(f\"  - {error['file']}: {error['error']}\")\n",
        "\n",
        "print(\"\n",
        "\" + \"=\"*60)\n",
        "print(\"\u2705 INFERENCE COMPLETE!\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
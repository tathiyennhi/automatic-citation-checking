<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents</title>
				<funder ref="#_wwSpeRc #_ruhxBAy #_NQubE2u">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
							<email>qi.zhang@temple.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Temple University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhƒ≥ia</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Temple University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huitong</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Temple University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
							<email>cornelia@uic.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Longin</forename><forename type="middle">Jan</forename><surname>Latecki</surname></persName>
							<email>latecki@temple.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Temple University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eduard</forename><surname>Dragut</surname></persName>
							<email>edragut@temple.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Temple University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SciER: An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">827DADBD58A02B6850F459EF9EE8292C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2025-10-07T06:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scientific information extraction (SciIE) is critical for converting unstructured knowledge from scholarly articles into structured data (entities and relations). Several datasets have been proposed for training and validating SciIE models. However, due to the high complexity and cost of annotating scientific texts, those datasets restrict their annotations to specific parts of paper, such as abstracts, resulting in the loss of diverse entity mentions and relations in context. In this paper, we release a new entity and relation extraction dataset for entities related to datasets, methods, and tasks in scientific articles. Our dataset contains 106 manually annotated full-text scientific publications with over 24k entities and 12k relations. To capture the intricate use and interactions among entities in full texts, our dataset contains a finegrained tag set for relations. Additionally, we provide an out-of-distribution test set to offer a more realistic evaluation. We conduct comprehensive experiments, including state-of-the-art supervised models and our proposed LLM baselines, and highlight the challenges presented by our dataset, encouraging the development of innovative models to further the field of SciIE.1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scientific Information Extraction (SciIE) is a core topic of scientific literature mining <ref type="bibr" target="#b22">(Luan et al., 2017;</ref><ref type="bibr" target="#b11">Groth et al., 2018;</ref><ref type="bibr" target="#b32">Sadat and Caragea, 2022;</ref><ref type="bibr" target="#b28">Park and Caragea, 2023;</ref><ref type="bibr">Pan et al., 2024a)</ref>. It typically includes scientific named entity extraction (SciNER) and scientific relation extraction (SciRE), and plays a critical role in downstream applications, including scientific knowledge graph construction <ref type="bibr" target="#b40">(Wang et al., 2021;</ref><ref type="bibr" target="#b10">Gautam et al., 2023)</ref>, data searching <ref type="bibr" target="#b36">(Viswanathan et al., 2023)</ref>, academic question answering <ref type="bibr" target="#b6">(Dasigi et al., 2021)</ref>, and method recommendation <ref type="bibr" target="#b21">(Luan et al., 2018)</ref> language models (LLMs) like Galactica <ref type="bibr" target="#b35">(Taylor et al., 2022)</ref> enable several practical applications such as citations suggestion, scientific question answering (QA), and scientific code generation <ref type="bibr" target="#b17">(Li et al., 2023)</ref>. However, their generated content is frequency-biased, often exhibits overconfidence, and lacks factual basis <ref type="bibr">(Xu et al., 2023)</ref>. SciIE, integrated with suitable retrieval, and QA systems can mitigate those issues and enhance model effectiveness in downstream tasks <ref type="bibr" target="#b34">(Shu et al., 2022;</ref><ref type="bibr">Xu et al., 2023)</ref>. SciIE faces unique challenges compared to general domain IE. First, data annotation for SciIE is highly dependent on expert annotators, resulting in a scarcity of high-quality labeled datasets. Second, SciIE needs to handle more complex text, which evolves constantly with novel terminology, unlike general domain IE. For instance, SciIE faces more severe temporal and conceptual shifts <ref type="bibr">(Zhang et al., 2019;</ref><ref type="bibr" target="#b37">Viswanathan et al., 2021;</ref><ref type="bibr">Zaporojets et al., 2022;</ref><ref type="bibr" target="#b5">Chen et al., 2022</ref><ref type="bibr" target="#b4">Chen et al., , 2024;;</ref><ref type="bibr" target="#b29">Pham et al., 2023)</ref>, whereas fundamental entities and relationships in general IE tend to remain more static over time compared to those in the scientific literature.</p><p>Existing SciIE datasets and benchmarks that support both SciNER and SciRE are limited to extracting information from specific parts of papers, such as particular paragraphs <ref type="bibr" target="#b0">(Augenstein et al., 2017)</ref> or abstracts <ref type="bibr" target="#b8">(G√°bor et al., 2018;</ref><ref type="bibr" target="#b21">Luan et al., 2018)</ref>. However, scientific entities like datasets, methods, and tasks entities, are distributed throughout the entire text of papers. Sentences in the body of a paper exhibit diverse linguistic styles and ways to mention entities <ref type="bibr" target="#b17">(Li et al., 2023)</ref> and semantics <ref type="bibr" target="#b15">(Jain et al., 2020)</ref>, which allows the extraction of more fine-grained and precise relation types. For example, abstracts do not say that method X is trained on dataset Y, but experimental sections give such details. Therefore, focusing on specific parts of scientific articles is likely to miss important information. Several datasets <ref type="bibr">(Pan et al., 2024b</ref><ref type="bibr" target="#b27">(Pan et al., , 2023;;</ref><ref type="bibr" target="#b24">Otto et al., 2023;</ref><ref type="bibr" target="#b15">Jain et al., 2020)</ref> attempt to create SciIE benchmarks with full-text annotation, but they ignore the SciRE task.</p><p>In this paper, we present SciER, an entity and relation extraction dataset for identifying dataset, method, and task entities in scientific documents as well as the relations between them. Our dataset is large, with 24K entities and 12k relations from 106 scientific articles, enabling the evaluation and development of SciIE models. These documents are taken from the publications included in Papers with Code (PwC)2, covering artificial intelligence (AI) topics, such as natural language processing (NLP), machine learning (ML), computer vision (CV), and AI for Science (AI4Science). Figure <ref type="figure">1</ref> shows an annotated sentence from our dataset, which gives the entities, their types, i.e., METHOD and TASK, respectively, and the relation between them USED-FOR. Our dataset can be used to evaluate NER and RE as separate tasks, but it can also support the evaluation of end-to-end entity and relation extraction (ERE) from scientific publications <ref type="bibr" target="#b21">(Luan et al., 2018;</ref><ref type="bibr">Ye et al., 2022)</ref>. The table in Figure <ref type="figure">1</ref> describes those settings. For example, in NER the input is a sentence and the output is the set of entities in the sentence. In RE, the input is the sentence along with the entities and the output is the relation between those entities. Finally, in ERE the triplet &lt;subject, relation, object&gt; is the expected output from a sentence.</p><p>We address the limitations of existing datasets by annotating entire scientific papers for both entity and their relations. This is a much harder task 2https://paperswithcode.com/ compared to annotating abstracts. Furthermore, comparing with existing datasets <ref type="bibr" target="#b0">(Augenstein et al., 2017;</ref><ref type="bibr" target="#b8">G√°bor et al., 2018;</ref><ref type="bibr" target="#b21">Luan et al., 2018)</ref>, we provide more fine-grained relation types to describe the interactions between datasets, methods, and tasks. For example, we use TRAINED-WITH and EVALUATED-WITH to describe the interactions between methods and datasets. These relation types need to be extracted from the body of a paper, and are not supported by previous datasets. ¬ß3.3 gives a detailed comparison between our dataset and existing ones. Finally, to evaluate the model's robustness to temporal and conceptual shifts in the SciIE, we set in-distributed (ID) and out-of-distribution (OOD) test sets. The documents in the OOD set were all published after the training documents and feature entirely different topics. We conduct evaluation experiments by employing three stateof-the-art supervised methods and LLMs-based in-context learning (ICL) methods and provide analysis. Specifically, for LLMs-based methods, we tested both pipeline and joint approaches, optimizing the prompts through retrieval-based ICL, tag-based entity extraction, and the incorporation of annotation guidelines. The experimental results show that for LLMs, pipeline modeling, which splits the ERE task into two sub-tasks of NER and RE, outperformas joint extraction. In the challenging ERE task, the best supervised method achieves an F1 score of 61.10%, while the best LLM method achieves an F1 score of 41.22%.</p><p>Our contributions can be summarized as follows:</p><p>‚Ä¢ We provide a manually annotated dataset consisting of 106 full-text scientific publications, containing over 24k entities and 12k relations. Our dataset is significantly larger than previous datasets that support both SciNER and SciRE tasks. ‚Ä¢ We introduce a fine-grained tag set designed for scientific relation extraction, customized to reflect the use and interaction of machine learning datasets, methods, and tasks entities in scientific publications. ‚Ä¢ We conducted experiments on LLMs baselines using both pipeline and joint approaches. We optimized the prompt through retrieval-based ICL, tag-based entity extraction, and the incorporation of annotation guidelines. We also provided a comparative analysis between LLMs methods and three state-of-the-art supervised baselines, highlighting the key challenges. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many datasets for SciNER have been proposed. <ref type="bibr" target="#b12">(Heddes et al., 2021)</ref> and DMDD <ref type="bibr" target="#b27">(Pan et al., 2023)</ref> are two datasets for dataset mention detection. The <ref type="bibr" target="#b12">(Heddes et al., 2021)</ref> dataset comprises 6000 annotated sentences selected based on the occurrence of dataset related word patterns from four major AI conference publications. DMDD is annotated on the full text and comprises 31219 scientific articles automatically annotated with distant supervision <ref type="bibr">(Zhang et al., 2018)</ref>. TDMSci <ref type="bibr" target="#b13">(Hou et al., 2021)</ref> supports three types of entities: TASK, DATASET, and METHOD. It has 2000 sentences extracted from NLP papers. SciREX <ref type="bibr" target="#b15">(Jain et al., 2020)</ref> offers comprehensive coverage with 438 full text annotated documents and supports four entity types: TASK, DATASET, METHOD, and METRIC. SciREX does not annotate relations between pairs of those entity types. <ref type="bibr" target="#b24">(Otto et al., 2023)</ref> manually annotates 100 documents for fine-grained SciNER by defining 10 different entity types in 3 categories: MLModel related, Dataset related and miscellaneous. SciDMT <ref type="bibr">(Pan et al., 2024b)</ref> uses the PwC as knowledge created a very large scale dataset for DATA, METHOD, and TASK. SciDMT includes 48 thousand scientific articles with over 1.8 million weakly annotated mention annotations in their main corpus. However, given the inherent complexity of the NER task, employing weak labels may cause models to overfit on noisy data, thereby substantially impacting their performance <ref type="bibr" target="#b19">(Liu et al., 2021;</ref><ref type="bibr" target="#b2">Bhowmick et al., 2022</ref><ref type="bibr" target="#b3">Bhowmick et al., , 2023))</ref>.</p><p>Although there has been growing interest in research on developing methods and datasets for SciIE, very few datasets support both NER and RE tasks for scientific text. An overview of existing SciIE benchmarks that support both SciNER and SciRE is shown in Table <ref type="table">1</ref>. <ref type="bibr">SEMEVAL-2017 TASK 10 (SemEval 17)</ref>  <ref type="bibr" target="#b0">(Augenstein et al., 2017)</ref> includes 500 paragraphs from open-access journals and supports three types of entities: TASK, METHOD, and MATERIAL and two relation types: HYPONYM-OF and SYNONYM-OF. SEMEVAL-2018 TASK 7 (Se-mEval 18) <ref type="bibr" target="#b8">(G√°bor et al., 2018)</ref> has been proposed for predicting six types of relations between entities. All sentences in SemEval 18 are from the abstracts of NLP papers and have only entity spans (i.e., without annotation of entity types). SciERC <ref type="bibr" target="#b21">(Luan et al., 2018)</ref> contains 500 scientific abstracts with the annotations for scientific entities, their relations, and coreference clusters. SciERC defines six types of entities and seven types of relations. However, these three datasets are limited on annotating abstracts or pre-selected paragraphs. Thus, a significant number of sentences that contain more diverse entity mention forms and semantics are lost.</p><p>Compared to those resources, our dataset contains 106 scientific publications with minute manual annotations. The dataset has nine relation types, allowing for more nuanced relations between entities. The scale of our dataset, which contains more than 24k entities and over 12k relations, which is significantly larger than previous datasets, except for those that are created with distant supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SciER</head><p>In this section, we detail the curation of our dataset, including data collection process in ¬ß3.1, the data annotation process in ¬ß3.2, and present the final dataset statistics and comparisons in ¬ß3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection and Processing</head><p>Our dataset includes 106 documents from two sources. ‚ù∂ One hundred of these documents come from the SciDMT validation set (SciDMT-E). These documents are from the PwC website and we use the corresponding PDF parsed version released by the S2ORC <ref type="bibr">(Lo et al., 2020)</ref>. These papers cover different machine learning topics and have publication dates prior to 2022. We re-check the entity annotations from SciDMT-E3 and then add relation annotations. ‚ù∑ We selected additional six papers from top AI conferences as an out-ofdistribution (OOD) test set. To simulate a more realistic application scenario, we chose these six papers published in 2023-2024, four of which focus on AI4Science topics not included in the first 100 documents. For these six OOD test documents, we first collected their PDF files and then used Grobid <ref type="bibr">(GRO, 2008</ref><ref type="bibr">(GRO, -2024) )</ref> for parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Annotation</head><p>Annotation Scheme For the entity annotation, we use the SciDMT annotation scheme, which defined three types of entities: DATASET, METHOD, and TASK. To maintain consistency with the PwC website database, we only annotate the factual entities, unlike previous works <ref type="bibr" target="#b21">(Luan et al., 2018;</ref><ref type="bibr" target="#b24">Otto et al., 2023)</ref> which annotate both factual and nonfactual entities. For example, the "CoNLL03" and "SNLI" are factual entities, but the "a high-coverage sense-annotated corpus" is not a factual entity.</p><p>For the relation annotation, we define nine finegrained tag set to establish interaction relationships between datasets, methods, and tasks entities in scientific documents. They are EVALUATED-WITH, COMPARE-WITH, SUBCLASS-OF, BENCHMARK-FOR, TRAINED-WITH, USED-FOR, SUBTASK-OF, PART-OF, and SYNONYM-OF. Directionality is taken into account except for the two symmetric relation types (SYNONYM-OF and COMPARE-WITH). We provide our semantic relation typology and corresponding examples in Table <ref type="table" target="#tab_2">2</ref>. Specifically, compared to previous datasets <ref type="bibr" target="#b0">(Augenstein et al., 2017;</ref><ref type="bibr" target="#b21">Luan et al., 2018;</ref><ref type="bibr" target="#b8">G√°bor et al., 2018)</ref>, we employ more specific relation types for identical entity types and extend usage relations among different types of entities in a more granular manner. For example, we use SUBTASK-OF and SUBCLASS-OF to describe the hierarchical relations between tasks and methods, respectively. This can provide better interpretability and allows for direct usage in practical applications such as building taxonomies. Additionally, we use TRAINED-WITH and EVALUATED-WITH to describe the more precise interactions between methods and datasets. We provide more detailed definitions of the labels for entities and relations in our annotation guidelines in Appendix E. Annotation Strategy We have five annotators with backgrounds in computer science and machine learning. We conduct the annotation using INCEp-TION4 platform. All annotators had annotation training before starting to annotate on assigned documents. For the 100 documents from SciDMT-E, we asked annotators to first re-check the SciNER annotation before proceeding to the SciRE annotation. For the six OOD documents, annotators need to annotate both SciNER and SciRE from scratch. Human Agreement One annotator leads the entire annotation process and annotates all the documents in the dataset and each document is also 4https://inception-project.github.io/ annotated by at least two other annotators. For the first 100 documents, the kappa score <ref type="bibr" target="#b7">(Davies and Fleiss, 1982)</ref> for entity annotation is 94.2%, relation annotation is 70.8%; for the six OOD documents, the kappa score for entity annotation is 74.1%, relation annotation is 73.8%. The almost perfect agreement of entity annotation on the first 100 documents is because we derive the original annotation from SciDMT-E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset Statistics and Comparison</head><p>After the annotation process, our dataset contains over 24k entities and 12k relations, with each document averaging about 114 relations. As shown in Table <ref type="table">1</ref>, our dataset is significantly larger than previous datasets supporting both entity and relation extraction task. Specifically, for the widely used SciERC dataset, when we only consider Dataset, Method, and Task entities, it contains only about 1.5k entity and 1.5k relation annotations, where more details are provided in Appendix A.2. We randomly split the first 100 documents into train, development, and ID test sets, containing 80, 10, and 10 documents, respectively. We used six OOD documents as the OOD test set. Appendix A.3 lists the number of samples for each relation type in each set of our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we provide the details of evaluation experiments of both state-of-the-art supervised baselines and LLMs-based baselines on the proposed dataset. We first formally define the problem of end-to-end relation extraction in ¬ß4.1, then describe the supervised methods in ¬ß4.2 and the LLMs-based methods in ¬ß4.3. Finally, we present our implementation details in ¬ß4.4 and evaluation settings in ¬ß4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Definition</head><p>We aim our dataset as a means to train and evaluate SciIE models. Formally, the input document is denoted as ùê∑, which contains a sequence of paragraphs ùëÉ = {ùëù 1 , ùëù 2 , ..., ùëù ùëõ }. Each paragraph ùëù is composed of a sequence of sentences {ùë† 1 , ùë† 2 , ..., ùë† ùëõ } and each sentence is composed of a sequence of words {ùë§ 1 , ùë§ 2 , ..., ùë§ ùëõ }. Formally, the problem of end-to-end relation extraction can be decomposed into two sub-tasks: Named Entity Recognition Let E denote a set of pre-defined entity types. The NER task is to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Supervised Baselines</head><p>We apply three supervised methods: ‚ù∂ PURE (Zhong and Chen, 2021) utilizes two independent encoders to perform pipeline extraction. The outputs of entity encoder are fed into the relation encoder to facilitate end-to-end relation extraction. This method emphasizes the significance of unique representations for entities and relations, the early integration of entity information, and leveraging global context to improve performance. ‚ù∑ PL-Marker (Ye et al., 2022) introduces a novel span representation technique that augments the outputs of pre-trained encoders to perform pipeline extraction. It leverages two specialized packing strategies-neighborhood-oriented for identifying entity boundaries, and subject-oriented for classifying complex span pairs-which helps understand the interrelations between spans. ‚ù∏ HGERE <ref type="bibr">(Yan et al., 2023)</ref> proposes a joint ERE method by incorporating a high-recall pruner to reduce error propagation and by employing a hypergraph neural network to model complex interactions among entities and relations. This approach has led to significant performance improvements, establishing new state-of-the-art results in the joint ERE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">LLMs-based Baselines</head><p>LLMs via in-context learning (ICL) represents a significant advancement in NLP <ref type="bibr" target="#b31">(Qin et al., 2023)</ref>.</p><p>To comprehensively evaluate the LLMs' capability on SciIE, we employ LLMs with zero-shot and few-shot settings to perform both pipeline extraction and joint ERE. Several studies suggest that choosing few-shot in-context examples for each test example dynamically instead of using a fixed set of in-context examples yields strong improvements for ICL <ref type="bibr" target="#b16">(Jimenez Gutierrez et al., 2022;</ref><ref type="bibr" target="#b18">Liu et al., 2022)</ref>. In our experiments, we follow this setting by employing a retriever to find top similar samples from training set as in-context examples. We will first detail the prompt template construction to formalize the NER, RE, and joint ERE as a language generation task <ref type="bibr" target="#b16">(Jimenez Gutierrez et al., 2022)</ref>. Then we will introduce the specific settings and efforts to improve the prompt for each task.</p><p>We construct an unique prompt for each given test example, which is fed to the LLM. Each prompt consists of the following components: Instruction ùêº The task instruction ùêº provides the LLMs with a basic description of the task the LLM needs to perform and in what format it should output the results. Demonstrations ùê∑ The demonstrations are retrieved from the training set for as in-context examples to help the model better understand the task. Specifically, we will employ a retriever to compute the sentence similarity score and acquire the most similar ùëò demonstrations (ùë• ùëñ , ùë¶ ùëñ ) to build ùê∑. Test Input ùë• ùë°ùëíùë†ùë° Following the same format as demonstrations, we offer the test input ùë• ùë°ùëíùë†ùë° , and LLM is expected to generate the corresponding output result ùë¶ ùë°ùëíùë†ùë° .</p><p>In summary, LLMs-based few-shot in-context learning (ICL) for each task can be formulated as:  When performing zero-shot ICL, ùê∑ will be removed from the prompt. Our LLMs-based baseline framework is shown in Figure <ref type="figure" target="#fig_0">2</ref>. Existing work indicates that for information extraction tasks, LLMs require clearer instructions to improve the performance <ref type="bibr" target="#b31">(Qin et al., 2023;</ref><ref type="bibr" target="#b14">Hu et al., 2024;</ref><ref type="bibr" target="#b33">Sainz et al., 2023;</ref><ref type="bibr" target="#b16">Jimenez Gutierrez et al., 2022)</ref>. Therefore, we use annotation guidelines to optimize our prompts. Specifically, for each task, we include two additional instruction components: ‚ù∂ label definitions and ‚ù∑ annotation notes. For label definition, we provide definitions of all entities for the NER task, and definitions of all relations for the RE task. For the Joint ERE task, which requires the model to perform both NER and RE simultaneously, we provide definitions of both entities and relations. For annotation notes, we derive suitable instructions from the human annotation guidelines (see Appendix E) for each task and provide them to the LLMs. We believe that introducing entity and relation definitions and annotation notes offers comprehensive and unambiguous descriptions of the target extraction information.</p><formula xml:id="formula_0">ùëÉ(ùë¶ ùë°ùëíùë†ùë° |ùêº, ùê∑, ùë• ùë°ùëíùë†ùë° ) (1)</formula><p>In terms of formatting the NER annotation in prompt, we present it as HTML span tag. This is because <ref type="bibr" target="#b38">(Wadhwa et al., 2023;</ref><ref type="bibr" target="#b14">Hu et al., 2024)</ref> demonstrated that when using LLMs for information extraction, the generated results might have the same meaning as in the input text but differs in surface form. For example, the entity "CNNs" in the input sentence might be generated as "CNN". To mitigate this error in NER, we instruct the LLMs use HTML span tags to mark all entities in the input sentence to extract the entity spans and use the class attribute to determine the entity types. For example, the entity "CNNs" in the input text will be marked as "&lt;span class="Method"&gt;CNNs&lt;/span&gt;". We pro-vide the complete prompt used in our experiments in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation Details</head><p>For the supervised methods, we use the scibertscivocab-uncased <ref type="bibr" target="#b1">(Beltagy et al., 2019)</ref> as encoder. For the LLMs-based methods, we test the GPT-3.5-Turbo Llama3-70b, and Qwen2-72b as the LLM. For few-shot ICL setting, we retrieve 30 demonstrations for each task, and we use the SimCSE <ref type="bibr" target="#b9">(Gao et al., 2021)</ref> as the retriever. For consistent comparison, all experiments are conducted at the sentence-level. Appendix B has additional implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation Settings</head><p>To evaluate the pipeline extraction and joint ERE, we compute the performance for each subtask, including NER, end-to-end RE (using NER results for relation extraction), and RE (relation extraction with given gold standard entities). For NER, we conduct span-level evaluation, where both the entity boundary and entity type need to be correctly extracted. For the end-to-end RE, similar to (Zhong and <ref type="bibr">Chen, 2021;</ref><ref type="bibr">Ye et al., 2022;</ref><ref type="bibr">Yan et al., 2023)</ref>, we report two evaluation metrics: ‚ù∂ Boundaries evaluation <ref type="bibr">(Rel)</ref>, which requires the model to correctly predict the boundaries of the subject entity and the object entity, as well as the entity relation; ‚ù∑ Strict evaluation (Rel+), which further requires the model to predict the entity types based on the requirements of the boundary prediction. For the RE, given any pair of subject and object entity, the model needs to determine whether a pre-defined relation exists. If a relation does exist, the model must predict the corresponding type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>Table 3 reports the experimental results on ID test set and OOD test set. As described in ¬ß4.5, for the pipeline extraction methods, we present additional RE results when gold standard entities are given. Supervised Baselines We observe that HGERE achieves the best performance on both ID and OOD test sets in NER, Rel, and Rel+, demonstrating the robustness of this current SOTA method. When comparing the results of ID and OOD, we find that all methods show performance drop on the OOD test set for NER, Rel, and Rel+. This is because OOD test provides more challenging and realistic validation scenarios, which require the models to extract information from newly published papers containing new entities. We also observe that the decline in NER scores is more significant, especially for PURE and PL-Marker, whose performance dropped by nearly 10 F1 points. This indicates that extracting unseen entities is more challenging for supervised models compared to relation extraction, which is further supported by the slight decline in RE performance for PURE and PL-Marker in OOD compared to ID. We provide a qualitative example in Appendix C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLMs-based Baselines</head><p>From the results of both zero-shot and few-shot setting, we have the following observations: ‚ù∂ Qwen2-72b exhibits the best overall performance than GPT-3.5-turbo and Llama3-70b in both zero-shot and few-shot settings (except the NER task). ‚ù∑ Pipeline extraction outperforms joint ERE in both zero-shot and few-shot settings. Surprisingly, for both Llama3-70b and Qwen2-72b, pipeline extraction shows a significant improvement over joint ERE. We observed that the NER performance in the pipeline extraction is significantly better than in the joint ERE. This indicates that performing LLMs for this endto-end relation extraction task by decomposing it into seperate NER and RE processes yields better results than joint extraction. ‚ù∏ For LLMs-based baselines, the performance of ID does not always outperform OOD and such pattern is very different from supervised baselines. We believe this is due to the extensive training of LLMs on large-scale data. Specifically, for the RE, even though few-shot settings provide similar demonstrations of test data, the ID results are still worse than OOD. However, for the NER, Rel, and Rel+ tasks under few-shot settings, the performance on ID tends to be better than on OOD. Additionally, compared to OOD, the overall performance improvement on ID after using few-shot settings is generally greater than on OOD. This is because, the demonstrations provided to the LLMs are more similar to the ID data.</p><p>Previous works <ref type="bibr" target="#b39">(Wan et al., 2023;</ref><ref type="bibr" target="#b16">Jimenez Gutierrez et al., 2022;</ref><ref type="bibr" target="#b23">Ma et al., 2023)</ref> showed that information extraction tasks are very challenging for LLMs compared to supervised methods. However, for NER, we found that with appropriate prompt settings, LLMs can be a competent NER model, as reaching an F1 score of 61.69 in zero-shot setting, comparing to the best. This suggests that incorporating LLMs into the NER dataset creation process is a feasible solution to reduce human labor. LLMs perform worse on RE tasks. This is because the test samples for RE tasks contain a large number of NULL labels (see C.2), and large language models have a strong tendency to classify the NULL into predefined types, which has also been confirmed by recent works <ref type="bibr" target="#b16">(Jimenez Gutierrez et al., 2022;</ref><ref type="bibr" target="#b39">Wan et al., 2023)</ref>. Our experiments show that for end-to-end relation extraction (Rel and Rel+), including the current state-of-the-art (SOTA) models and LLMs-based baselines, there is still significant room for improvement in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>To validate the effectiveness of the annotation guideline-enhanced prompt design used in LLMbased baselines, we conducted an ablation study using the Llama3-70b model in a few-shot setting. Specifically, for all tasks, we removed the additional instructions derived from the annotation guidelines, retaining only the basic task description in the instruction ùêº. For the NER task, we further removed the requirement of using HTML span tags, allowing the model to directly generate all entities from the input text rather than tagging the input text. Figure <ref type="figure" target="#fig_1">3</ref> presents the results of our ablation study. The results indicate that incorporating label definitions and comprehensive annotation task guidelines significantly improve the model's performance across all tasks. Additionally, for NER, the use of HTML span tags further enhances performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Train Size Experiment</head><p>Annotating datasets for information extraction within specific domains presents certain challenges. Comparing to partial text, such as sentence and abstracts, full-text annotation further exacerbates the difficulties for annotation. In the training stage,  the number of fully annotated documents plays a crucial role, as documents with fewer annotations have a significant cost advantage. We conducted an experiment aimed at assessing the performance of different scientific information extraction tasks across different numbers of training documents.</p><p>Figure <ref type="figure" target="#fig_2">4</ref> shows the performance trends of the training pipeline extraction model PL-Marker for NER, end-to-end RE (Rel and Rel+), and RE. We observe that NER shows a relatively slowed-down improvement as the dataset size increases, suggesting that while it benefits from more data, it experiences diminishing returns when the amount of data becomes large. In contrast, both end-to-end </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce SciER, a dataset for entity and relation extraction in scientific documents, specifically focusing on datasets, methods, and task entities.</p><p>To address the limitations of existing datasets, we annotate entire scientific papers for both entities and relations, resulting in a large-scale dataset comprising 106 full-text scientific publications from various AI topics, containing over 24,000 entities and 12,000 relations. Additionally, we introduce a fine-grained relation set to describe the interactions between datasets, methods, and tasks. To evaluate the model's robustness to emporal and conceptual shifts in the SciIE, we also set an OOD test set. We conduct comprehensive evaluation experiments, including supervised state-of-the-art (SOTA) models and LLM-based ICL baselines, to highlight the challenges in this task. Specifically, for LLM-based methods, we tested both pipeline and joint approaches, optimizing the prompts through retrieval-based ICL, tag-based entity extraction, and the incorporation of annotation guidelines. The experimental results of LLMs-based methods show that: ‚ù∂ For the ERE task, pipeline modeling, which decomposes the task into NER and RE sub-tasks, significantly outperforms joint modeling; ‚ù∑ Although LLM-based approaches require less labeled data, there remains a performance gap compared to supervised methods. For future work, we aim to further optimize prompts to enhance the performance of LLMs in Scientific Information Extraction (SciIE) and domain-specific IE tasks. Additionally, a LLM-in-the-loop data annotation system to reduce the high costs of creating domain-specific IE datasets is feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Despite our diligent efforts, developing a gold standard dataset for entity and relation extraction using a fine-grained and comprehensive relation tag set focused on machine learning datasets, methods, and tasks remains a nontrivial undertaking. This leads to the following limitations associated with the creation of our corpus. Our dataset only supports three entity types: DATASET, METHOD, and TASK. Incorporating more diverse entity types would be more beneficial for the development of SciIE. Additionally, many scientific entities are nested, which we have not included. We also observed that parsing documents from PDF format contains some errors, which increases the difficulty of document processing and cause some of our sentences contain errors. Finally, we believe that further evluation experiments can be conducted, such as optimizing the ICL baselines for LLMs. However, due to space constraints, we will consider these as future work.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 2021 Conference of the North American Chapter of the</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Comparison with SciERC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 SciER Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Implementation Details B.1 Supervised Baselines</head><p>We followed the hyperparameter settings recommended in the PURE, PL-Maker, and HGERE papers respectively. All experiments were conducted using two NVIDIA A100 80GB GPUs for training. All reported experimental results represent the average of five runs, each with a different random seed.</p><p>Hyperparameter GPT-3.5-Turbo Llama3-70b Qwen2-72b Engine gpt-3.5-turbo-0125 Llama3-70b-instruct Qwen2-72b-instruct Temperature 0.3 0.3 0.3 Max_tokens 256 256 256 Top_p 0.9 0.9 0.9</p><p>Table <ref type="table">8</ref>: Hyperparamters of GPT-3.5-turbo, Llama3-70b and Qwen2-72b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 LLM-based Baselines</head><p>The hyperparameters of GPT-3.5-turbo, Llama3-70b, and Qwen2-72b are presented in the Table <ref type="table">8</ref>. The used version of SimCSE is sup-simcse-roberta-large5. To ensure fairness in the comparison, we kept the inference hyperparameters consistent for both models. For the GPT-3.5-turbo experiments, due to cost considerations, we sampled 200 sentences from each test set for testing, conducted the tests three times, and then averaged the results. The total cost of GPT-3.5-turbo experiments are 50.25 dollars.</p><p>For the Llama3-70b and Qwen2-72b, we used two NVIDIA A100 80GB GPUs for inference. We tested on all samples in each test set, conducted the tests five times, and then averaged the results. Due to the input length limitation of Llama3-70b 5https://huggingface.co/princeton-nlp/ sup-simcse-roberta-large and the lengths of our prompt templates, we set the number of demonstrations for each task as 30, which is also recommended by recent GPT-3 based relation extraction work <ref type="bibr" target="#b39">(Wan et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More analysis C.1 Qualitative Example</head><p>Table <ref type="table" target="#tab_14">9</ref> shows one OOD test example for different models. We observe that both PL-Marker and HGERE fail on this example due to the NER results. PL-Marker ignores the TASK "therapeutic molecular generation", and HGERE predicts the wrong span. But if we provide the gold standard entities to PL-Marker, i.e., the PL-Marker (RE) . It predict correctly. All LLMs-based baselines perform well on this example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Relation Extraction Statistic</head><p>We present the proportion of NULL categories in the RE task in the table 10. We found that the proportion exceeds 60%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Prompt Design</head><p>In this section, we provide the details of annotation guideline-enhanced prompt designs for each task. We list the few-shot version of NER, RE, and Joint ERE. To save the space, we only keep provide 1 demonstration for each task. In our experiments, we use 30 demonstrations. All the zero-shot version are just removed the demonstrations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-Shot NER</head><p>### Task: Generate an HTML version of an input text, marking up specific entities related to machine learning and artificial intelligence. The entities to be identified are: 'Dataset', 'Task', and 'Method'. Use HTML &lt;span&gt; tags to highlight these entities. Each &lt;span&gt; should have a class attribute indicating the type of the entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>### Entity Definitions:</head><p>-'Task': A task in machine learning refers to the specific problem or type of problem that a ML/AI model/method is designed to solve. Tasks can be broad, like classification, regression, or clustering, or they can be very specific, such as Pedestrian Detection, Autonomous Driving, Sentiment Analysis, Named Entity Recognition and Relation Extraction...</p><p>-'Method': A method entity refers to the approach, algorithm, or technique used to solve a specific task/problem. Methods encompass the computational algorithms, model architectures, and the training procedures that are employed to make predictions or decisions based on data. For example, Convolutional Neural Networks, Dropout, data augmentation, recurrent neural networks... -'Dataset': A realistic collection of data that is used for training, validating, or testing the algorithms. These datasets can consist of various forms of data such as text, images, videos, or structured data. For example, MNIST, COCO, AGNews, IMDb...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>### Entity Markup Guide:</head><p>-Use &lt;span class="Task"&gt; to denote a Task entity.</p><p>-Use &lt;span class="Method"&gt; to denote a Method entity.</p><p>-Use &lt;span class="Dataset"&gt; to denote a Dataset entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>### Other Notes:</head><p>-Generics cannot be used independently to refer to any specific entities, e.g., 'This task', 'the dataset', and 'a public corpus' are not entities.</p><p>-The determiners should not be part of an entity span. For example, given span 'the SQuAD v1.1 dataset', where the determiner 'the' should be excluded the entity span.</p><p>-If both the full name and the abbreviation are present in the sentence, annotate the abbreviation and its corresponding full name separately. For instance, '20-newsgroup ( 20NG )', the annoation should be '&lt;span class="Dataset"&gt;20-newsgroup&lt;/span&gt; ( &lt;span class="Dataset"&gt;20NG&lt;/span&gt; )'.</p><p>-If one entity with exact same span text appears many times within a sentence, all span text should be marked up.</p><p>-If one sentence without any entities appear, do not mark up any span text.</p><p>-Only annotate "factual, content-bearing" entities. Task, dataset, and method entities normally have specific names and their meanings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Figure <ref type="figure">5</ref> shows the process undertaken by GxVAEs for therapeutic molecular generation .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USED-FOR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PL-Marker</head><p>Figure <ref type="figure">5</ref> shows the process undertaken by GxVAEs for therapeutic molecular generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HGERE</head><p>Figure <ref type="figure">5</ref> shows the process undertaken by GxVAEs for therapeutic molecular generation .</p><p>USED-FOR PL-Marker (RE) Figure <ref type="figure">5</ref> shows the process undertaken by GxVAEs for therapeutic molecular generation .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USED-FOR</head><p>Llama3-70b (joint) Figure <ref type="figure">5</ref> shows the process undertaken by GxVAEs for therapeutic molecular generation .</p><p>USED-FOR GPT-3.5-Turbo(Joint) Figure <ref type="figure">5</ref> shows the process undertaken by GxVAEs for therapeutic molecular generation .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USED-FOR</head><p>Llama3-70b (pipeline) Figure <ref type="figure">5</ref> shows the process undertaken by GxVAEs for therapeutic molecular generation .</p><p>USED-FOR GPT-3.5-Turbo(pipeline) Figure <ref type="figure">5</ref> shows the process undertaken by GxVAEs for therapeutic molecular generation .</p><p>USED-FOR  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>### Relationship Definitions:</head><p>-'Part-Of': This relationship denotes that one method is a component or a part of another method.</p><p>-'SubClass-Of': Specifies that one method is a subclass or a specialized version of another method.</p><p>-'SubTask-Of': Indicates that one task is a subset or a specific aspect of another broader task.</p><p>-'Benchmark-For': Shows that a dataset serves as a standard or benchmark for evaluating the performance of methods on a specific task.</p><p>-'Trained-With': Indicates that a method is trained using a specific dataset.</p><p>-'Evaluated-With': This relationship denotes that a method is evaluated using a specific dataset to test its performance or conduct the experiments.</p><p>-'Synonym-Of': Indicates that two terms or entities are considered to have the same or very similar meaning, such as abbreviation.</p><p>-'Used-For': Shows that one entity is utilized for achieving or performing another entity. For example, one Method is Used-For one Task. This relationship is highly flexible, allowing for generic relationships across diverse entities.</p><p>-'Compare-With': This relationship is used when one entity is compared with another to highlight differences, similarities, or both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>### Notes:</head><p>- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>### Relationship Definitions:</head><p>-'Part-Of': This relationship denotes that one method is a component or a part of another method.</p><p>-'SubClass-Of': Specifies that one method is a subclass or a specialized version of another method.</p><p>-'SubTask-Of': Indicates that one task is a subset or a specific aspect of another broader task.</p><p>-'Benchmark-For': Shows that a dataset serves as a standard or benchmark for evaluating the performance of methods on a specific task.</p><p>-'Trained-With': Indicates that a method is trained using a specific dataset.</p><p>-'Evaluated-With': This relationship denotes that a method is evaluated using a specific dataset to test its performance or conduct the experiments.</p><p>-'Synonym-Of': Indicates that two terms or entities are considered to have the same or very similar meaning, such as abbreviation.</p><p>-'Used-For': Shows that one entity is utilized for achieving or performing another entity. For example, one Method is Used-For one Task. This relationship is highly flexible, allowing for generic relationships across diverse entities.</p><p>-'Compare-With': This relationship is used when one entity is compared with another to highlight differences, similarities, or both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>### Notes:</head><p>- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Annotation Guideline</head><p>This section contains the basic information from our annotation guideline for double-blind review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Annotation Tool</head><p>We use the INCEpTION6 as our annotation platform. Figure <ref type="figure">5</ref> shows our annotation interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Entity Annotation</head><p>Scientific entities in the machine learning (ML) or Artificial intelligence (AI) domains refer to key 6https://github.com/inception-project/ inception concepts or components that are integral to the structure and study of ML/AI papers. We follow the definition of entities/terms and build our annotation guides for NER based on the ACL RD-TEC Annotation Guideline <ref type="bibr" target="#b30">(QasemiZadeh and Schumann, 2016)</ref>, Papers With Code (PwC) and SciDMT <ref type="bibr">(Pan et al., 2024b)</ref>. We are interested in three specific entity types: Dataset, Task, and Method.</p><p>Dataset: A realistic collection of data that is used for training, validating, or testing the algorithms. These datasets can consist of various forms of data such as text, images, videos, or structured data. For example, MNIST, COCO, AGNews, IMDb, etc.</p><p>Task: A task in machine learning refers to the specific problem or type of problem that a ML/AI model is designed to solve. Tasks can be broad, like classification, regression, or clustering, or they can be very specific, such as Pedestrian Detection, Autonomous Driving, Sentiment Analysis, Named Entity Recognition and Relation Extraction.</p><p>Method: A method entity refers to the approach, algorithm, or technique used to solve a specific task/problem. Methods encompass the computational algorithms, model architectures, and the training procedures that are employed to make predictions or decisions based on data. For example, Convolutional Neural Networks (CNNs),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation Notes:</head><p>Considering that annotators may have varying understandings of the annotation details, we have defined a set of rules and notes to standardize the annotation process:</p><p>-Do not annotate generics and determiners. Generics cannot be used independently to refer to any specific entities, e.g., "This task", "the dataset", "a public corpus" etc. The determiners should not be part of an entity span. For example, given span "the SQuAD v1.1 dataset", where the determiner "the" should be excluded the entity span. We refer ignoring.</p><p>-Minimum span principle. Annotators should annotate only the minimum span necessary to represent the original meaning of task/dataset/metric (e.g.: "The", "dataset", "public", 'method', 'technique' are often omitted).</p><p>-Only annotate "factual, content-bearing" entities. Task, dataset, and method entities normally have specific names and their meanings are consistent across different papers. For example, the "CoNLL03", "SNLI" are factual entities.</p><p>-If one entity with exact same span text appears many times within a sentence, all span text should be annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Relation Annotation</head><p>Relation links cannot exceed the sentence boundary. We define 9 types of relations for Dataset, Method, and Task entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Definitions:</head><p>-'Part-Of': This relationship denotes that one method is a component or a part of another method.</p><p>-'SubClass-Of': Specifies that one method is a subclass or a specialized version of another method.</p><p>-'SubTask-Of': Indicates that one task is a subset or a specific aspect of another broader task.</p><p>-'Benchmark-For': Shows that a dataset serves as a standard or benchmark for evaluating the performance of methods on a specific task.</p><p>-'Trained-With': Indicates that a method is trained using a specific dataset.</p><p>-'Evaluated-With': This relationship denotes that a method is evaluated using a specific dataset to test its performance or conduct the experiments.</p><p>-'Synonym-Of': Indicates that two terms or entities are considered to have the same or very similar meaning, such as abbreviation.</p><p>-'Used-For': Shows that one entity is utilized for achieving or performing another entity. For example, one Method is Used-For one Task. This relationship is highly flexible, allowing for generic relationships across diverse entities.</p><p>-'Compare-With': This relationship is used when one entity is compared with another to highlight differences, similarities, or both. Annotation Notes:</p><p>-Do not annotate negative relations. For example, X is not used in Y or X is hard to be applied in Y.</p><p>-Verify that the entities involved in the relation match the prescribed types (e.g., Method-Dataset for Trained-With). Incorrect entity types should not be linked by these specific relations.</p><p>-Annotate a relationship only if there is direct evidence or clear implication in the text. Avoid inferring relationships that are not explicitly mentioned or clearly implied.</p><p>-Ensure consistency in how relationships are annotated across different texts. If uncertain, refer back to the guideline definitions or consult with a supervisor.</p><p>-Do not make assumptions about relationships based on personal knowledge or external information. Rely solely on the information provided in the text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overall architecture of LLM in-context learning (few-shot) baselines for NER, RE and joint Entity and Relation Extraction (ERE) (first). The few-shot prompt templates for NER (second), RE (third), and Joint ERE (fourth). Different colors indicate different prompt design elements: gray for annotation guideline-based task instructions ùêº, blue for retrieved demonstrations ùê∑, orange denotes the test example input ùë• ùë°ùëíùë†ùë° , and the green represents the expected output of test example output, which will be omitted during testing. ùë¶ ùë°ùëíùë†ùë° . Due to space constraints, we shortened the text of our prompts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Ablation study for the effectiveness of using annotation guideline to improve the Instruction ùêº. "NER w/ Tag" denotes the performance gain with addtional HTML tag setting.</figDesc><graphic coords="8,70.86,377.89,218.28,136.83" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance trends of PL-Marker trained on varying number of documents for NER, end-to-end RE (Rel and Rel+) and RE. RE (Rel and Rel+) and RE show better improvements with an increase in the number of training documents. This indicates that relation extraction is more data-sensitive, requiring more nuanced and varied annotation data for optimal performance.</figDesc><graphic coords="8,306.14,377.89,218.28,136.57" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head/><label/><figDesc>. Scientific large 1Dataset and code are publicly available: https:// github.com/edzq/SciER ùëÜ 1 : We train a deep CNN for semantic segmentation.</figDesc><table><row><cell/><cell/><cell>USED-FOR</cell></row><row><cell/><cell>ùê∏ 1 : METHOD</cell><cell>ùê∏ 2 : TASK</cell></row><row><cell>Task</cell><cell>Input</cell><cell>Output</cell></row><row><cell cols="2">NER RE ùëÜ 1 , [ùê∏ 1 , ùê∏ 2 ] ùëÜ 1 ERE ùëÜ 1</cell><cell>ùê∏ 1 , ùê∏ 2 USED-FOR [ùê∏ 1 , USED-FOR, ùê∏ 2 ]</cell></row></table><note><p>Figure 1: Top: An annotation sample of our SciER dataset, illustrating the labeling process and data structure. The sentence ùëÜ 1 contains two annotated spans denoting two entities ùê∏ 1 and ùê∏ 2 , with respective types METHOD and TASK. Bottom: A table detailing the input and output of the three tasks supported by our SciER dataset, including Named Entity Recognition (NER), Relation Extraction (RE), and Entity and Relation Extraction (ERE).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Semantic relation typology for DATASET, METHOD, and TASK entities. identify all entity mentions from the input sentence ùë† = {ùë§ 1 , ùë§ 2 , ..., ùë§ ùëõ }. For each identified entity, we need to give its span ùëí ùëñ = {ùë§ ùëô , ..., ùë§ ùëü }, where ùëô and ùëü represent the left and right word indices of the span, and classify its entity type ùë° ‚àà E.</figDesc><table><row><cell>Relation Type</cell><cell>Explanation</cell></row></table><note><p><p>EVALUATED-WITH Methods are evaluated by datasets</p>We use COCO to evaluate ConerNet-Lite and compare it wither other detectors. EVALUATED-WITH COMPARE-WITH Entities are linked by comparison relation MAC ...outperforms all tested RANSAC-fashion estimators , such as SAC-COT ... COMPARE-WITH SUBCLASS-OF SUBCLASS-OF One method is a specialized class of another BENCHMARK-FOR Datasets are used to evaluate tasks FlyingChairs is a synthetic dataset designed for training CNNs to estimate optical flow . ..is critical for dense prediction tasks such as object detection ... SUBTASK-OF PART-OF Entities are in a part-whole relation Adding attention to our deep learning-based network translated to... PART-OF SYNONYM-OF Entities have same or very similar meanings ...to improve Generative Adversarial Network ( GAN ) for ... SYNONYM-OF Relation Extraction Let R denote a set of predefined relation types. The task is to predict the relation type ùëü ‚àà R for every pair of entities (ùëí ùëñ , ùëí ùëó ), if one exists, and ùëü = {NULL} otherwise. Since end-to-end relation extraction comprises two subtasks, this task is typically addressed using ‚ù∂ joint entity and relation extraction (ERE) or ‚ù∑ pipeline extraction, i.e., performing the NER task first and then using the NER results for RE.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>85 62.32 61.10 - 81.32 61.31 58.32</head><label/><figDesc>Zhong and Chen, 2021) 81.60 53.27 52.67 73.99 71.99 50.44 49.46 73.63 PL-Marker (Ye et al., 2022) 83.31 60.06 59.24 77.11 73.93 59.02 56.68 76.83 HGERE (Yan et al., 2023) 86.</figDesc><table><row><cell>ID Test Rel+ Supervised Baselines Rel RE PURE (-OOD Test Methods NER NER Rel Rel+ RE Zero-Shot LLMs-based Baselines GPT3.5-Turbo (Joint) 34.76 11.38 10.34 -37.48 10.95 9.97 -GPT3.5-Turbo (Pipeline) 51.19 13.57 13.57 35.48 37.73 12.06 11.34 40.74 Llama3-70b (Joint) 48.87 17.31 17.01 -44.28 17.12 16.63 -Llama3-70b (Pipeline) 61.69 22.28 21.71 37.35 53.09 27.87 25.57 53.87 Qwen2-72b (Joint) 42.15 16.27 14.99 -40.47 15.54 14.31 -Qwen2-72b (Pipeline) 58.57 25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>.76 25.76 53.50 56.43 31.25 28.13 55.37</head><label/><figDesc/><table><row><cell>GPT3.5-Turbo (Joint) GPT3.5-Turbo (Pipeline) Llama3-70b (Joint) Llama3-70b (Pipeline) Qwen2-72b (Joint) Qwen2-72b (Pipeline)</cell><cell>Few-Shot LLMs-based Baselines 62.36 23.71 23.49 -66.27 27.27 24.94 43.26 55.82 22.37 21.49 44.12 51.12 20.12 20.12 -63.23 29.21 29.16 -53.12 20.06 19.93 -76.02 37.55 36.74 56.06 63.98 31.33 29.64 62.71 63.73 35.84 34.87 -49.21 33.17 33.17 -71.44 41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>.51 41.22 60.21 61.72 39.12 37.13 63.93Table 3 :</head><label>3</label><figDesc/><table/><note><p>Test F1 scores of different baselines on our proposed dataset. "Joint" denotes joint ERE. "Pipeline" refers to performing NER and RE separately. "Rel" and "Rel+" denote the results of end-to-end relation extraction under boundaries evaluation and strict evaluation, respectively. "RE" indicates performing relation extraction with given gold standard entities, applicable only to pipeline extraction methods.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head/><label/><figDesc>Association for Computational Linguistics: Human Language Technologies: Demonstrations, pages 66-77, Online. Association for Computational Linguistics.</figDesc><table><row><cell>A More statistics</cell></row><row><cell>A.1 Re-annotating documents from SciDMT</cell></row><row><cell>Silei Xu, Shicheng Liu, Theo Culhane, Elizaveta Pert-seva, Meng-Hsi Wu, Sina Semnani, and Monica Lam. 2023. Fine-tuned LLMs know more, hallucinate less with few-shot sequence-to-sequence semantic parsing over Wikidata. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Process-ing, pages 5778-5791, Singapore. Association for Computational Linguistics. Zhaohui Yan, Songlin Yang, Wei Liu, and Kewei Tu. 2023. Joint entity and relation extraction with span pruning and hypergraph neural networks. In Proceed-Table 4 presents the details of the entities annotation workload of the 100 documents from SciDMT. Specifically, the 100 documents from SciDMT-E original contains 21281 entity annotations. After our re-annotation process, we compare against the previous SciDMT-E entity annotation, we find that we keep 15989 correctly annotated entities, and remove 709 wrongly annotated entities, fixed 4583 entities and add 2651 new entities. Finally, for this 100 publications we derive from SciDMT contains 23223 entity annotations. Totally, we revived 7234 entities. ings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7512-7526, Singapore. Association for Computational Linguis-tics. #Initial #Correct # Removed #Fixed # Added # Final 21281 15989 709 4583 2651 23223</cell></row><row><cell>Deming Ye, Yankai Lin, Peng Li, and Maosong Sun. 2022. Packed levitated marker for entity and relation extraction. In Proceedings of the 60th Annual Meet-ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4904-4917, Dublin, Ireland. Association for Computational Linguistics.</cell></row><row><cell>Klim Zaporojets, Lucie-Aim√©e Kaffee, Johannes Deleu, Thomas Demeester, Chris Develder, and Isabelle Augenstein. 2022. Tempel: Linking dynamically evolving and newly emerging entities. Advances in Neural Information Processing Systems, 35:1850-1866.</cell></row><row><cell>Shanshan Zhang, Lihong He, Eduard Dragut, and Slobo-dan Vucetic. 2019. How to invest my time: Lessons from human-in-the-loop entity extraction. In Pro-ceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining, pages 2305-2313.</cell></row><row><cell>Shanshan Zhang, Lihong He, Slobodan Vucetic, and Ed-uard Dragut. 2018. Regular expression guided entity mention mining from noisy web data. In Proceed-ings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1991-2000, Brussels, Belgium. Association for Computational Linguistics.</cell></row><row><cell>Zexuan Zhong and Danqi Chen. 2021. A frustratingly easy approach for entity and relation extraction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-tional Linguistics: Human Language Technologies, pages 50-61, Online. Association for Computational Linguistics.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>The details of our entity annotations efforts for the first 100 documents.</figDesc><table/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 and</head><label>5</label><figDesc>Table 6 show the label statistics of SciERC when only keep the DATASET, METHOD, and TASK entities. We can find that, though SciERC annotated 500 abstract, there are only 1575 entities and 1575 relations related to DATASET, METHOD, and TASK.</figDesc><table><row><cell>Relation type FEATURE-OF CONJUNCTION USED-FOR COMPARE HYPONYM-OF PART-OF EVALUATION-FOR 69 # 28 292 876 78 154 78 Total 1575</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>The relation types distribution of datasets (material), methods, and tasks in SciERC.</figDesc><table><row><cell>Dataset Dataset Method Task Total SciERC 561 1592 997 1575 SciER 3942 15881 4695 24518</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>The entity distribution of datasets (material), methods, and tasks in SciERC and SciER.</figDesc><table/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head/><label/><figDesc>Table 7 provide the label distribution of the train, development, ID test and OOD test of our proposed SciER.</figDesc><table><row><cell>Rel./Ent. Type DATASET DATASET TASK Total PART-OF USED-FOR EVALUATED-WITH 863 Train Dev ID Test OOD Test Total 11424 1549 1890 1018 15881 3220 269 370 83 3942 3397 416 688 194 4695 18041 2234 2948 1295 24518 1865 214 304 111 2494 2398 343 546 167 3454 78 131 49 1121 SYNONYM-OF 880 76 170 89 1215 COMPARE-WITH 875 175 114 54 1218 SUBCLASS-OF 697 114 176 73 1060 BENCHMARK-FOR 551 64 85 28 728 SUBTASK-OF 210 31 65 9 315 TRAINED-WITH 404 37 35 2 478 Total 8743 1132 1626 582 12083</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>The label distribution of our SciER.</figDesc><table/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Test results of one OOD test example with PL-Marker, HGERE, Llama3-70b (joint), GPT-3.5-Turbo (joint), Llama3-70b (pipeline), GPT-3.5-Turbo (pipeline). The PL-Marker (RE) means using PL-Marker to predict the relation with given two entities.</figDesc><table><row><cell>ID test set OOD test set Dev Train</cell><cell># relation # NULL 1626 4715 582 1109 1132 2053 8743 20923</cell><cell>Tot, 6341 1691 3185 29666</cell><cell>NULL (%) 74.46% 65.58% 64.46% 70.53%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Statistics of datasets for relation extraction. "NULL" means the given subject and object pairs do not have relation.</figDesc><table><row><cell/><cell>apply them .</cell></row><row><cell/><cell>### Input: Specifically , we investigate</cell></row><row><cell/><cell>the attention and feature extraction</cell></row><row><cell/><cell>mechanisms of state -of -the -art</cell></row><row><cell/><cell>recurrent neural networks and self -</cell></row><row><cell/><cell>attentive architectures for sentiment</cell></row><row><cell/><cell>analysis , entailment and machine</cell></row><row><cell/><cell>translation under adversarial attacks .</cell></row><row><cell>are consistent across different papers. For example, the "CoNLL03", "SNLI" are</cell><cell>### Output:</cell></row><row><cell>factual entities. -Minimum span principle. Annotators</cell><cell>Few-Shot RE</cell></row><row><cell>should annotate only the minimum span</cell><cell>### Task: Based on the given sentence,</cell></row><row><cell>necessary to represent the original</cell><cell>and subject entity and object entity</cell></row><row><cell>meaning of task/dataset/metric (e.g.:</cell><cell>from the sentence, answer the questions</cell></row><row><cell>"The", "dataset", "public", 'method',</cell><cell>to determine the relationship between</cell></row><row><cell>'technique' are often omitted).</cell><cell>them. The potential relations are:</cell></row><row><cell>### Examples: Input: In particular we briefly introduce the principal concepts behind deep Convolutional Neural Networks ( CNNs ) , describe the architectures used in our analysis and the algorithms</cell><cell>['Part-Of', 'SubClass-Of', 'SubTask-Of', 'Benchmark-For', 'Trained-With', 'Evaluated-With', 'Synonym-Of', 'Used-For' , 'Compare-With']. Answer 'NULL' to indicate that there is no relationship between the entities.</cell></row><row><cell>adopted to train and apply them .</cell><cell/></row><row><cell>Output: In particular we briefly</cell><cell/></row><row><cell>introduce the principal concepts behind</cell><cell/></row><row><cell>deep &lt;span class="Method"&gt;Convolutional</cell><cell/></row><row><cell>Neural Networks&lt;/span&gt; ( &lt;span</cell><cell/></row><row><cell>class="Method"&gt;CNNs&lt;/span&gt; ) , describe</cell><cell/></row><row><cell>the architectures used in our analysis</cell><cell/></row><row><cell>and the algorithms adopted to train and</cell><cell/></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>3We provide details of our re-checking workload on SciDMT-E in the Appendix A.1.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">National Science Foundation</rs> awards <rs type="grantNumber">III-2107213</rs>, <rs type="grantNumber">III-2107518</rs>, and <rs type="grantNumber">ITE-2333789</rs>. We also thank <rs type="person">Saiyun Dong</rs> and <rs type="person">Faezeh Rajabi Kouchi</rs> at <rs type="affiliation">Temple University</rs>, and <rs type="person">Seyedeh Fatemeh Ahmadi</rs> at <rs type="institution">UIC</rs> for their valuable contributions to our project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wwSpeRc">
					<idno type="grant-number">III-2107213</idno>
				</org>
				<org type="funding" xml:id="_ruhxBAy">
					<idno type="grant-number">III-2107518</idno>
				</org>
				<org type="funding" xml:id="_NQubE2u">
					<idno type="grant-number">ITE-2333789</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Statement</head><p>The data included in our newly proposed dataset includes a subset of the data collected and freely published by <ref type="bibr">(Pan et al., 2024b)</ref> within the SciDMT project. All the other data are public from scientific documents. We release dataset for scientific information extraction tasks. There are no risks in our work.</p></div>			</div>
			<div type="references">

				</div>
		</back>
	</text>
</TEI>

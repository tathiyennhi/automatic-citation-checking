Scientific Information Extraction (SciIE) is a core topic of scientific literature mining  (Luan et al., 2017; Groth et al., 2018; Sadat and Caragea, 2022; Park and Caragea, 2023; Pan et al., 2024a) . It typically includes scientific named entity extraction (SciNER) and scientific relation extraction (SciRE), and plays a critical role in downstream applications, including scientific knowledge graph construction  (Wang et al., 2021; Gautam et al., 2023) , data searching  (Viswanathan et al., 2023) , academic question answering  (Dasigi et al., 2021) , and method recommendation  (Luan et al., 2018)  language models (LLMs) like Galactica  (Taylor et al., 2022)  enable several practical applications such as citations suggestion, scientific question answering (QA), and scientific code generation  (Li et al., 2023) . However, their generated content is frequency-biased, often exhibits overconfidence, and lacks factual basis  (Xu et al., 2023) . SciIE, integrated with suitable retrieval, and QA systems can mitigate those issues and enhance model effectiveness in downstream tasks  (Shu et al., 2022; Xu et al., 2023) . SciIE faces unique challenges compared to general domain IE. First, data annotation for SciIE is highly dependent on expert annotators, resulting in a scarcity of high-quality labeled datasets. Second, SciIE needs to handle more complex text, which evolves constantly with novel terminology, unlike general domain IE. For instance, SciIE faces more severe temporal and conceptual shifts  (Zhang et al., 2019; Viswanathan et al., 2021; Zaporojets et al., 2022; Chen et al., 2022 Chen et al., , 2024;; Pham et al., 2023) , whereas fundamental entities and relationships in general IE tend to remain more static over time compared to those in the scientific literature.

Existing SciIE datasets and benchmarks that support both SciNER and SciRE are limited to extracting information from specific parts of papers, such as particular paragraphs  (Augenstein et al., 2017)  or abstracts  (G√°bor et al., 2018; Luan et al., 2018) . However, scientific entities like datasets, methods, and tasks entities, are distributed throughout the entire text of papers. Sentences in the body of a paper exhibit diverse linguistic styles and ways to mention entities  (Li et al., 2023)  and semantics  (Jain et al., 2020) , which allows the extraction of more fine-grained and precise relation types. For example, abstracts do not say that method X is trained on dataset Y, but experimental sections give such details. Therefore, focusing on specific parts of scientific articles is likely to miss important information. Several datasets  (Pan et al., 2024b (Pan et al., , 2023;; Otto et al., 2023; Jain et al., 2020)  attempt to create SciIE benchmarks with full-text annotation, but they ignore the SciRE task.

In this paper, we present SciER, an entity and relation extraction dataset for identifying dataset, method, and task entities in scientific documents as well as the relations between them. Our dataset is large, with 24K entities and 12k relations from 106 scientific articles, enabling the evaluation and development of SciIE models. These documents are taken from the publications included in Papers with Code (PwC)2, covering artificial intelligence (AI) topics, such as natural language processing (NLP), machine learning (ML), computer vision (CV), and AI for Science (AI4Science). Figure  1  shows an annotated sentence from our dataset, which gives the entities, their types, i.e., METHOD and TASK, respectively, and the relation between them USED-FOR. Our dataset can be used to evaluate NER and RE as separate tasks, but it can also support the evaluation of end-to-end entity and relation extraction (ERE) from scientific publications  (Luan et al., 2018; Ye et al., 2022) . The table in Figure  1  describes those settings. For example, in NER the input is a sentence and the output is the set of entities in the sentence. In RE, the input is the sentence along with the entities and the output is the relation between those entities. Finally, in ERE the triplet <subject, relation, object> is the expected output from a sentence.

We address the limitations of existing datasets by annotating entire scientific papers for both entity and their relations. This is a much harder task 2https://paperswithcode.com/ compared to annotating abstracts. Furthermore, comparing with existing datasets  (Augenstein et al., 2017; G√°bor et al., 2018; Luan et al., 2018) , we provide more fine-grained relation types to describe the interactions between datasets, methods, and tasks. For example, we use TRAINED-WITH and EVALUATED-WITH to describe the interactions between methods and datasets. These relation types need to be extracted from the body of a paper, and are not supported by previous datasets. ¬ß3.3 gives a detailed comparison between our dataset and existing ones. Finally, to evaluate the model's robustness to temporal and conceptual shifts in the SciIE, we set in-distributed (ID) and out-of-distribution (OOD) test sets. The documents in the OOD set were all published after the training documents and feature entirely different topics. We conduct evaluation experiments by employing three stateof-the-art supervised methods and LLMs-based in-context learning (ICL) methods and provide analysis. Specifically, for LLMs-based methods, we tested both pipeline and joint approaches, optimizing the prompts through retrieval-based ICL, tag-based entity extraction, and the incorporation of annotation guidelines. The experimental results show that for LLMs, pipeline modeling, which splits the ERE task into two sub-tasks of NER and RE, outperformas joint extraction. In the challenging ERE task, the best supervised method achieves an F1 score of 61.10%, while the best LLM method achieves an F1 score of 41.22%.

Our contributions can be summarized as follows:

‚Ä¢ We provide a manually annotated dataset consisting of 106 full-text scientific publications, containing over 24k entities and 12k relations. Our dataset is significantly larger than previous datasets that support both SciNER and SciRE tasks. ‚Ä¢ We introduce a fine-grained tag set designed for scientific relation extraction, customized to reflect the use and interaction of machine learning datasets, methods, and tasks entities in scientific publications. ‚Ä¢ We conducted experiments on LLMs baselines using both pipeline and joint approaches. We optimized the prompt through retrieval-based ICL, tag-based entity extraction, and the incorporation of annotation guidelines. We also provided a comparative analysis between LLMs methods and three state-of-the-art supervised baselines, highlighting the key challenges.

Many datasets for SciNER have been proposed.  (Heddes et al., 2021)  and DMDD  (Pan et al., 2023)  are two datasets for dataset mention detection. The  (Heddes et al., 2021)  dataset comprises 6000 annotated sentences selected based on the occurrence of dataset related word patterns from four major AI conference publications. DMDD is annotated on the full text and comprises 31219 scientific articles automatically annotated with distant supervision  (Zhang et al., 2018) . TDMSci  (Hou et al., 2021)  supports three types of entities: TASK, DATASET, and METHOD. It has 2000 sentences extracted from NLP papers. SciREX  (Jain et al., 2020)  offers comprehensive coverage with 438 full text annotated documents and supports four entity types: TASK, DATASET, METHOD, and METRIC. SciREX does not annotate relations between pairs of those entity types.  (Otto et al., 2023)  manually annotates 100 documents for fine-grained SciNER by defining 10 different entity types in 3 categories: MLModel related, Dataset related and miscellaneous. SciDMT  (Pan et al., 2024b)  uses the PwC as knowledge created a very large scale dataset for DATA, METHOD, and TASK. SciDMT includes 48 thousand scientific articles with over 1.8 million weakly annotated mention annotations in their main corpus. However, given the inherent complexity of the NER task, employing weak labels may cause models to overfit on noisy data, thereby substantially impacting their performance  (Liu et al., 2021; Bhowmick et al., 2022 Bhowmick et al., , 2023)) .

Although there has been growing interest in research on developing methods and datasets for SciIE, very few datasets support both NER and RE tasks for scientific text. An overview of existing SciIE benchmarks that support both SciNER and SciRE is shown in Table  1 .  SEMEVAL-2017 TASK 10 (SemEval 17)    (Augenstein et al., 2017)  includes 500 paragraphs from open-access journals and supports three types of entities: TASK, METHOD, and MATERIAL and two relation types: HYPONYM-OF and SYNONYM-OF. SEMEVAL-2018 TASK 7 (Se-mEval 18)  (G√°bor et al., 2018)  has been proposed for predicting six types of relations between entities. All sentences in SemEval 18 are from the abstracts of NLP papers and have only entity spans (i.e., without annotation of entity types). SciERC  (Luan et al., 2018)  contains 500 scientific abstracts with the annotations for scientific entities, their relations, and coreference clusters. SciERC defines six types of entities and seven types of relations. However, these three datasets are limited on annotating abstracts or pre-selected paragraphs. Thus, a significant number of sentences that contain more diverse entity mention forms and semantics are lost.

Compared to those resources, our dataset contains 106 scientific publications with minute manual annotations. The dataset has nine relation types, allowing for more nuanced relations between entities. The scale of our dataset, which contains more than 24k entities and over 12k relations, which is significantly larger than previous datasets, except for those that are created with distant supervision.

In this section, we detail the curation of our dataset, including data collection process in ¬ß3.1, the data annotation process in ¬ß3.2, and present the final dataset statistics and comparisons in ¬ß3.3.

Our dataset includes 106 documents from two sources. ‚ù∂ One hundred of these documents come from the SciDMT validation set (SciDMT-E). These documents are from the PwC website and we use the corresponding PDF parsed version released by the S2ORC  (Lo et al., 2020) . These papers cover different machine learning topics and have publication dates prior to 2022. We re-check the entity annotations from SciDMT-E3 and then add relation annotations. ‚ù∑ We selected additional six papers from top AI conferences as an out-ofdistribution (OOD) test set. To simulate a more realistic application scenario, we chose these six papers published in 2023-2024, four of which focus on AI4Science topics not included in the first 100 documents. For these six OOD test documents, we first collected their PDF files and then used Grobid  (GRO, 2008 (GRO, -2024) )  for parsing.

Annotation Scheme For the entity annotation, we use the SciDMT annotation scheme, which defined three types of entities: DATASET, METHOD, and TASK. To maintain consistency with the PwC website database, we only annotate the factual entities, unlike previous works  (Luan et al., 2018; Otto et al., 2023)  which annotate both factual and nonfactual entities. For example, the "CoNLL03" and "SNLI" are factual entities, but the "a high-coverage sense-annotated corpus" is not a factual entity.

For the relation annotation, we define nine finegrained tag set to establish interaction relationships between datasets, methods, and tasks entities in scientific documents. They are EVALUATED-WITH, COMPARE-WITH, SUBCLASS-OF, BENCHMARK-FOR, TRAINED-WITH, USED-FOR, SUBTASK-OF, PART-OF, and SYNONYM-OF. Directionality is taken into account except for the two symmetric relation types (SYNONYM-OF and COMPARE-WITH). We provide our semantic relation typology and corresponding examples in Table  2 . Specifically, compared to previous datasets  (Augenstein et al., 2017; Luan et al., 2018; G√°bor et al., 2018) , we employ more specific relation types for identical entity types and extend usage relations among different types of entities in a more granular manner. For example, we use SUBTASK-OF and SUBCLASS-OF to describe the hierarchical relations between tasks and methods, respectively. This can provide better interpretability and allows for direct usage in practical applications such as building taxonomies. Additionally, we use TRAINED-WITH and EVALUATED-WITH to describe the more precise interactions between methods and datasets. We provide more detailed definitions of the labels for entities and relations in our annotation guidelines in Appendix E. Annotation Strategy We have five annotators with backgrounds in computer science and machine learning. We conduct the annotation using INCEp-TION4 platform. All annotators had annotation training before starting to annotate on assigned documents. For the 100 documents from SciDMT-E, we asked annotators to first re-check the SciNER annotation before proceeding to the SciRE annotation. For the six OOD documents, annotators need to annotate both SciNER and SciRE from scratch. Human Agreement One annotator leads the entire annotation process and annotates all the documents in the dataset and each document is also 4https://inception-project.github.io/ annotated by at least two other annotators. For the first 100 documents, the kappa score  (Davies and Fleiss, 1982)  for entity annotation is 94.2%, relation annotation is 70.8%; for the six OOD documents, the kappa score for entity annotation is 74.1%, relation annotation is 73.8%. The almost perfect agreement of entity annotation on the first 100 documents is because we derive the original annotation from SciDMT-E.

After the annotation process, our dataset contains over 24k entities and 12k relations, with each document averaging about 114 relations. As shown in Table  1 , our dataset is significantly larger than previous datasets supporting both entity and relation extraction task. Specifically, for the widely used SciERC dataset, when we only consider Dataset, Method, and Task entities, it contains only about 1.5k entity and 1.5k relation annotations, where more details are provided in Appendix A.2. We randomly split the first 100 documents into train, development, and ID test sets, containing 80, 10, and 10 documents, respectively. We used six OOD documents as the OOD test set. Appendix A.3 lists the number of samples for each relation type in each set of our dataset.

In this section, we provide the details of evaluation experiments of both state-of-the-art supervised baselines and LLMs-based baselines on the proposed dataset. We first formally define the problem of end-to-end relation extraction in ¬ß4.1, then describe the supervised methods in ¬ß4.2 and the LLMs-based methods in ¬ß4.3. Finally, we present our implementation details in ¬ß4.4 and evaluation settings in ¬ß4.5.

We aim our dataset as a means to train and evaluate SciIE models. Formally, the input document is denoted as ùê∑, which contains a sequence of paragraphs ùëÉ = {ùëù 1 , ùëù 2 , ..., ùëù ùëõ }. Each paragraph ùëù is composed of a sequence of sentences {ùë† 1 , ùë† 2 , ..., ùë† ùëõ } and each sentence is composed of a sequence of words {ùë§ 1 , ùë§ 2 , ..., ùë§ ùëõ }. Formally, the problem of end-to-end relation extraction can be decomposed into two sub-tasks: Named Entity Recognition Let E denote a set of pre-defined entity types. The NER task is to

We apply three supervised methods: ‚ù∂ PURE (Zhong and Chen, 2021) utilizes two independent encoders to perform pipeline extraction. The outputs of entity encoder are fed into the relation encoder to facilitate end-to-end relation extraction. This method emphasizes the significance of unique representations for entities and relations, the early integration of entity information, and leveraging global context to improve performance. ‚ù∑ PL-Marker (Ye et al., 2022) introduces a novel span representation technique that augments the outputs of pre-trained encoders to perform pipeline extraction. It leverages two specialized packing strategies-neighborhood-oriented for identifying entity boundaries, and subject-oriented for classifying complex span pairs-which helps understand the interrelations between spans. ‚ù∏ HGERE  (Yan et al., 2023)  proposes a joint ERE method by incorporating a high-recall pruner to reduce error propagation and by employing a hypergraph neural network to model complex interactions among entities and relations. This approach has led to significant performance improvements, establishing new state-of-the-art results in the joint ERE.

LLMs via in-context learning (ICL) represents a significant advancement in NLP  (Qin et al., 2023) .

To comprehensively evaluate the LLMs' capability on SciIE, we employ LLMs with zero-shot and few-shot settings to perform both pipeline extraction and joint ERE. Several studies suggest that choosing few-shot in-context examples for each test example dynamically instead of using a fixed set of in-context examples yields strong improvements for ICL  (Jimenez Gutierrez et al., 2022; Liu et al., 2022) . In our experiments, we follow this setting by employing a retriever to find top similar samples from training set as in-context examples. We will first detail the prompt template construction to formalize the NER, RE, and joint ERE as a language generation task  (Jimenez Gutierrez et al., 2022) . Then we will introduce the specific settings and efforts to improve the prompt for each task.

We construct an unique prompt for each given test example, which is fed to the LLM. Each prompt consists of the following components: Instruction ùêº The task instruction ùêº provides the LLMs with a basic description of the task the LLM needs to perform and in what format it should output the results. Demonstrations ùê∑ The demonstrations are retrieved from the training set for as in-context examples to help the model better understand the task. Specifically, we will employ a retriever to compute the sentence similarity score and acquire the most similar ùëò demonstrations (ùë• ùëñ , ùë¶ ùëñ ) to build ùê∑. Test Input ùë• ùë°ùëíùë†ùë° Following the same format as demonstrations, we offer the test input ùë• ùë°ùëíùë†ùë° , and LLM is expected to generate the corresponding output result ùë¶ ùë°ùëíùë†ùë° .

In summary, LLMs-based few-shot in-context learning (ICL) for each task can be formulated as:  When performing zero-shot ICL, ùê∑ will be removed from the prompt. Our LLMs-based baseline framework is shown in Figure  2 . Existing work indicates that for information extraction tasks, LLMs require clearer instructions to improve the performance  (Qin et al., 2023; Hu et al., 2024; Sainz et al., 2023; Jimenez Gutierrez et al., 2022) . Therefore, we use annotation guidelines to optimize our prompts. Specifically, for each task, we include two additional instruction components: ‚ù∂ label definitions and ‚ù∑ annotation notes. For label definition, we provide definitions of all entities for the NER task, and definitions of all relations for the RE task. For the Joint ERE task, which requires the model to perform both NER and RE simultaneously, we provide definitions of both entities and relations. For annotation notes, we derive suitable instructions from the human annotation guidelines (see Appendix E) for each task and provide them to the LLMs. We believe that introducing entity and relation definitions and annotation notes offers comprehensive and unambiguous descriptions of the target extraction information.

In terms of formatting the NER annotation in prompt, we present it as HTML span tag. This is because  (Wadhwa et al., 2023; Hu et al., 2024)  demonstrated that when using LLMs for information extraction, the generated results might have the same meaning as in the input text but differs in surface form. For example, the entity "CNNs" in the input sentence might be generated as "CNN". To mitigate this error in NER, we instruct the LLMs use HTML span tags to mark all entities in the input sentence to extract the entity spans and use the class attribute to determine the entity types. For example, the entity "CNNs" in the input text will be marked as "<span class="Method">CNNs</span>". We pro-vide the complete prompt used in our experiments in Appendix D.

For the supervised methods, we use the scibertscivocab-uncased  (Beltagy et al., 2019)  as encoder. For the LLMs-based methods, we test the GPT-3.5-Turbo Llama3-70b, and Qwen2-72b as the LLM. For few-shot ICL setting, we retrieve 30 demonstrations for each task, and we use the SimCSE  (Gao et al., 2021)  as the retriever. For consistent comparison, all experiments are conducted at the sentence-level. Appendix B has additional implementation details.

To evaluate the pipeline extraction and joint ERE, we compute the performance for each subtask, including NER, end-to-end RE (using NER results for relation extraction), and RE (relation extraction with given gold standard entities). For NER, we conduct span-level evaluation, where both the entity boundary and entity type need to be correctly extracted. For the end-to-end RE, similar to (Zhong and  Chen, 2021; Ye et al., 2022; Yan et al., 2023) , we report two evaluation metrics: ‚ù∂ Boundaries evaluation  (Rel) , which requires the model to correctly predict the boundaries of the subject entity and the object entity, as well as the entity relation; ‚ù∑ Strict evaluation (Rel+), which further requires the model to predict the entity types based on the requirements of the boundary prediction. For the RE, given any pair of subject and object entity, the model needs to determine whether a pre-defined relation exists. If a relation does exist, the model must predict the corresponding type.

Table 3 reports the experimental results on ID test set and OOD test set. As described in ¬ß4.5, for the pipeline extraction methods, we present additional RE results when gold standard entities are given. Supervised Baselines We observe that HGERE achieves the best performance on both ID and OOD test sets in NER, Rel, and Rel+, demonstrating the robustness of this current SOTA method. When comparing the results of ID and OOD, we find that all methods show performance drop on the OOD test set for NER, Rel, and Rel+. This is because OOD test provides more challenging and realistic validation scenarios, which require the models to extract information from newly published papers containing new entities. We also observe that the decline in NER scores is more significant, especially for PURE and PL-Marker, whose performance dropped by nearly 10 F1 points. This indicates that extracting unseen entities is more challenging for supervised models compared to relation extraction, which is further supported by the slight decline in RE performance for PURE and PL-Marker in OOD compared to ID. We provide a qualitative example in Appendix C.1.

From the results of both zero-shot and few-shot setting, we have the following observations: ‚ù∂ Qwen2-72b exhibits the best overall performance than GPT-3.5-turbo and Llama3-70b in both zero-shot and few-shot settings (except the NER task). ‚ù∑ Pipeline extraction outperforms joint ERE in both zero-shot and few-shot settings. Surprisingly, for both Llama3-70b and Qwen2-72b, pipeline extraction shows a significant improvement over joint ERE. We observed that the NER performance in the pipeline extraction is significantly better than in the joint ERE. This indicates that performing LLMs for this endto-end relation extraction task by decomposing it into seperate NER and RE processes yields better results than joint extraction. ‚ù∏ For LLMs-based baselines, the performance of ID does not always outperform OOD and such pattern is very different from supervised baselines. We believe this is due to the extensive training of LLMs on large-scale data. Specifically, for the RE, even though few-shot settings provide similar demonstrations of test data, the ID results are still worse than OOD. However, for the NER, Rel, and Rel+ tasks under few-shot settings, the performance on ID tends to be better than on OOD. Additionally, compared to OOD, the overall performance improvement on ID after using few-shot settings is generally greater than on OOD. This is because, the demonstrations provided to the LLMs are more similar to the ID data.

Previous works  (Wan et al., 2023; Jimenez Gutierrez et al., 2022; Ma et al., 2023)  showed that information extraction tasks are very challenging for LLMs compared to supervised methods. However, for NER, we found that with appropriate prompt settings, LLMs can be a competent NER model, as reaching an F1 score of 61.69 in zero-shot setting, comparing to the best. This suggests that incorporating LLMs into the NER dataset creation process is a feasible solution to reduce human labor. LLMs perform worse on RE tasks. This is because the test samples for RE tasks contain a large number of NULL labels (see C.2), and large language models have a strong tendency to classify the NULL into predefined types, which has also been confirmed by recent works  (Jimenez Gutierrez et al., 2022; Wan et al., 2023) . Our experiments show that for end-to-end relation extraction (Rel and Rel+), including the current state-of-the-art (SOTA) models and LLMs-based baselines, there is still significant room for improvement in the future.

To validate the effectiveness of the annotation guideline-enhanced prompt design used in LLMbased baselines, we conducted an ablation study using the Llama3-70b model in a few-shot setting. Specifically, for all tasks, we removed the additional instructions derived from the annotation guidelines, retaining only the basic task description in the instruction ùêº. For the NER task, we further removed the requirement of using HTML span tags, allowing the model to directly generate all entities from the input text rather than tagging the input text. Figure  3  presents the results of our ablation study. The results indicate that incorporating label definitions and comprehensive annotation task guidelines significantly improve the model's performance across all tasks. Additionally, for NER, the use of HTML span tags further enhances performance.

Annotating datasets for information extraction within specific domains presents certain challenges. Comparing to partial text, such as sentence and abstracts, full-text annotation further exacerbates the difficulties for annotation. In the training stage,  the number of fully annotated documents plays a crucial role, as documents with fewer annotations have a significant cost advantage. We conducted an experiment aimed at assessing the performance of different scientific information extraction tasks across different numbers of training documents.

Figure  4  shows the performance trends of the training pipeline extraction model PL-Marker for NER, end-to-end RE (Rel and Rel+), and RE. We observe that NER shows a relatively slowed-down improvement as the dataset size increases, suggesting that while it benefits from more data, it experiences diminishing returns when the amount of data becomes large. In contrast, both end-to-end

We introduce SciER, a dataset for entity and relation extraction in scientific documents, specifically focusing on datasets, methods, and task entities.

To address the limitations of existing datasets, we annotate entire scientific papers for both entities and relations, resulting in a large-scale dataset comprising 106 full-text scientific publications from various AI topics, containing over 24,000 entities and 12,000 relations. Additionally, we introduce a fine-grained relation set to describe the interactions between datasets, methods, and tasks. To evaluate the model's robustness to emporal and conceptual shifts in the SciIE, we also set an OOD test set. We conduct comprehensive evaluation experiments, including supervised state-of-the-art (SOTA) models and LLM-based ICL baselines, to highlight the challenges in this task. Specifically, for LLM-based methods, we tested both pipeline and joint approaches, optimizing the prompts through retrieval-based ICL, tag-based entity extraction, and the incorporation of annotation guidelines. The experimental results of LLMs-based methods show that: ‚ù∂ For the ERE task, pipeline modeling, which decomposes the task into NER and RE sub-tasks, significantly outperforms joint modeling; ‚ù∑ Although LLM-based approaches require less labeled data, there remains a performance gap compared to supervised methods. For future work, we aim to further optimize prompts to enhance the performance of LLMs in Scientific Information Extraction (SciIE) and domain-specific IE tasks. Additionally, a LLM-in-the-loop data annotation system to reduce the high costs of creating domain-specific IE datasets is feasible.

Despite our diligent efforts, developing a gold standard dataset for entity and relation extraction using a fine-grained and comprehensive relation tag set focused on machine learning datasets, methods, and tasks remains a nontrivial undertaking. This leads to the following limitations associated with the creation of our corpus. Our dataset only supports three entity types: DATASET, METHOD, and TASK. Incorporating more diverse entity types would be more beneficial for the development of SciIE. Additionally, many scientific entities are nested, which we have not included. We also observed that parsing documents from PDF format contains some errors, which increases the difficulty of document processing and cause some of our sentences contain errors. Finally, we believe that further evluation experiments can be conducted, such as optimizing the ICL baselines for LLMs. However, due to space constraints, we will consider these as future work.

We followed the hyperparameter settings recommended in the PURE, PL-Maker, and HGERE papers respectively. All experiments were conducted using two NVIDIA A100 80GB GPUs for training. All reported experimental results represent the average of five runs, each with a different random seed.

Hyperparameter GPT-3.5-Turbo Llama3-70b Qwen2-72b Engine gpt-3.5-turbo-0125 Llama3-70b-instruct Qwen2-72b-instruct Temperature 0.3 0.3 0.3 Max_tokens 256 256 256 Top_p 0.9 0.9 0.9

Table  8 : Hyperparamters of GPT-3.5-turbo, Llama3-70b and Qwen2-72b.

The hyperparameters of GPT-3.5-turbo, Llama3-70b, and Qwen2-72b are presented in the Table  8 . The used version of SimCSE is sup-simcse-roberta-large5. To ensure fairness in the comparison, we kept the inference hyperparameters consistent for both models. For the GPT-3.5-turbo experiments, due to cost considerations, we sampled 200 sentences from each test set for testing, conducted the tests three times, and then averaged the results. The total cost of GPT-3.5-turbo experiments are 50.25 dollars.

For the Llama3-70b and Qwen2-72b, we used two NVIDIA A100 80GB GPUs for inference. We tested on all samples in each test set, conducted the tests five times, and then averaged the results. Due to the input length limitation of Llama3-70b 5https://huggingface.co/princeton-nlp/ sup-simcse-roberta-large and the lengths of our prompt templates, we set the number of demonstrations for each task as 30, which is also recommended by recent GPT-3 based relation extraction work  (Wan et al., 2023) .

Table  9  shows one OOD test example for different models. We observe that both PL-Marker and HGERE fail on this example due to the NER results. PL-Marker ignores the TASK "therapeutic molecular generation", and HGERE predicts the wrong span. But if we provide the gold standard entities to PL-Marker, i.e., the PL-Marker (RE) . It predict correctly. All LLMs-based baselines perform well on this example.

We present the proportion of NULL categories in the RE task in the table 10. We found that the proportion exceeds 60%.

In this section, we provide the details of annotation guideline-enhanced prompt designs for each task. We list the few-shot version of NER, RE, and Joint ERE. To save the space, we only keep provide 1 demonstration for each task. In our experiments, we use 30 demonstrations. All the zero-shot version are just removed the demonstrations.

### Task: Generate an HTML version of an input text, marking up specific entities related to machine learning and artificial intelligence. The entities to be identified are: 'Dataset', 'Task', and 'Method'. Use HTML <span> tags to highlight these entities. Each <span> should have a class attribute indicating the type of the entity.

-'Task': A task in machine learning refers to the specific problem or type of problem that a ML/AI model/method is designed to solve. Tasks can be broad, like classification, regression, or clustering, or they can be very specific, such as Pedestrian Detection, Autonomous Driving, Sentiment Analysis, Named Entity Recognition and Relation Extraction...

-'Method': A method entity refers to the approach, algorithm, or technique used to solve a specific task/problem. Methods encompass the computational algorithms, model architectures, and the training procedures that are employed to make predictions or decisions based on data. For example, Convolutional Neural Networks, Dropout, data augmentation, recurrent neural networks... -'Dataset': A realistic collection of data that is used for training, validating, or testing the algorithms. These datasets can consist of various forms of data such as text, images, videos, or structured data. For example, MNIST, COCO, AGNews, IMDb...

-Use <span class="Task"> to denote a Task entity.

-Use <span class="Method"> to denote a Method entity.

-Use <span class="Dataset"> to denote a Dataset entity.

-Generics cannot be used independently to refer to any specific entities, e.g., 'This task', 'the dataset', and 'a public corpus' are not entities.

-The determiners should not be part of an entity span. For example, given span 'the SQuAD v1.1 dataset', where the determiner 'the' should be excluded the entity span.

-If both the full name and the abbreviation are present in the sentence, annotate the abbreviation and its corresponding full name separately. For instance, '20-newsgroup ( 20NG )', the annoation should be '<span class="Dataset">20-newsgroup</span> ( <span class="Dataset">20NG</span> )'.

-If one entity with exact same span text appears many times within a sentence, all span text should be marked up.

-If one sentence without any entities appear, do not mark up any span text.

-Only annotate "factual, content-bearing" entities. Task, dataset, and method entities normally have specific names and their meanings

Figure  5  shows the process undertaken by GxVAEs for therapeutic molecular generation .

Figure  5  shows the process undertaken by GxVAEs for therapeutic molecular generation.

Figure  5  shows the process undertaken by GxVAEs for therapeutic molecular generation .

USED-FOR PL-Marker (RE) Figure  5  shows the process undertaken by GxVAEs for therapeutic molecular generation .

Llama3-70b (joint) Figure  5  shows the process undertaken by GxVAEs for therapeutic molecular generation .

USED-FOR GPT-3.5-Turbo(Joint) Figure  5  shows the process undertaken by GxVAEs for therapeutic molecular generation .

Llama3-70b (pipeline) Figure  5  shows the process undertaken by GxVAEs for therapeutic molecular generation .

USED-FOR GPT-3.5-Turbo(pipeline) Figure  5  shows the process undertaken by GxVAEs for therapeutic molecular generation .

USED-FOR

-'Part-Of': This relationship denotes that one method is a component or a part of another method.

-'SubClass-Of': Specifies that one method is a subclass or a specialized version of another method.

-'SubTask-Of': Indicates that one task is a subset or a specific aspect of another broader task.

-'Benchmark-For': Shows that a dataset serves as a standard or benchmark for evaluating the performance of methods on a specific task.

-'Trained-With': Indicates that a method is trained using a specific dataset.

-'Evaluated-With': This relationship denotes that a method is evaluated using a specific dataset to test its performance or conduct the experiments.

-'Synonym-Of': Indicates that two terms or entities are considered to have the same or very similar meaning, such as abbreviation.

-'Used-For': Shows that one entity is utilized for achieving or performing another entity. For example, one Method is Used-For one Task. This relationship is highly flexible, allowing for generic relationships across diverse entities.

-'Compare-With': This relationship is used when one entity is compared with another to highlight differences, similarities, or both.

-

-'Part-Of': This relationship denotes that one method is a component or a part of another method.

-'SubClass-Of': Specifies that one method is a subclass or a specialized version of another method.

-'SubTask-Of': Indicates that one task is a subset or a specific aspect of another broader task.

-'Benchmark-For': Shows that a dataset serves as a standard or benchmark for evaluating the performance of methods on a specific task.

-'Trained-With': Indicates that a method is trained using a specific dataset.

-'Evaluated-With': This relationship denotes that a method is evaluated using a specific dataset to test its performance or conduct the experiments.

-'Synonym-Of': Indicates that two terms or entities are considered to have the same or very similar meaning, such as abbreviation.

-'Used-For': Shows that one entity is utilized for achieving or performing another entity. For example, one Method is Used-For one Task. This relationship is highly flexible, allowing for generic relationships across diverse entities.

-'Compare-With': This relationship is used when one entity is compared with another to highlight differences, similarities, or both.

-

This section contains the basic information from our annotation guideline for double-blind review.

We use the INCEpTION6 as our annotation platform. Figure  5  shows our annotation interface.

Scientific entities in the machine learning (ML) or Artificial intelligence (AI) domains refer to key 6https://github.com/inception-project/ inception concepts or components that are integral to the structure and study of ML/AI papers. We follow the definition of entities/terms and build our annotation guides for NER based on the ACL RD-TEC Annotation Guideline  (QasemiZadeh and Schumann, 2016) , Papers With Code (PwC) and SciDMT  (Pan et al., 2024b) . We are interested in three specific entity types: Dataset, Task, and Method.

Dataset: A realistic collection of data that is used for training, validating, or testing the algorithms. These datasets can consist of various forms of data such as text, images, videos, or structured data. For example, MNIST, COCO, AGNews, IMDb, etc.

Task: A task in machine learning refers to the specific problem or type of problem that a ML/AI model is designed to solve. Tasks can be broad, like classification, regression, or clustering, or they can be very specific, such as Pedestrian Detection, Autonomous Driving, Sentiment Analysis, Named Entity Recognition and Relation Extraction.

Method: A method entity refers to the approach, algorithm, or technique used to solve a specific task/problem. Methods encompass the computational algorithms, model architectures, and the training procedures that are employed to make predictions or decisions based on data. For example, Convolutional Neural Networks (CNNs),

Considering that annotators may have varying understandings of the annotation details, we have defined a set of rules and notes to standardize the annotation process:

-Do not annotate generics and determiners. Generics cannot be used independently to refer to any specific entities, e.g., "This task", "the dataset", "a public corpus" etc. The determiners should not be part of an entity span. For example, given span "the SQuAD v1.1 dataset", where the determiner "the" should be excluded the entity span. We refer ignoring.

-Minimum span principle. Annotators should annotate only the minimum span necessary to represent the original meaning of task/dataset/metric (e.g.: "The", "dataset", "public", 'method', 'technique' are often omitted).

-Only annotate "factual, content-bearing" entities. Task, dataset, and method entities normally have specific names and their meanings are consistent across different papers. For example, the "CoNLL03", "SNLI" are factual entities.

-If one entity with exact same span text appears many times within a sentence, all span text should be annotated.

Relation links cannot exceed the sentence boundary. We define 9 types of relations for Dataset, Method, and Task entities.

-'Part-Of': This relationship denotes that one method is a component or a part of another method.

-'SubClass-Of': Specifies that one method is a subclass or a specialized version of another method.

-'SubTask-Of': Indicates that one task is a subset or a specific aspect of another broader task.

-'Benchmark-For': Shows that a dataset serves as a standard or benchmark for evaluating the performance of methods on a specific task.

-'Trained-With': Indicates that a method is trained using a specific dataset.

-'Evaluated-With': This relationship denotes that a method is evaluated using a specific dataset to test its performance or conduct the experiments.

-'Synonym-Of': Indicates that two terms or entities are considered to have the same or very similar meaning, such as abbreviation.

-'Used-For': Shows that one entity is utilized for achieving or performing another entity. For example, one Method is Used-For one Task. This relationship is highly flexible, allowing for generic relationships across diverse entities.

-'Compare-With': This relationship is used when one entity is compared with another to highlight differences, similarities, or both. Annotation Notes:

-Do not annotate negative relations. For example, X is not used in Y or X is hard to be applied in Y.

-Verify that the entities involved in the relation match the prescribed types (e.g., Method-Dataset for Trained-With). Incorrect entity types should not be linked by these specific relations.

-Annotate a relationship only if there is direct evidence or clear implication in the text. Avoid inferring relationships that are not explicitly mentioned or clearly implied.

-Ensure consistency in how relationships are annotated across different texts. If uncertain, refer back to the guideline definitions or consult with a supervisor.

-Do not make assumptions about relationships based on personal knowledge or external information. Rely solely on the information provided in the text.

Figure 1: Top: An annotation sample of our SciER dataset, illustrating the labeling process and data structure. The sentence ùëÜ 1 contains two annotated spans denoting two entities ùê∏ 1 and ùê∏ 2 , with respective types METHOD and TASK. Bottom: A table detailing the input and output of the three tasks supported by our SciER dataset, including Named Entity Recognition (NER), Relation Extraction (RE), and Entity and Relation Extraction (ERE).

EVALUATED-WITH Methods are evaluated by datasets We use COCO to evaluate ConerNet-Lite and compare it wither other detectors. EVALUATED-WITH COMPARE-WITH Entities are linked by comparison relation MAC ...outperforms all tested RANSAC-fashion estimators , such as SAC-COT ... COMPARE-WITH SUBCLASS-OF SUBCLASS-OF One method is a specialized class of another BENCHMARK-FOR Datasets are used to evaluate tasks FlyingChairs is a synthetic dataset designed for training CNNs to estimate optical flow . ..is critical for dense prediction tasks such as object detection ... SUBTASK-OF PART-OF Entities are in a part-whole relation Adding attention to our deep learning-based network translated to... PART-OF SYNONYM-OF Entities have same or very similar meanings ...to improve Generative Adversarial Network ( GAN ) for ... SYNONYM-OF Relation Extraction Let R denote a set of predefined relation types. The task is to predict the relation type ùëü ‚àà R for every pair of entities (ùëí ùëñ , ùëí ùëó ), if one exists, and ùëü = {NULL} otherwise. Since end-to-end relation extraction comprises two subtasks, this task is typically addressed using ‚ù∂ joint entity and relation extraction (ERE) or ‚ù∑ pipeline extraction, i.e., performing the NER task first and then using the NER results for RE.

EVALUATED-WITH Methods are evaluated by datasets

Test F1 scores of different baselines on our proposed dataset. "Joint" denotes joint ERE. "Pipeline" refers to performing NER and RE separately. "Rel" and "Rel+" denote the results of end-to-end relation extraction under boundaries evaluation and strict evaluation, respectively. "RE" indicates performing relation extraction with given gold standard entities, applicable only to pipeline extraction methods.

3We provide details of our re-checking workload on SciDMT-E in the Appendix A.1.
<?xml version='1.0' encoding='utf-8'?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This Reference Does Not Exist: An Exploration of LLM Citation Accuracy and Relevance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Courtni</forename><surname>Byun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brigham Young University Provo</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Piper</forename><surname>Vasicek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brigham Young University Provo</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Seppi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Brigham Young University Provo</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This Reference Does Not Exist: An Exploration of LLM Citation Accuracy and Relevance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">27E30E240355F6DC87E8BF665100CA50</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-10-18T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Citations are a fundamental and indispensable part of research writing. They provide support and lend credibility to research findings. Recent GPT-fueled interest in large language models (LLMs) has shone a spotlight on the capabilities and limitations of these models when generating relevant citations for a document. Recent work has focused largely on title and author accuracy. We underline this effort and expand on it with a preliminary exploration in relevance of model-recommended citations. We define three citation-recommendation tasks. We also collect and annotate a dataset of modelrecommended citations for those tasks. We find that GPT-4 largely outperforms earlier models on both author and title accuracy in two markedly different CS venues, but may not recommend references that are more relevant than those recommended by the earlier models. The two venues we compare are CHI and EMNLP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Citations are a common feature of research writing. They lend credibility to claims and can help identify gaps in prior research. They can also provide a chain of ideas from prior work to a research task.</p><p>The last year has seen a drastic increase of interest about large language models (LLMs). ChatGPT <ref type="bibr" target="#b25">(OpenAI, 2022)</ref> has opened the eyes of the general public to the potential of LLMs. ChatGPT and its related GPT-X LLMs are being applied to a growing array of tasks <ref type="bibr">(Araoz, 2020;</ref><ref type="bibr" target="#b26">OpenAI, 2023;</ref><ref type="bibr" target="#b5">Byun et al., 2023;</ref><ref type="bibr" target="#b38">Xiao et al., 2023)</ref>.</p><p>One task that has drawn both interest and ire is that of using LLMs to identify citations for a topic. Several recent blog posts and articles have warned of ChatGPT's hallucinated references <ref type="bibr" target="#b36">(Welborn, 2023;</ref><ref type="bibr" target="#b37">Wilkinson, 2023;</ref><ref type="bibr" target="#b24">Neumeister, 2023)</ref>. We build on recent work to assess the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Various citation recommendation systems exist, relying on an array of NLP and information retrieval (IR) approaches. <ref type="bibr" target="#b9">Farber and Jatowt (2020)</ref> offer a thorough survey of automated citation recommendation approaches.</p><p>More recently, use of LLMs has been explored, leading to discussion of the tendency LLMs have to hallucinate output. <ref type="bibr" target="#b7">Day (2023)</ref> offered an early exploration of hallucinated references by ChatGPT. They assessed references output by ChatGPT based on accuracy of journal name, volume, issue and page number and found the model incapable of generating any valid references.</p><p>On the other hand, MW <ref type="bibr">Wagner (2023)</ref> found ChatGPT capable of some accuracy when answering questions about clinical radiological sources.</p><p>A letter of warning from <ref type="bibr" target="#b21">McGowan et al. (2023)</ref> discussed fabricated references from both ChatGPT and Google's Bard <ref type="bibr" target="#b20">(Manyika, 2023)</ref> in psychiatry literature. They found real authors are often included, even when a paper title is fabricated. They also raised the alarm on the possibility of fake references entering into automated indexes. <ref type="bibr" target="#b11">Gravel et al. (2023)</ref> found ChatGPT output in response to medical reference questions was of limited quality, but that references offered by the model were deceptively realistic.</p><p>Orduna-Malea and Cabezas-Clavijo (2023) compared ChatGPT and Bard 2.0 citations in English, Spanish, and Italian. They explored reasons for fabricated citations and steps to address the issue. <ref type="bibr" target="#b32">Taylor et al. (2022)</ref> fine-tuned their own LLM, Galactica, and assessed it on three citation generation tasks. They found LLM accuracy for citation generation appears to improve with scale.</p><p>Finally, <ref type="bibr" target="#b0">Agrawal et al. (2023)</ref> found LLMs tend to hallucinate different authors of fabricated references in multiple independent query sessions, but consistently hallucinate authors in the same session.</p><p>They compared accuracy on GPT text-davinci-003, ChatGPT, and GPT-4.</p><p>Previous work has primarily focused on metrics related to accuracy of information. While understanding accuracy is important, accurate citations that are irrelevant will still be of little use to researchers. In this work we still assess accuracy, but we also offer a preliminary assessment of the relevance of citations identified by three models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We define three citation recommendation tasks, intended to model aspects of academic writing that could be supplemented by use of LLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Models</head><p>We compare performance between three GPT-X models: GPT-3 text-davinci-003 (GPT-3), GPT-3.5-turbo (GPT-3.5), and GPT-4. All model hyperparameters used can be found in Appendix A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tasks</head><p>We define three tasks, each with a unique prompt. The full prompt evolutions and all final prompt designs can be found in Appendices B and C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Abstract→Citations List Task</head><p>This task asks the model generate a list of relevant sources a researcher could explore and incorporate into their paper (target paper). We provide the models with a prompt including a paper title and its accompanying abstract and request the model generate ten relevant citations to be used in the target paper. We request citations in APA format because it is common and having all citations in a consistent format aids in annotating and analysing the data. See Figure <ref type="figure" target="#fig_1">1</ref> for prompt template.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Abstract→Related Works Task</head><p>The goal of this task is to explore how well the models identify relevant citations when also asked to discuss them, without the textual scaffolding of a provided Related Works section. The prompt for this task builds on the prompt for the first task, but replaces the final section with: Write a Related Works section for your paper. Include 10 in-text citations. Also include a list of those citations with each citation in APA format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Discussion→Supported Discussion Task</head><p>The goal of this task is to test model citation recommendation and discussion when some textual scaffolding is provided. The prompt for this task builds  on the prompt for the first task by including the target paper title and abstract in the prompt, but the prompt additionally includes a portion of the results discussion. The final section of the prompt, which follows the discussion, is changed to: Rewrite the Discussion section to include 10 in-text citations. Also include a list of those citations with each citation in APA format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset</head><p>We randomly sampled twenty papers from two toptier, but different venues, CHI (HCI) and EMNLP (NLP). Ten papers were randomly sampled from recent publications of each venue. See Appendix E for the list of papers. The paper title, abstract, and discussion of results were extracted for each paper. For some papers this was taken from the Results section and for others, the Discussion section. Some discussions were too long for the models. For these, we extracted only the first paragraph of each section within the discussion. We also extracted the bibliography from each paper.</p><p>This information was used to fill the prompt templates, which were then input to each model. The output was collected and the citations extracted. While we requested citations in APA, the models sometimes used different formatting. We reformatted each citation to ensure it was in APA. Some model-generated citations lacked titles. These we exclude from our final dataset because we cannot verify whether they are real papers. Our final dataset has 1616 annotated citations.</p><p>We used Google Scholar to check whether each model-recommended citation was for a real paper. Nearly all real papers had an exact match in the first three results of a page, so we restricted our search to the first page of results. <ref type="bibr" target="#b28">Petiska (2023)</ref> found that ChatGPT tends to use Google Scholar citation counts when recommending citations, so relying only on Google Scholar results should be sufficient.</p><p>A citation was marked as fabricated if an exact match was not found in the first page of Google Scholar results. A citation with an exact match was marked as a real paper and the APA citation for the true paper was collected and checked against the citation generated by the model. We automatically compared information in the citations generated by the models against the information collected from the real papers. We collected information for how many citations were fabricated vs real. We also calculated author precision and recall between the authors in a recommended citation and those on real papers. We tested relevance by checking whether a real paper's title was found in the bibliographies of the target papers and whether the authors of the model-generated citations were found in the bibliographies of the target papers.</p><p>While more elaborate metrics for determining citation relevance exist <ref type="bibr" target="#b1">(Belter, 2017;</ref><ref type="bibr" target="#b4">Boyack and Klavans, 2010)</ref>, these often involve creating a network of citations. The overlap between citations is then checked. This includes overlap with the target papers. However, we needed target papers that were excluded from the models' training data, which meant very recent papers that had not been cited yet. This meant we needed a different metric for relevance. We focus on several basic metrics based on the idea that if there is overlap between papers models recommend and papers authors actually use, then those papers and authors that overlap must be relevant. This means true relevance could be higher, but our strict definition should offer a reasonable exploratory view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Accuracy results can be found in Tables <ref type="table" target="#tab_0">1</ref> and <ref type="table">3</ref>, while relevance results can be found in Table <ref type="table" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Accuracy</head><p>Title Accuracy is the percentage of citations recommended by the model that had real paper titles. Author Precision, Author Recall, and Year Accuracy were only calculated for citations of real papers. Year Accuracy was calculated by taking the absolute value of the year a real paper was published, minus the year in the model-recommended citation.</p><p>As seen in Table <ref type="table" target="#tab_0">1</ref>, the models tend to perform better on NLP papers, particularly with respect to paper titles. This is reiterated by the results in Table <ref type="table">3</ref>, where for nearly every model, for every task there appears to be a significant difference between NLP and HCI papers on this metric.</p><p>The distinction is less clear for other metrics. For example both GPT-3 and GPT-3.5 perform better for HCI papers in terms of Author Recall for the Abstract→Citations List task and GPT-4 performs better for HCI papers in terms of Author Precision for the Discussion→Supported Discussion task.</p><p>GPT-4 typically outperforms the other models in terms of accuracy, which is unsurprising given the findings of <ref type="bibr" target="#b32">Taylor et al. (2022)</ref> that LLM citation accuracy improves with model scale. There are, Abstract → Citations List Abstract → Related Works Discussion → Supported Discussion Title Relevance Title Relevance Title Relevance HCI NLP Total HCI NLP Total HCI NLP Total GPT-3 0.17% 22.92% 16.90% GPT-3 -10.71% 10.71% GPT-3 0.12% 22.22% 12.5% GPT-3.5 0.18% 25.00% 17.86% GPT-3.5 0.24% 29.79% 24.19% GPT-3.5 0.25% 33.33% 25.00% GPT-4 0.20% 29.49% 20.45% GPT-4 0.17% 27.71% 17.31% GPT-4 0.08% 19.23% 8.33% Real Author Relevance Real Author Relevance Real Author Relevance HCI NLP Total HCI NLP Total HCI NLP Total GPT-3 4.35% 6.25% 5.63% GPT-3 -3.57% 3.57% GPT-3 6.67% 0.00% 4.17% GPT-3.5 7.14% 5.36% 5.95% GPT-3.5 0.00% 10.64% 8.06% GPT-3.5 0.00% 11.11% 6.25% GPT-4 3.70% 3.85% 3.79% GPT-4 4.11% 6.02% 5.13% GPT-4 1.72% 3.85% 2.38% False Author Relevance False Author Relevance False Author Relevance HCI NLP Total HCI NLP Total HCI NLP Total GPT-3 13.04% 8.33% 9.86% GPT-3 -25.00% 25.00% GPT-3 26.67% 0.00% 16.67% GPT-3.5 7.14% 8.93% 8.33% GPT-3.5 13.33% 17.02% 16.13% GPT-3.5 21.43% 27.78% 25.00% GPT-4 16.67% 25.64% 21.97% GPT-4 28.77% 19.28% 23.72% GPT-4 6.9% 11.54% 8.33% Table 3: Two-sample t-tests for title accuracy on HCI vs NLP papers. Calculated via SciPy and NumPy <ref type="bibr">(Virtanen et al., 2020;</ref><ref type="bibr">Harris et al., 2020)</ref>.</p><p>however, exceptions to this. For example, GPT-3.5 outperforms GPT-4 on Title Accuracy, Author Precision, and Author Recall for the NLP papers on the Discussion→Supported Discussion task.</p><p>The models appear to struggle with the Discussion→Supported Discussion task. This could be due to our poor prompt design for this task. CHI papers typically include a separate Discussion section, while EMNLP papers often include a discussion of results with the Results section. We distinctly asked models to support our Discussion sections. Future research could explore whether changing Discussion to Results in the prompt could yield better results for NLP papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relevance</head><p>Title Relevance reports the percentage of real papers cited in the target paper. Real Author Relevance reports the percentage of authors from a model-recommended citation that were real authors on that paper and who had a paper cited in the target paper. False Author Relevance reports the percentage of authors from a model-recommended citation that were not real authors on that paper, but who had papers cited in the target paper.</p><p>In terms of relevance, we again see better performance for NLP papers in terms of title relevance. The distinction becomes less clear for other metrics. For example, GPT-4 on the Abstract→Related Works task and False Author Relevance. However, there does not appear to be a large difference between models. In multiple instances the older models perform better than GPT-4, for example GPT-3 for the Abstract→Related Works task on the False Author Relevance metric for NLP papers and both GPT-3 and GPT-3.5 on the Discussion→Supported Discussion task on all relevance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We evaluated GPT-3, GPT-3.5, and GPT-4 on three different citation recommendation tasks and compared them across two research disciplines. We found contrasts in terms of relevance and accuracy between those disciplines. This is important because individuals outside of NLP are beginning to use these models in their research. It is important for researchers from other disciplines to recognize these models' limitations for their disciplines.</p><p>Finally, while GPT-4 typically outperforms previous models on accuracy, it does not clearly perform better in terms of relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>While 1616 citations seems like enough for a thorough run of statistical tests, this is not the case. Due to how poorly GPT-3 and GPT-3.5 perform on many of the tasks and how many ways we split the data, several of our sample sizes are slightly under 30, with the smallest being 24. We have run significance tests comparing performance between models and between HCI and NLP papers for other metrics, but considering the small sample sizes of some of the groups, we felt the limited space of this short paper would be best utilized reporting our other results.</p><p>Our largest sample sizes are for the Title Accuracy metric because this included all citations, while the other metrics excluded citations for papers that did not exist. This is why we only report significance results for Title Accuracy between HCI and NLP papers. We exclude our significance results for Title Accuracy between models due to the length limitations of this paper. Previous research has shown a difference between models of different sizes. Our results reiterate those findings.</p><p>We also did not compare accuracy of other citation information, like page numbers, publication venues, and URLs. Preliminary tests showed much worse model performance on these citation features. We chose to focus on the features the models appeared to recreate more accurately. We leave exploration of these other features to future work.</p><p>Additionally, due to the inherently messy nature of text data, some aspects of data collection and curation were done manually. While we did multiple checks at each step of the process to maintain quality, there could still be errors we did not catch.</p><p>We also relied on Google Scholar results to determine veracity of citation titles. It is possible that some of the citations marked as fabricated could be real papers that did not show up on the first page of results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters</head><p>• Temperature: 0.0 • Top P: 1 • Frequency Penalty: 0.5 • Presence Penalty: 0.5 • Maximum Tokens: 2000 We chose a temperature of 0 because, while a temperature of 0 does not guarantee identical output each time, it does increase the likelihood of very similar output. This was the best option available at the time for generating reproducible results. We used 0.5 for both frequency and presence penalties because both GPT-3 and GPT-3.5 are prone to repeating citations when they are set to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Prompt Engineering</head><p>The following are the various prompt evolutions we used before settling on our final prompt designs.</p><p>We went through several iterations of prompt design for each of the three tasks in this paper. The prompt variations were primarily focused around the request portion of the prompt. All prompts included either a CHI or EMNLP paper title and abstract. The Results→Supported Results task prompts also included discussion from the same CHI or EMNLP prompt paper.</p><p>All of the prompts in this subsection follow the GPT-3 design. The main difference between the GPT-3 and newer model prompts was a change to a first person perspective. We did not ultimately include GPT-3 in our results for the Abstract→Related Works and Results→Supported Results tasks because the final prompt design was too long for the GPT-3 limited context length. However, GPT-3 was included and evaluated on earlier variations of prompts for those tasks. We found GPT-3 was virtually incapable of identifying any citations of real papers for the Abstract→Related Works and Results→Supported Results tasks, even for prompt designs short enough to fit the GPT-3 context. We found the models have a tendency to cite older sources, so we next adjusted the prompt to request only recent citations. We updated the prompt to the following, with the changed portion in bold:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Abstract→Citations Prompt Evolution</head><p>You are an [HCI or NLP] researcher working on a paper to submit to <ref type="bibr">[CHI or EMNLP]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The paper you are working on is titled: [PAPER TITLE]</head><p>The abstract for your paper is:</p><formula xml:id="formula_0">[PAPER ABSTRACT]</formula><p>Five relevant papers from the last five years you could cite in your related works sections are:</p><p>We did find the models do often claim to cite recent papers using this prompt, but we also noticed they have a tendency to hallucinate paper publication years as more recent than they actually are. We did not, however, do an official comparison between how these prompt designs impact citation year hallucinations. This would be an interesting item for future research.</p><p>We ultimately decided to request ten, rather than five citations, to hopefully get a large enough sample size to run statistical tests. We also decided to remove the the request for papers from the last five years because it did not appear to have a strong impact on the results. Finally, we added a request for the model to output the citations in APA format. We found that not requesting a specific format often resulted in the models just choosing a format. The format they chose was sometimes not even a standard format and occasionally the format could change throughout the same output. Our final prompt design was:</p><p>You are an [HCI or NLP] researcher working on a paper to submit to [CHI or EMNLP].</p><p>The paper you are working on is titled:</p><formula xml:id="formula_1">[PAPER TITLE]</formula><p>The abstract for your paper is: [PAPER ABSTRACT]</p><p>List 10 relevant papers you could cite in your Related Works section. Write each citation in APA format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Abstract→Related Works Prompt Evolution</head><p>The prompt format for this task is nearly identical to that of the Abstract→Citations task. The main difference is in the final line of the prompt.</p><p>You are an [HCI or NLP] researcher working on a paper to submit to [CHI or EMNLP].</p><p>The paper you are working on is titled:</p><formula xml:id="formula_2">[PAPER TITLE]</formula><p>The abstract for your paper is:</p><formula xml:id="formula_3">[PAPER ABSTRACT]</formula><p>The Related Works section of your paper is:</p><p>Again, we realized the models have a tendency to cite older sources, so we updated the prompt to request recent sources. We also followed the same pattern of changing the design to make specific requests, rather than asking the model to continue with writing a related works section. The changed portion of the prompt is in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>You are an [HCI or NLP] researcher working on a paper to submit to [CHI or EMNLP].</head><p>The paper you are working on is titled:</p><formula xml:id="formula_4">[PAPER TITLE]</formula><p>The abstract for your paper is: [PAPER ABSTRACT] Write the related works section for this paper. Discuss 3 sources. Each source must be from the last five years and must include the paper name.</p><p>We decided to allow the model to include a higher number of sources. We updated the prompt to reflect that. We also wanted enough information about each citation to be able to verify it, so we updated the prompt to request the model to include the paper title and a complete list of authors. The prompt design can be found below. Write the related works section for this paper. Discuss up to 10 sources. Each source must be from the last five years and must include the paper name and full list of authors.</p><p>We wondered if model performance could be impacted by the difference in citation formatting by asking the model to include a full list of authors and paper title. We updated our prompt design to allow the models to use in-text citations as one normally would (author name, year), but we included a request for the models to include a list of used citations after their prose. The final prompt design can be found below: Write the related works section for this paper. Discuss ten sources. Each source must be from the last five years. Include a list of the citations used following your related works section.</p><p>Again, we found that including a request for recent sources had little impact, so we removed that portion of the prompt. We also found it necessary to request APA formatting. Our final prompt design for this task can be found below:</p><p>You are an <ref type="bibr">[HCI or NLP]</ref> researcher working on a paper to submit to [CHI or EMNLP]. The paper you are working on is titled: [PAPER TITLE] The abstract for your paper is: [PAPER ABSTRACT] Write a Related Works section for your paper. Include 10 in-text citations. Also include a list of those citations with each citation in APA format. B.3 Results→Supported Results Prompt Evolution Again, prompts included either a CHI or EMNLP prompt paper title and abstract, but the Results→Supported Results task prompts included discussion from the same CHI or EMNLP prompt paper. Our original prompt design for this task was: You are an [HCI or NLP] researcher working on a paper to submit to [CHI or EMNLP]. The paper you are working on is titled: [PAPER TITLE] The abstract for your paper is: [PAPER ABSTRACT]</p><p>The Discussion section for your paper is:</p><formula xml:id="formula_5">[PAPER DISCUSSION]</formula><p>A revised version of your Discussion section including supporting sources is:</p><p>We updated this prompt design to also request recent sources. Additionally, we decided to change to a specific request, rather than having the model simply continue on. The updated prompt can be found below, with the changes in bold. We modified the prompt to allow the models to include up to ten sources. We also noted that earlier prompt designs led to output following standard intext citation formats, in which only the name of the lead author and publication year were included. We updated the prompt to request the complete list of authors and full paper name. We made this change to make verification of sources possible. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Results→Supported Results</head><p>The final prompt designs provided to each model for this task can be found below. Rewrite the Discussion section to include 10 in-text citations. Also include a list of those citations with each citation in APA format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Abstract→Related Works</head><p>The final prompt designs provided to each model for this task can be found below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Example Citations</head><p>All citations in the following subsections were identified by GPT-X models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 GPT-4 Citations of Real Papers and Correct Authors</head><p>The citations in this section are examples of GPT-4identified citations. The citation titles and authors are correct, though other information in these citations, like year or publisher, may be hallucinated.</p><p>1. Kang, R., Dabbish, L., Fruchter, N., &amp; Kiesler, S. (2015). "My data just goes everywhere: " User mental models of the internet and implications for privacy and security. In Eleventh Symposium On Usable Privacy and Security (SOUPS 2015), pp. 39-52. 2. 10. Wash, R., &amp; Rader, E. (2015). Too much knowledge? Security beliefs and protective behaviors among United States internet users. In Eleventh Symposium On Usable Privacy and Security (SOUPS 2015), pp. 309-325. 3. 1. Aker, J. C., &amp; Mbiti, I. M. (2020). Mobile Phones and Economic Development in Africa. Journal of Economic Perspectives, 34(3), 207-232. D.2 GPT-4 Citations of Real Papers and Incorrect Authors</p><p>The citations in this section are examples of GPT-4-identified citations. The citation titles are correct, though other information in these citations, like year or publisher, may be hallucinated. At least a portion of one author in each citation is hallucinated. In the section citation, the second author</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc>You are an[NLP or HCI]  researcher working on a paper to submit to [EMNLP or CHI]. The paper you are working on is titled: [PAPER TITLE] The abstract for your paper is: [PAPER ABSTRACT] List 10 relevant papers you could cite in your Related Works section. Write each citation in APA format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Prompt template for Abstract→Citations List task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc>You are an [HCI or NLP] researcher working on a paper to submit to [CHI or EMNLP]. The paper you are working on is titled: [PAPER TITLE] The abstract for your paper is: [PAPER ABSTRACT] Five relevant papers you could cite in your related works sections are:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>You are an [HCI or NLP] researcher working on a paper to submit to [CHI or EMNLP]. The paper you are working on is titled: [PAPER TITLE] The abstract for your paper is: [PAPER ABSTRACT]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc>You are an[HCI or NLP]  researcher working on a paper to submit to [CHI or EMNLP]. The paper you are working on is titled: [PAPER TITLE] The abstract for your paper is: [PAPER ABSTRACT]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc>You are an[HCI or NLP]  researcher working on a paper to submit to[CHI  or EMNLP].The paper you are working on is titled: [PAPER TITLE]The abstract for your paper is: [PAPER ABSTRACT]The Discussion section for your paper is:[PAPER DISCUSSION]Modify this Discussion section by including supporting sources. Discuss 3 sources. Each source must be from the last five years and must include the paper name.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc>You are an [HCI or NLP] researcher working on a paper to submit to[CHI  or EMNLP].The paper you are working on is titled:[PAPER TITLE]The abstract for your paper is: [PAPER ABSTRACT]The Discussion section for your paper is:[PAPER DISCUSSION]Write a revised version of this discussion. Include up to 10 supporting sources. Each source must be from the last five years and must include the paper name and full list of authors.This prompt design was eventually changed to request the model to include the list of sources following the prose, to allow for a format more similar to the models' training data. The final prompt can be found below:You are an [HCI or NLP] researcher working on a paper to submit to[CHI  or EMNLP].The paper you are working on is titled: [PAPER TITLE]The abstract for your paper is: [PAPER ABSTRACT]The Discussion section for your paper is:[PAPER DISCUSSION]Rewrite the Discussion section to include 10 in-text citations. Also include a list of those citations with each citation in APA format.C Final Prompt Templates for all Models C.1 Abstract→CitationsThe final prompt designs provided to each model for this task can be found below. You are an [HCI or NLP] researcher working on a paper to submit to [CHI or EMNLP]. The paper you are working on is titled: [PAPER TITLE] The abstract for your paper is: [PAPER ABSTRACT] List 10 relevant papers you could cite in your Related Works section. Write each citation in APA format</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>C. 2</head><label>2</label><figDesc>.1 GPT-3.5 &amp; GPT-4 You are an [HCI or NLP] researcher working on a paper to submit to [CHI or EMNLP]. The paper you are working on is titled: [PAPER TITLE] The abstract for your paper is: [PAPER ABSTRACT] The Discussion section for your paper is: [PAPER DISCUSSION]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>SYSTEM:</head><figDesc>You are an[HCI or NLP]   researcher working on a paper to submit to[CHI or EMNLP].USER: The paper you are working on is titled: [PAPER TITLE]The abstract for your paper is: [PAPER ABSTRACT] Write a Related Works section for your paper. Include 10 in-text citations. Also include a list of those citations with each citation in APA format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy scores for each model, for each of the tasks, broken out between HCI and NLP.</figDesc><table><row><cell cols="3">Abstract → Citations List Title Accuracy</cell><cell cols="3">Abstract → Related Works Title Accuracy</cell><cell cols="3">Discussion → Supported Discussion Title Accuracy</cell></row><row><cell>HCI</cell><cell>NLP</cell><cell>Total</cell><cell>HCI</cell><cell>NLP</cell><cell>Total</cell><cell>HCI</cell><cell>NLP</cell><cell>Total</cell></row><row><cell cols="3">GPT-3 GPT-3.5 28.00% 56.00% 42.00% 24.47% 48.98% 36.98% GPT-4 54.00% 78.00% 66.00%</cell><cell cols="3">GPT-3 GPT-3.5 13.51% 50.54% 30.39% 0.00% 36.84% 19.44% GPT-4 68.87% 75.45% 72.22%</cell><cell cols="3">GPT-3 GPT-3.5 12.96% 51.43% 22.38% 34.88% 18.37% 26.09% GPT-4 47.15% 25.74% 37.50%</cell></row><row><cell cols="2">Author Precision</cell><cell/><cell cols="2">Author Precision</cell><cell/><cell cols="2">Author Precision</cell><cell/></row><row><cell>HCI</cell><cell>NLP</cell><cell>Total</cell><cell>HCI</cell><cell>NLP</cell><cell>Total</cell><cell>HCI</cell><cell>NLP</cell><cell>Total</cell></row><row><cell cols="3">GPT-3 GPT-3.5 81.46% 76.16% 77.93% 75.00% 70.29% 71.82% GPT-4 88.11% 89.01% 88.64%</cell><cell cols="3">GPT-3 GPT-3.5 62.13% 73.23% 70.55% -72.71% 72.71% GPT-4 82.07% 84.78% 83.51%</cell><cell cols="3">GPT-3 GPT-3.5 66.86% 68.94% 60.03% 55.53% 41.67% 50.33% GPT-4 82.14% 63.15% 76.26%</cell></row><row><cell cols="2">Author Recall</cell><cell/><cell>Author Recall</cell><cell/><cell/><cell>Author Recall</cell><cell/><cell/></row><row><cell>HCI</cell><cell>NLP</cell><cell>Total</cell><cell>HCI</cell><cell>NLP</cell><cell>Total</cell><cell>HCI</cell><cell>NLP</cell><cell>Total</cell></row><row><cell cols="3">GPT-3 GPT-3.5 81.46% 70.88% 73.95% 72.65% 41.54% 51.62% GPT-4 88.11% 89.01% 88.64%</cell><cell cols="3">GPT-3 GPT-3.5 61.73% 71.02% 68.77% -70.89% 70.89% GPT-4 82.07% 84.78% 83.51%</cell><cell cols="3">GPT-3 GPT-3.5 64.00% 68.11% 66.31% 37.93% 33.33% 36.21% GPT-4 82.14% 63.15% 76.26%</cell></row><row><cell cols="2">Year Accuracy</cell><cell/><cell cols="2">Year Accuracy</cell><cell/><cell cols="2">Year Accuracy</cell><cell/></row><row><cell>HCI</cell><cell>NLP</cell><cell>Total</cell><cell>HCI</cell><cell>NLP</cell><cell>Total</cell><cell>HCI</cell><cell>NLP</cell><cell>Total</cell></row><row><cell>GPT-3 GPT-3.5 0.29 1.43 GPT-4 1.44</cell><cell>0.31 0.46 1.26</cell><cell>0.68 0.40 1.33</cell><cell>GPT-3 GPT-3.5 1.06 -GPT-4 1.59</cell><cell>1.57 0.55 0.70</cell><cell>1.57 0.68 1.12</cell><cell>GPT-3 GPT-3.5 6.29 2.40 GPT-4 2.59</cell><cell>0.89 0.61 3.08</cell><cell>1.83 3.09 2.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Relevance scores for each model, for each task, broken out between HCI and NLP papers.</figDesc><table><row><cell/><cell cols="3">Abstract→Citations List Title Accuracy Significance</cell></row><row><cell/><cell>HCI</cell><cell>NLP</cell><cell/></row><row><cell/><cell cols="4">Mean SD Mean SD t-statistic p-value</cell></row><row><cell cols="4">GPT-3 GPT-3.5 0.28 0.45 0.56 0.50 0.24 0.43 0.49 0.50 GPT-4 0.54 0.50 0.78 0.41 Abstract→Related Works -3.62 -4.16 -3.68 Title Accuracy Significance</cell><cell>0.00 0.00 0.00</cell></row><row><cell/><cell>HCI</cell><cell>NLP</cell><cell/></row><row><cell/><cell cols="4">Mean SD Mean SD t-statistic p-value</cell></row><row><cell cols="3">GPT-3 GPT-3.5 0.14 0.34 0.51 0.50 0.00 0.00 0.37 0.48 GPT-4 0.69 0.46 0.75 0.43</cell><cell>-6.25 -6.22 -1.08</cell><cell>0.00 0.00 0.28</cell></row><row><cell/><cell cols="3">Discussion→Supported Discussion Title Accuracy Significance</cell></row><row><cell/><cell>HCI</cell><cell>NLP</cell><cell/></row><row><cell/><cell cols="4">Mean SD Mean SD t-statistic p-value</cell></row><row><cell>GPT-3</cell><cell cols="2">0.35 0.48 0.18 0.39</cell><cell>1.81</cell><cell>0.07</cell></row><row><cell cols="3">GPT-3.5 0.13 0.34 0.51 0.50 GPT-4 0.47 0.50 0.26 0.44</cell><cell>-5.13 3.36</cell><cell>0.00 0.00</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				</div>
		</back>
	</text>
</TEI>

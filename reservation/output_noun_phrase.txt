Preceding Text_True: reward systems
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The analysis of acknowledgments is particularly interesting as acknowledgments may give an insight into aspects of the scientific community, such as reward systems', 163, 164)
================================================================
Preceding Text_FALSE: , collaboration patterns, and hidden research trends
================================================================
Preceding Text_True: grant numbers
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('To our knowledge, previous works on automatic acknowledgment analysis were mostly concerned with the extraction of funding organizations and grant numbers', 154, 155)
================================================================
Preceding Text_True: acknowledgment texts
================================================================
Preceding Text_FALSE_không_phải_trích_dẫn_đầu: or classification of acknowledgment texts
================================================================
Preceding Text_True: informal scientific collaboration
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Analysis of the acknowledged individuals provides insight into informal scientific collaboration', 96, 97)
================================================================
Preceding Text_True: universities
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Acknowledged universities and corporations reveal interactions and knowledge exchange between industry and universities', 25, 120)
================================================================
Preceding Text_True: CoNLL2003 dataset
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The stateoftheart named entity recognition (NER) models showed a great performance on the CoNLL2003 dataset', 107, 108)
================================================================
Preceding Text_True: CoNLL2003 corpus
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('CoNLL2003 corpus', 16, 17)
================================================================
Preceding Text_True: article
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('1 3   Scientometrics The present paper is an extended version of the article', 76, 77)
================================================================
Preceding Text_FALSE: 2 presented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Scientific Documents (EEKE2022).3 Flair, an opensource natural language processing (NLP) framework
================================================================
Preceding Text_True: processed data
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data', 145, 146)
================================================================
Preceding Text_FALSE: 
================================================================
Preceding Text_True: learning process
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The annotation process is crucial as insufficient or redundant metadata can slow down and bias a learning process', 113, 114)
================================================================
Preceding Text_True: transformer architectures
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The stateoftheart models are based on deep recurrent models, convolutionbased, or 1 3   Scientometrics pretrained transformer architectures', 139, 140)
================================================================
Preceding Text_True: 4class Stanford Entity Recognizer
================================================================
Preceding Text_Nested_INDEX_KHAC_0: 4class Stanford Entity Recognizer
================================================================
Preceding Text_True: acknowledged entities
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('7 AckNER showed better performance as Flair, but is specifically designed to recognize two types of acknowledged entities', 121, 122)
================================================================
Preceding Text_True: PyTorch
================================================================
Preceding Text_Nested_NOUN_PHRASE: ('PyTorch', 78, 79)
================================================================
Preceding Text_True: Flair Embeddings
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Flair has three default training algorithms for NER which were used for the first experiment in the present research: a) NER Model with Flair Embeddings (later on Flair Embeddings)', 152, 181)
================================================================
Preceding Text_True: Transformers
================================================================
Preceding Text_FALSE_không_phải_trích_dẫn_đầu: , b) NER Model with Transformers (later on Transformers)
================================================================
Preceding Text_True: TARS
================================================================
Preceding Text_FALSE_không_phải_trích_dẫn_đầu: , and c) Zeroshot NER with TARS (later on TARS)
================================================================
Preceding Text_True: contextual string embeddings
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The Flair Embeddings model uses stacked embeddings, i.e., a combination of contextual string embeddings', 103, 104)
================================================================
Preceding Text_True: their separate uses
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Stacked embedding is an important Flair feature, as a combination of different embeddings might bring better results than their separate uses', 141, 142)
================================================================
Preceding Text_True: multilingual XMLRoBERTa transformer model
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('1 3   Scientometrics Table 2  Number of sentences/texts in the training corpora Table 3  Number of sentences/texts from each scientific domain in the training corpora LSTMCRF with the multilingual XMLRoBERTa transformer model', 225, 226)
================================================================
Preceding Text_True: fewshort learning
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The TARS (taskaware representation of sentences) is a transformerbased model, which allows performing training without any training data (zeroshot learning) or with a small dataset (fewshort learning)', 199, 201)
================================================================
Preceding Text_True: metadata records
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('As WoS contains millions of metadata records', 44, 45)
================================================================
Preceding Text_True: natural and life sciences domains
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('At the same time, information on technical and instrumental support is more common for the natural and life sciences domains', 124, 125)
================================================================
Preceding Text_True: GloVe
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('We applied GloVe', 16, 17)
================================================================
Preceding Text_True: RoBERTa model
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('For Transformers, training was initiated with the RoBERTa model', 63, 64)
================================================================
Preceding Text_True: standard approach
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('We used a standard approach, where only a linear classifier layer was added on the top of the transformer, as adding the additional CRF decoder between the transformer and linear classifier did not increase accuracy compared with this standard approach', 27, 253)
================================================================
Preceding Text_True: Adam Optimisation Algorithm
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The training was initiated with a small learning rate using the Adam Optimisation Algorithm', 91, 92)
================================================================
Preceding Text_True: TARS NER model
================================================================
Preceding Text_Nested_NOUN_PHRASE: ('TARS NER model', 75, 76)
================================================================
Preceding Text_True: most efficient algorithm
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('On the one hand, Flair developers claimed Transformers to be the most efficient algorithm', 89, 90)
================================================================
Preceding Text_True: their separate uses
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('On the other, the stacked embeddings are an important feature of the Flair tool, as a combination of different embeddings might bring better results than their separate uses', 173, 174)
================================================================
Preceding Text_True: English corpus
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Thus, Flair Embeddings were trained on the 1billion words English corpus', 72, 73)
================================================================
Preceding Text_FALSE: Previous works showed improvements in downstream tasks using embedding models finetuned for the domain used
================================================================
Preceding Text_FALSE: Moreover, further analysis of acknowledged entities showed that the miscellaneous category contained very inhomogeneous and partly irrelevant data, making the analysis more complicated
================================================================
Preceding Text_True: wrong category
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Analysis of the extracted entities showed that many entities were extracted correctly, but were assigned to the wrong category', 126, 127)
================================================================
Preceding Text_True: reward systems
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The analysis of acknowledgments is particularly interesting as acknowledgments may give an insight into aspects of the scientific community, such as reward systems', 163, 164)
================================================================
Preceding Text_FALSE: , collaboration patterns, and hidden research trends
================================================================
Preceding Text_True: grant numbers
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('To our knowledge, previous works on automatic acknowledgment analysis were mostly concerned with the extraction of funding organizations and grant numbers', 154, 155)
================================================================
Preceding Text_True: acknowledgment texts
================================================================
Preceding Text_FALSE_không_phải_trích_dẫn_đầu: or classification of acknowledgment texts
================================================================
Preceding Text_True: informal scientific collaboration
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Analysis of the acknowledged individuals provides insight into informal scientific collaboration', 96, 97)
================================================================
Preceding Text_True: universities
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Acknowledged universities and corporations reveal interactions and knowledge exchange between industry and universities', 25, 120)
================================================================
Preceding Text_True: CoNLL2003 dataset
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The stateoftheart named entity recognition (NER) models showed a great performance on the CoNLL2003 dataset', 107, 108)
================================================================
Preceding Text_True: CoNLL2003 corpus
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('CoNLL2003 corpus', 16, 17)
================================================================
Preceding Text_True: article
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('1 3   Scientometrics The present paper is an extended version of the article', 76, 77)
================================================================
Preceding Text_FALSE: 2 presented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Scientific Documents (EEKE2022).3 Flair, an opensource natural language processing (NLP) framework
================================================================
Preceding Text_True: processed data
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data', 145, 146)
================================================================
Preceding Text_FALSE: 
================================================================
Preceding Text_True: learning process
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The annotation process is crucial as insufficient or redundant metadata can slow down and bias a learning process', 113, 114)
================================================================
Preceding Text_True: transformer architectures
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The stateoftheart models are based on deep recurrent models, convolutionbased, or 1 3   Scientometrics pretrained transformer architectures', 139, 140)
================================================================
Preceding Text_True: 4class Stanford Entity Recognizer
================================================================
Preceding Text_Nested_INDEX_KHAC_0: 4class Stanford Entity Recognizer
================================================================
Preceding Text_True: acknowledged entities
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('7 AckNER showed better performance as Flair, but is specifically designed to recognize two types of acknowledged entities', 121, 122)
================================================================
Preceding Text_True: PyTorch
================================================================
Preceding Text_Nested_NOUN_PHRASE: ('PyTorch', 78, 79)
================================================================
Preceding Text_True: Flair Embeddings
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Flair has three default training algorithms for NER which were used for the first experiment in the present research: a) NER Model with Flair Embeddings (later on Flair Embeddings)', 152, 181)
================================================================
Preceding Text_True: Transformers
================================================================
Preceding Text_FALSE_không_phải_trích_dẫn_đầu: , b) NER Model with Transformers (later on Transformers)
================================================================
Preceding Text_True: TARS
================================================================
Preceding Text_FALSE_không_phải_trích_dẫn_đầu: , and c) Zeroshot NER with TARS (later on TARS)
================================================================
Preceding Text_True: contextual string embeddings
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The Flair Embeddings model uses stacked embeddings, i.e., a combination of contextual string embeddings', 103, 104)
================================================================
Preceding Text_True: their separate uses
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Stacked embedding is an important Flair feature, as a combination of different embeddings might bring better results than their separate uses', 141, 142)
================================================================
Preceding Text_True: multilingual XMLRoBERTa transformer model
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('1 3   Scientometrics Table 2  Number of sentences/texts in the training corpora Table 3  Number of sentences/texts from each scientific domain in the training corpora LSTMCRF with the multilingual XMLRoBERTa transformer model', 225, 226)
================================================================
Preceding Text_True: fewshort learning
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The TARS (taskaware representation of sentences) is a transformerbased model, which allows performing training without any training data (zeroshot learning) or with a small dataset (fewshort learning)', 199, 201)
================================================================
Preceding Text_True: metadata records
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('As WoS contains millions of metadata records', 44, 45)
================================================================
Preceding Text_True: natural and life sciences domains
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('At the same time, information on technical and instrumental support is more common for the natural and life sciences domains', 124, 125)
================================================================
Preceding Text_True: GloVe
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('We applied GloVe', 16, 17)
================================================================
Preceding Text_True: RoBERTa model
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('For Transformers, training was initiated with the RoBERTa model', 63, 64)
================================================================
Preceding Text_True: standard approach
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('We used a standard approach, where only a linear classifier layer was added on the top of the transformer, as adding the additional CRF decoder between the transformer and linear classifier did not increase accuracy compared with this standard approach', 27, 253)
================================================================
Preceding Text_True: Adam Optimisation Algorithm
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The training was initiated with a small learning rate using the Adam Optimisation Algorithm', 91, 92)
================================================================
Preceding Text_True: TARS NER model
================================================================
Preceding Text_Nested_NOUN_PHRASE: ('TARS NER model', 75, 76)
================================================================
Preceding Text_True: most efficient algorithm
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('On the one hand, Flair developers claimed Transformers to be the most efficient algorithm', 89, 90)
================================================================
Preceding Text_True: their separate uses
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('On the other, the stacked embeddings are an important feature of the Flair tool, as a combination of different embeddings might bring better results than their separate uses', 173, 174)
================================================================
Preceding Text_True: English corpus
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Thus, Flair Embeddings were trained on the 1billion words English corpus', 72, 73)
================================================================
Preceding Text_FALSE: Previous works showed improvements in downstream tasks using embedding models finetuned for the domain used
================================================================
Preceding Text_FALSE: Moreover, further analysis of acknowledged entities showed that the miscellaneous category contained very inhomogeneous and partly irrelevant data, making the analysis more complicated
================================================================
Preceding Text_True: wrong category
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Analysis of the extracted entities showed that many entities were extracted correctly, but were assigned to the wrong category', 126, 127)
================================================================
Preceding Text_True: reward systems
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The analysis of acknowledgments is particularly interesting as acknowledgments may give an insight into aspects of the scientific community, such as reward systems', 163, 164)
================================================================
Preceding Text_FALSE: , collaboration patterns, and hidden research trends
================================================================
Preceding Text_True: grant numbers
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('To our knowledge, previous works on automatic acknowledgment analysis were mostly concerned with the extraction of funding organizations and grant numbers', 154, 155)
================================================================
Preceding Text_True: acknowledgment texts
================================================================
Preceding Text_FALSE_không_phải_trích_dẫn_đầu: or classification of acknowledgment texts
================================================================
Preceding Text_True: informal scientific collaboration
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Analysis of the acknowledged individuals provides insight into informal scientific collaboration', 96, 97)
================================================================
Preceding Text_True: universities
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Acknowledged universities and corporations reveal interactions and knowledge exchange between industry and universities', 25, 120)
================================================================
Preceding Text_True: CoNLL2003 dataset
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The stateoftheart named entity recognition (NER) models showed a great performance on the CoNLL2003 dataset', 107, 108)
================================================================
Preceding Text_True: CoNLL2003 corpus
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('CoNLL2003 corpus', 16, 17)
================================================================
Preceding Text_True: article
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('1 3   Scientometrics The present paper is an extended version of the article', 76, 77)
================================================================
Preceding Text_FALSE: 2 presented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Scientific Documents (EEKE2022).3 Flair, an opensource natural language processing (NLP) framework
================================================================
Preceding Text_True: processed data
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data', 145, 146)
================================================================
Preceding Text_FALSE: 
================================================================
Preceding Text_True: learning process
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The annotation process is crucial as insufficient or redundant metadata can slow down and bias a learning process', 113, 114)
================================================================
Preceding Text_True: transformer architectures
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The stateoftheart models are based on deep recurrent models, convolutionbased, or 1 3   Scientometrics pretrained transformer architectures', 139, 140)
================================================================
Preceding Text_True: 4class Stanford Entity Recognizer
================================================================
Preceding Text_Nested_INDEX_KHAC_0: 4class Stanford Entity Recognizer
================================================================
Preceding Text_True: acknowledged entities
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('7 AckNER showed better performance as Flair, but is specifically designed to recognize two types of acknowledged entities', 121, 122)
================================================================
Preceding Text_True: PyTorch
================================================================
Preceding Text_Nested_NOUN_PHRASE: ('PyTorch', 78, 79)
================================================================
Preceding Text_True: Flair Embeddings
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Flair has three default training algorithms for NER which were used for the first experiment in the present research: a) NER Model with Flair Embeddings (later on Flair Embeddings)', 152, 181)
================================================================
Preceding Text_True: Transformers
================================================================
Preceding Text_FALSE_không_phải_trích_dẫn_đầu: , b) NER Model with Transformers (later on Transformers)
================================================================
Preceding Text_True: TARS
================================================================
Preceding Text_FALSE_không_phải_trích_dẫn_đầu: , and c) Zeroshot NER with TARS (later on TARS)
================================================================
Preceding Text_True: contextual string embeddings
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The Flair Embeddings model uses stacked embeddings, i.e., a combination of contextual string embeddings', 103, 104)
================================================================
Preceding Text_True: their separate uses
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Stacked embedding is an important Flair feature, as a combination of different embeddings might bring better results than their separate uses', 141, 142)
================================================================
Preceding Text_True: multilingual XMLRoBERTa transformer model
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('1 3   Scientometrics Table 2  Number of sentences/texts in the training corpora Table 3  Number of sentences/texts from each scientific domain in the training corpora LSTMCRF with the multilingual XMLRoBERTa transformer model', 225, 226)
================================================================
Preceding Text_True: fewshort learning
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The TARS (taskaware representation of sentences) is a transformerbased model, which allows performing training without any training data (zeroshot learning) or with a small dataset (fewshort learning)', 199, 201)
================================================================
Preceding Text_True: metadata records
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('As WoS contains millions of metadata records', 44, 45)
================================================================
Preceding Text_True: natural and life sciences domains
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('At the same time, information on technical and instrumental support is more common for the natural and life sciences domains', 124, 125)
================================================================
Preceding Text_True: GloVe
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('We applied GloVe', 16, 17)
================================================================
Preceding Text_True: RoBERTa model
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('For Transformers, training was initiated with the RoBERTa model', 63, 64)
================================================================
Preceding Text_True: standard approach
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('We used a standard approach, where only a linear classifier layer was added on the top of the transformer, as adding the additional CRF decoder between the transformer and linear classifier did not increase accuracy compared with this standard approach', 27, 253)
================================================================
Preceding Text_True: Adam Optimisation Algorithm
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The training was initiated with a small learning rate using the Adam Optimisation Algorithm', 91, 92)
================================================================
Preceding Text_True: TARS NER model
================================================================
Preceding Text_Nested_NOUN_PHRASE: ('TARS NER model', 75, 76)
================================================================
Preceding Text_True: most efficient algorithm
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('On the one hand, Flair developers claimed Transformers to be the most efficient algorithm', 89, 90)
================================================================
Preceding Text_True: their separate uses
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('On the other, the stacked embeddings are an important feature of the Flair tool, as a combination of different embeddings might bring better results than their separate uses', 173, 174)
================================================================
Preceding Text_True: English corpus
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Thus, Flair Embeddings were trained on the 1billion words English corpus', 72, 73)
================================================================
Preceding Text_FALSE_ELSE: Previous works showed improvements in downstream tasks using embedding models finetuned for the domain used
================================================================
Preceding Text_FALSE_ELSE: Moreover, further analysis of acknowledged entities showed that the miscellaneous category contained very inhomogeneous and partly irrelevant data, making the analysis more complicated
================================================================
Preceding Text_True: wrong category
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Analysis of the extracted entities showed that many entities were extracted correctly, but were assigned to the wrong category', 126, 127)
================================================================
Preceding Text_True: reward systems
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The analysis of acknowledgments is particularly interesting as acknowledgments may give an insight into aspects of the scientific community, such as reward systems', 163, 164)
================================================================
Preceding Text_FALSE: , collaboration patterns, and hidden research trends
================================================================
Preceding Text_True: grant numbers
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('To our knowledge, previous works on automatic acknowledgment analysis were mostly concerned with the extraction of funding organizations and grant numbers', 154, 155)
================================================================
Preceding Text_True: acknowledgment texts
================================================================
Preceding Text_FALSE_không_phải_trích_dẫn_đầu: or classification of acknowledgment texts
================================================================
Preceding Text_True: informal scientific collaboration
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Analysis of the acknowledged individuals provides insight into informal scientific collaboration', 96, 97)
================================================================
Preceding Text_True: universities
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Acknowledged universities and corporations reveal interactions and knowledge exchange between industry and universities', 25, 120)
================================================================
Preceding Text_True: CoNLL2003 dataset
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The stateoftheart named entity recognition (NER) models showed a great performance on the CoNLL2003 dataset', 107, 108)
================================================================
Preceding Text_True: CoNLL2003 corpus
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('CoNLL2003 corpus', 16, 17)
================================================================
Preceding Text_True: article
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('1 3   Scientometrics The present paper is an extended version of the article', 76, 77)
================================================================
Preceding Text_FALSE: 2 presented at the 3rd Workshop on Extraction and Evaluation of Knowledge Entities from Scientific Documents (EEKE2022).3 Flair, an opensource natural language processing (NLP) framework
================================================================
Preceding Text_True: processed data
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data', 145, 146)
================================================================
Preceding Text_FALSE: 
================================================================
Preceding Text_True: learning process
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The annotation process is crucial as insufficient or redundant metadata can slow down and bias a learning process', 113, 114)
================================================================
Preceding Text_True: transformer architectures
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The stateoftheart models are based on deep recurrent models, convolutionbased, or 1 3   Scientometrics pretrained transformer architectures', 139, 140)
================================================================
Preceding Text_True: 4class Stanford Entity Recognizer
================================================================
Preceding Text_Nested_INDEX_KHAC_0: 4class Stanford Entity Recognizer
================================================================
Preceding Text_True: acknowledged entities
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('7 AckNER showed better performance as Flair, but is specifically designed to recognize two types of acknowledged entities', 121, 122)
================================================================
Preceding Text_True: PyTorch
================================================================
Preceding Text_Nested_NOUN_PHRASE: ('PyTorch', 78, 79)
================================================================
Preceding Text_True: Flair Embeddings
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Flair has three default training algorithms for NER which were used for the first experiment in the present research: a) NER Model with Flair Embeddings (later on Flair Embeddings)', 152, 181)
================================================================
Preceding Text_True: Transformers
================================================================
Preceding Text_FALSE_không_phải_trích_dẫn_đầu: , b) NER Model with Transformers (later on Transformers)
================================================================
Preceding Text_True: TARS
================================================================
Preceding Text_FALSE_không_phải_trích_dẫn_đầu: , and c) Zeroshot NER with TARS (later on TARS)
================================================================
Preceding Text_True: contextual string embeddings
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The Flair Embeddings model uses stacked embeddings, i.e., a combination of contextual string embeddings', 103, 104)
================================================================
Preceding Text_True: their separate uses
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Stacked embedding is an important Flair feature, as a combination of different embeddings might bring better results than their separate uses', 141, 142)
================================================================
Preceding Text_True: multilingual XMLRoBERTa transformer model
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('1 3   Scientometrics Table 2  Number of sentences/texts in the training corpora Table 3  Number of sentences/texts from each scientific domain in the training corpora LSTMCRF with the multilingual XMLRoBERTa transformer model', 225, 226)
================================================================
Preceding Text_True: fewshort learning
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The TARS (taskaware representation of sentences) is a transformerbased model, which allows performing training without any training data (zeroshot learning) or with a small dataset (fewshort learning)', 199, 201)
================================================================
Preceding Text_True: metadata records
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('As WoS contains millions of metadata records', 44, 45)
================================================================
Preceding Text_True: natural and life sciences domains
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('At the same time, information on technical and instrumental support is more common for the natural and life sciences domains', 124, 125)
================================================================
Preceding Text_True: GloVe
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('We applied GloVe', 16, 17)
================================================================
Preceding Text_True: RoBERTa model
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('For Transformers, training was initiated with the RoBERTa model', 63, 64)
================================================================
Preceding Text_True: standard approach
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('We used a standard approach, where only a linear classifier layer was added on the top of the transformer, as adding the additional CRF decoder between the transformer and linear classifier did not increase accuracy compared with this standard approach', 27, 253)
================================================================
Preceding Text_True: Adam Optimisation Algorithm
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('The training was initiated with a small learning rate using the Adam Optimisation Algorithm', 91, 92)
================================================================
Preceding Text_True: TARS NER model
================================================================
Preceding Text_Nested_NOUN_PHRASE: ('TARS NER model', 75, 76)
================================================================
Preceding Text_True: most efficient algorithm
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('On the one hand, Flair developers claimed Transformers to be the most efficient algorithm', 89, 90)
================================================================
Preceding Text_True: their separate uses
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('On the other, the stacked embeddings are an important feature of the Flair tool, as a combination of different embeddings might bring better results than their separate uses', 173, 174)
================================================================
Preceding Text_True: English corpus
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Thus, Flair Embeddings were trained on the 1billion words English corpus', 72, 73)
================================================================
Preceding Text_FALSE_ELSE: Previous works showed improvements in downstream tasks using embedding models finetuned for the domain used
================================================================
Preceding Text_FALSE_ELSE: Moreover, further analysis of acknowledged entities showed that the miscellaneous category contained very inhomogeneous and partly irrelevant data, making the analysis more complicated
================================================================
Preceding Text_True: wrong category
================================================================
Preceding Text_FALSE_trích_dẫn_đầu_NOUN_PHRASE: ('Analysis of the extracted entities showed that many entities were extracted correctly, but were assigned to the wrong category', 126, 127)
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The analysis of acknowledgments is particularly interesting as acknowledgments may give an insight into aspects of the scientific community, such as reward systems
================================================================
Preceding Text_FALSE_NOT_FIRST: , collaboration patterns, and hidden research trends
================================================================
Preceding Text_FALSE_NOUN_PHRASE: To our knowledge, previous works on automatic acknowledgment analysis were mostly concerned with the extraction of funding organizations and grant numbers
================================================================
Preceding Text_FALSE_NOT_FIRST: or classification of acknowledgment texts
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Analysis of the acknowledged individuals provides insight into informal scientific collaboration
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Acknowledged universities and corporations reveal interactions and knowledge exchange between industry and universities
================================================================
Preceding Text_Nested_NOUN_PHRASE: the CoNLL2003 dataset
================================================================
Preceding Text_Nested_NOUN_PHRASE: CoNLL2003 corpus
================================================================
Preceding Text_FALSE_NOUN_PHRASE: 1 3   Scientometrics The present paper is an extended version of the article
================================================================
Preceding Text_Nested_INDEX_KHAC_0: (EEKE2022).3 Flair
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data
================================================================
Preceding Text_FALSE_NOT_FIRST: 
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The annotation process is crucial as insufficient or redundant metadata can slow down and bias a learning process
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The stateoftheart models are based on deep recurrent models, convolutionbased, or 1 3   Scientometrics pretrained transformer architectures
================================================================
Preceding Text_Nested_INDEX_KHAC_0: the 4class Stanford Entity Recognizer
================================================================
Preceding Text_FALSE_NOUN_PHRASE: 7 AckNER showed better performance as Flair, but is specifically designed to recognize two types of acknowledged entities
================================================================
Preceding Text_Nested_NOUN_PHRASE: PyTorch
================================================================
Preceding Text_Nested_NOUN_PHRASE: Flair Embeddings
================================================================
Preceding Text_Nested_INDEX_KHAC_0: Transformers
================================================================
Preceding Text_FALSE_NOT_FIRST: , and c) Zeroshot NER with TARS (later on TARS)
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The Flair Embeddings model uses stacked embeddings, i.e., a combination of contextual string embeddings
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Stacked embedding is an important Flair feature, as a combination of different embeddings might bring better results than their separate uses
================================================================
Preceding Text_Nested_NOUN_PHRASE: the multilingual XMLRoBERTa transformer model
================================================================
Preceding Text_Nested_NOUN_PHRASE: fewshort learning
================================================================
Preceding Text_FALSE_NOUN_PHRASE: As WoS contains millions of metadata records
================================================================
Preceding Text_FALSE_NOUN_PHRASE: At the same time, information on technical and instrumental support is more common for the natural and life sciences domains
================================================================
Preceding Text_Nested_NOUN_PHRASE: GloVe
================================================================
Preceding Text_Nested_NOUN_PHRASE: the RoBERTa model
================================================================
Preceding Text_FALSE_NOUN_PHRASE: We used a standard approach, where only a linear classifier layer was added on the top of the transformer, as adding the additional CRF decoder between the transformer and linear classifier did not increase accuracy compared with this standard approach
================================================================
Preceding Text_Nested_NOUN_PHRASE: the Adam Optimisation Algorithm
================================================================
Preceding Text_Nested_NOUN_PHRASE: the TARS NER model
================================================================
Preceding Text_FALSE_NOUN_PHRASE: On the one hand, Flair developers claimed Transformers to be the most efficient algorithm
================================================================
Preceding Text_FALSE_NOUN_PHRASE: On the other, the stacked embeddings are an important feature of the Flair tool, as a combination of different embeddings might bring better results than their separate uses
================================================================
Preceding Text_Nested_NOUN_PHRASE: English corpus
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Previous works showed improvements in downstream tasks using embedding models finetuned for the domain used
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Moreover, further analysis of acknowledged entities showed that the miscellaneous category contained very inhomogeneous and partly irrelevant data, making the analysis more complicated
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Analysis of the extracted entities showed that many entities were extracted correctly, but were assigned to the wrong category
================================================================
Preceding Text_Nested_NOUN_PHRASE: PMCLlama
================================================================
Preceding Text_FALSE_NOT_FIRST: and Llavamed
================================================================
Preceding Text_FALSE_NOUN_PHRASE: For instance, PGRA
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Later research has aimed to improve these systems by either optimizing the retrieval processes using prior answers
================================================================
Preceding Text_Nested_NOUN_PHRASE: Biotechnology Information1(NCBI
================================================================
Preceding Text_Nested_NOUN_PHRASE: PubMedBERT
================================================================
Preceding Text_FALSE_NOUN_PHRASE: We enhanced this model using the CLIP (Contrastive LanguageImage Pretraining) technique
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Based on this, we constructed a local, highquality biological vector database
================================================================
Preceding Text_Nested_NOUN_PHRASE: GeneTuring
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , MedMCQA
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , Medical Genetics
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , College Biology
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , College Medicine
================================================================
Preceding Text_Nested_NOUN_PHRASE: PMCLlama
================================================================
Preceding Text_FALSE_NOT_FIRST: and BioMistral (7B)
================================================================
Preceding Text_Nested_NOUN_PHRASE: GeneGPT
================================================================
Preceding Text_Nested_NOUN_PHRASE: CRFs
================================================================
Preceding Text_Nested_INDEX_KHAC_0: relational Markov networks
================================================================
Preceding Text_FALSE_NOUN_PHRASE: To address this problem we have previously advocated
================================================================
Preceding Text_Nested_INDEX_KHAC_0: circularities
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Both these modeling choices are in contrast to other related work in using directed, generativelytrained probabilistic models for information extraction
================================================================
Preceding Text_Nested_NOUN_PHRASE: CiteSeer
================================================================
Preceding Text_Nested_NOUN_PHRASE: ICM
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Joint parameter estimation in this complex model is intractable, and thus, as in inference, we perform parameter estimation somewhat separately for each of    Building on earlier work in coreference that assumes perfect extraction
================================================================
Preceding Text_FALSE_NOT_FIRST: , we cast coreference as a problem in graph partitioning based on Markov random fields
================================================================
Preceding Text_Nested_NOUN_PHRASE: data
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Inference within the linear chain is performed exactly by dynamic programming; inference within the fullyconnected coreference is performed approximately by a simple graph partitioning algorithm, and inference  This paper presents a method for integrated information extraction and coreference based on conditionallytrained, undirected graphical models—also known as conditional random fields
================================================================
Preceding Text_FALSE_NOT_FIRST: , where further details and background    CRFs have shown recent success in a number of domains especially in sequence modeling for natural language tasks
================================================================
Preceding Text_Nested_NOUN_PHRASE: ICM
================================================================
Preceding Text_FALSE_NOUN_PHRASE: 
================================================================
Preceding Text_FALSE_NOT_FIRST: , except that the edge weights in the graph are determined by “inThis problem is an instance of correlation clustering, which has sparked recent theoretical interest
================================================================
Preceding Text_FALSE_NOUN_PHRASE: In addition, our previous experience with coreference
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Some specialized features were developed for matching and normalizing author name fields as    We employ feature induction as part of this training
================================================================
Preceding Text_Nested_NOUN_PHRASE: CiteSeer
================================================================
Preceding Text_FALSE_NOUN_PHRASE: We report the 2We used the Secondstring package, some of the functions of which are described in
================================================================
Preceding Text_FALSE_NOT_FIRST: Table 1: coreference performance measured by pairwise F1 (upper part) cluster recall (lower part) usfrom
================================================================
Preceding Text_FALSE_NOUN_PHRASE: See
================================================================
Preceding Text_FALSE_NOUN_PHRASE: model was trained on a completely separate data set of citations
================================================================
Preceding Text_FALSE_NOUN_PHRASE: To test the significance of the improvements, we use McNemar’s test on labeling disagreements
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The analysis of acknowledgments is particularly interesting as acknowledgments may give an insight into aspects of the scientific community, such as reward systems
================================================================
Preceding Text_FALSE_NOT_FIRST: , collaboration patterns, and hidden research trends
================================================================
Preceding Text_FALSE_NOUN_PHRASE: To our knowledge, previous works on automatic acknowledgment analysis were mostly concerned with the extraction of funding organizations and grant numbers
================================================================
Preceding Text_FALSE_NOT_FIRST: or classification of acknowledgment texts
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Analysis of the acknowledged individuals provides insight into informal scientific collaboration
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Acknowledged universities and corporations reveal interactions and knowledge exchange between industry and universities
================================================================
Preceding Text_Nested_NOUN_PHRASE: the CoNLL2003 dataset
================================================================
Preceding Text_Nested_NOUN_PHRASE: CoNLL2003 corpus
================================================================
Preceding Text_FALSE_NOUN_PHRASE: 1 3   Scientometrics The present paper is an extended version of the article
================================================================
Preceding Text_Nested_INDEX_KHAC_0: (EEKE2022).3 Flair
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data
================================================================
Preceding Text_FALSE_NOT_FIRST: 
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The annotation process is crucial as insufficient or redundant metadata can slow down and bias a learning process
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The stateoftheart models are based on deep recurrent models, convolutionbased, or 1 3   Scientometrics pretrained transformer architectures
================================================================
Preceding Text_Nested_INDEX_KHAC_0: the 4class Stanford Entity Recognizer
================================================================
Preceding Text_FALSE_NOUN_PHRASE: 7 AckNER showed better performance as Flair, but is specifically designed to recognize two types of acknowledged entities
================================================================
Preceding Text_Nested_NOUN_PHRASE: PyTorch
================================================================
Preceding Text_Nested_NOUN_PHRASE: Flair Embeddings
================================================================
Preceding Text_Nested_INDEX_KHAC_0: Transformers
================================================================
Preceding Text_FALSE_NOT_FIRST: , and c) Zeroshot NER with TARS (later on TARS)
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The Flair Embeddings model uses stacked embeddings, i.e., a combination of contextual string embeddings
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Stacked embedding is an important Flair feature, as a combination of different embeddings might bring better results than their separate uses
================================================================
Preceding Text_Nested_NOUN_PHRASE: the multilingual XMLRoBERTa transformer model
================================================================
Preceding Text_Nested_NOUN_PHRASE: fewshort learning
================================================================
Preceding Text_FALSE_NOUN_PHRASE: As WoS contains millions of metadata records
================================================================
Preceding Text_FALSE_NOUN_PHRASE: At the same time, information on technical and instrumental support is more common for the natural and life sciences domains
================================================================
Preceding Text_Nested_NOUN_PHRASE: GloVe
================================================================
Preceding Text_Nested_NOUN_PHRASE: the RoBERTa model
================================================================
Preceding Text_FALSE_NOUN_PHRASE: We used a standard approach, where only a linear classifier layer was added on the top of the transformer, as adding the additional CRF decoder between the transformer and linear classifier did not increase accuracy compared with this standard approach
================================================================
Preceding Text_Nested_NOUN_PHRASE: the Adam Optimisation Algorithm
================================================================
Preceding Text_Nested_NOUN_PHRASE: the TARS NER model
================================================================
Preceding Text_FALSE_NOUN_PHRASE: On the one hand, Flair developers claimed Transformers to be the most efficient algorithm
================================================================
Preceding Text_FALSE_NOUN_PHRASE: On the other, the stacked embeddings are an important feature of the Flair tool, as a combination of different embeddings might bring better results than their separate uses
================================================================
Preceding Text_Nested_NOUN_PHRASE: English corpus
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Previous works showed improvements in downstream tasks using embedding models finetuned for the domain used
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Moreover, further analysis of acknowledged entities showed that the miscellaneous category contained very inhomogeneous and partly irrelevant data, making the analysis more complicated
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Analysis of the extracted entities showed that many entities were extracted correctly, but were assigned to the wrong category
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The analysis of acknowledgments is particularly interesting as acknowledgments may give an insight into aspects of the scientific community, such as reward systems
================================================================
Preceding Text_FALSE_NOT_FIRST: , collaboration patterns, and hidden research trends
================================================================
Preceding Text_FALSE_NOUN_PHRASE: To our knowledge, previous works on automatic acknowledgment analysis were mostly concerned with the extraction of funding organizations and grant numbers
================================================================
Preceding Text_FALSE_NOT_FIRST: or classification of acknowledgment texts
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Analysis of the acknowledged individuals provides insight into informal scientific collaboration
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Acknowledged universities and corporations reveal interactions and knowledge exchange between industry and universities
================================================================
Preceding Text_Nested_NOUN_PHRASE: the CoNLL2003 dataset
================================================================
Preceding Text_Nested_NOUN_PHRASE: CoNLL2003 corpus
================================================================
Preceding Text_FALSE_NOUN_PHRASE: 1 3   Scientometrics The present paper is an extended version of the article
================================================================
Preceding Text_Nested_INDEX_KHAC_0: (EEKE2022).3 Flair
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data
================================================================
Preceding Text_FALSE_NOT_FIRST: 
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The annotation process is crucial as insufficient or redundant metadata can slow down and bias a learning process
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The stateoftheart models are based on deep recurrent models, convolutionbased, or 1 3   Scientometrics pretrained transformer architectures
================================================================
Preceding Text_Nested_INDEX_KHAC_0: the 4class Stanford Entity Recognizer
================================================================
Preceding Text_FALSE_NOUN_PHRASE: 7 AckNER showed better performance as Flair, but is specifically designed to recognize two types of acknowledged entities
================================================================
Preceding Text_Nested_NOUN_PHRASE: PyTorch
================================================================
Preceding Text_Nested_NOUN_PHRASE: Flair Embeddings
================================================================
Preceding Text_Nested_INDEX_KHAC_0: Transformers
================================================================
Preceding Text_FALSE_NOT_FIRST: , and c) Zeroshot NER with TARS (later on TARS)
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The Flair Embeddings model uses stacked embeddings, i.e., a combination of contextual string embeddings
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Stacked embedding is an important Flair feature, as a combination of different embeddings might bring better results than their separate uses
================================================================
Preceding Text_Nested_NOUN_PHRASE: the multilingual XMLRoBERTa transformer model
================================================================
Preceding Text_Nested_NOUN_PHRASE: fewshort learning
================================================================
Preceding Text_FALSE_NOUN_PHRASE: As WoS contains millions of metadata records
================================================================
Preceding Text_FALSE_NOUN_PHRASE: At the same time, information on technical and instrumental support is more common for the natural and life sciences domains
================================================================
Preceding Text_Nested_NOUN_PHRASE: GloVe
================================================================
Preceding Text_Nested_NOUN_PHRASE: the RoBERTa model
================================================================
Preceding Text_FALSE_NOUN_PHRASE: We used a standard approach, where only a linear classifier layer was added on the top of the transformer, as adding the additional CRF decoder between the transformer and linear classifier did not increase accuracy compared with this standard approach
================================================================
Preceding Text_Nested_NOUN_PHRASE: the Adam Optimisation Algorithm
================================================================
Preceding Text_Nested_NOUN_PHRASE: the TARS NER model
================================================================
Preceding Text_FALSE_NOUN_PHRASE: On the one hand, Flair developers claimed Transformers to be the most efficient algorithm
================================================================
Preceding Text_FALSE_NOUN_PHRASE: On the other, the stacked embeddings are an important feature of the Flair tool, as a combination of different embeddings might bring better results than their separate uses
================================================================
Preceding Text_Nested_NOUN_PHRASE: English corpus
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Previous works showed improvements in downstream tasks using embedding models finetuned for the domain used
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Moreover, further analysis of acknowledged entities showed that the miscellaneous category contained very inhomogeneous and partly irrelevant data, making the analysis more complicated
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Analysis of the extracted entities showed that many entities were extracted correctly, but were assigned to the wrong category
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The analysis of acknowledgments is particularly interesting as acknowledgments may give an insight into aspects of the scientific community, such as reward systems
================================================================
Preceding Text_FALSE_NOT_FIRST: , collaboration patterns, and hidden research trends
================================================================
Preceding Text_FALSE_NOUN_PHRASE: To our knowledge, previous works on automatic acknowledgment analysis were mostly concerned with the extraction of funding organizations and grant numbers
================================================================
Preceding Text_FALSE_NOT_FIRST: or classification of acknowledgment texts
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Analysis of the acknowledged individuals provides insight into informal scientific collaboration
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Acknowledged universities and corporations reveal interactions and knowledge exchange between industry and universities
================================================================
Preceding Text_Nested_NOUN_PHRASE: the CoNLL2003 dataset
================================================================
Preceding Text_Nested_NOUN_PHRASE: CoNLL2003 corpus
================================================================
Preceding Text_FALSE_NOUN_PHRASE: 1 3   Scientometrics The present paper is an extended version of the article
================================================================
Preceding Text_Nested_INDEX_KHAC_0: (EEKE2022).3 Flair
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data
================================================================
Preceding Text_FALSE_NOT_FIRST: 
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The annotation process is crucial as insufficient or redundant metadata can slow down and bias a learning process
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The stateoftheart models are based on deep recurrent models, convolutionbased, or 1 3   Scientometrics pretrained transformer architectures
================================================================
Preceding Text_Nested_INDEX_KHAC_0: the 4class Stanford Entity Recognizer
================================================================
Preceding Text_FALSE_NOUN_PHRASE: 7 AckNER showed better performance as Flair, but is specifically designed to recognize two types of acknowledged entities
================================================================
Preceding Text_Nested_NOUN_PHRASE: PyTorch
================================================================
Preceding Text_Nested_NOUN_PHRASE: Flair Embeddings
================================================================
Preceding Text_Nested_INDEX_KHAC_0: Transformers
================================================================
Preceding Text_FALSE_NOT_FIRST: , and c) Zeroshot NER with TARS (later on TARS)
================================================================
Preceding Text_FALSE_NOUN_PHRASE: The Flair Embeddings model uses stacked embeddings, i.e., a combination of contextual string embeddings
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Stacked embedding is an important Flair feature, as a combination of different embeddings might bring better results than their separate uses
================================================================
Preceding Text_Nested_NOUN_PHRASE: the multilingual XMLRoBERTa transformer model
================================================================
Preceding Text_Nested_NOUN_PHRASE: fewshort learning
================================================================
Preceding Text_FALSE_NOUN_PHRASE: As WoS contains millions of metadata records
================================================================
Preceding Text_FALSE_NOUN_PHRASE: At the same time, information on technical and instrumental support is more common for the natural and life sciences domains
================================================================
Preceding Text_Nested_NOUN_PHRASE: GloVe
================================================================
Preceding Text_Nested_NOUN_PHRASE: the RoBERTa model
================================================================
Preceding Text_FALSE_NOUN_PHRASE: We used a standard approach, where only a linear classifier layer was added on the top of the transformer, as adding the additional CRF decoder between the transformer and linear classifier did not increase accuracy compared with this standard approach
================================================================
Preceding Text_Nested_NOUN_PHRASE: the Adam Optimisation Algorithm
================================================================
Preceding Text_Nested_NOUN_PHRASE: the TARS NER model
================================================================
Preceding Text_FALSE_NOUN_PHRASE: On the one hand, Flair developers claimed Transformers to be the most efficient algorithm
================================================================
Preceding Text_FALSE_NOUN_PHRASE: On the other, the stacked embeddings are an important feature of the Flair tool, as a combination of different embeddings might bring better results than their separate uses
================================================================
Preceding Text_Nested_NOUN_PHRASE: English corpus
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Previous works showed improvements in downstream tasks using embedding models finetuned for the domain used
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Moreover, further analysis of acknowledged entities showed that the miscellaneous category contained very inhomogeneous and partly irrelevant data, making the analysis more complicated
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Analysis of the extracted entities showed that many entities were extracted correctly, but were assigned to the wrong category
================================================================
Preceding Text_Nested_NOUN_PHRASE: PMCLlama
================================================================
Preceding Text_FALSE_NOT_FIRST: and Llavamed
================================================================
Preceding Text_FALSE_NOUN_PHRASE: For instance, PGRA
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Later research has aimed to improve these systems by either optimizing the retrieval processes using prior answers
================================================================
Preceding Text_Nested_NOUN_PHRASE: Biotechnology Information1(NCBI
================================================================
Preceding Text_Nested_NOUN_PHRASE: PubMedBERT
================================================================
Preceding Text_FALSE_NOUN_PHRASE: We enhanced this model using the CLIP (Contrastive LanguageImage Pretraining) technique
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Based on this, we constructed a local, highquality biological vector database
================================================================
Preceding Text_Nested_NOUN_PHRASE: GeneTuring
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , MedMCQA
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , Medical Genetics
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , College Biology
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , College Medicine
================================================================
Preceding Text_Nested_NOUN_PHRASE: PMCLlama
================================================================
Preceding Text_FALSE_NOT_FIRST: and BioMistral (7B)
================================================================
Preceding Text_Nested_NOUN_PHRASE: GeneGPT
================================================================
Preceding Text_FALSE_NOUN_PHRASE: et al., 2019), and large language models tailored for    specific domains, such as PMCLlama
================================================================
Preceding Text_FALSE_NOT_FIRST: and Llavamed
================================================================
Preceding Text_FALSE_NOUN_PHRASE: For instance, PGRA
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Later research has aimed to improve these systems by either optimizing the retrieval processes using prior answers
================================================================
Preceding Text_FALSE_NOUN_PHRASE: To achieve this goal, we extract research papers from the global biomedical article database maintained by the National Center for Biotechnology Information1(NCBI)
================================================================
Preceding Text_Nested_NOUN_PHRASE: PubMedBERT
================================================================
Preceding Text_FALSE_NOUN_PHRASE: We enhanced this model using the CLIP (Contrastive LanguageImage Pretraining) technique
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Based on this, we constructed a local, highquality biological vector database
================================================================
Preceding Text_FALSE_NOUN_PHRASE: It is especially useful for ob3.1 Datasets We conduct experiments on 6 popularly used biologicalrelated QA datasets to evaluate our proposed BIORAG, i.e., GeneTuring
================================================================
Preceding Text_FALSE_NOT_FIRST: , MedMCQA
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , Medical Genetics
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , College Biology
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , College Medicine
================================================================
Preceding Text_FALSE_NOUN_PHRASE: • BioLLM (Biological LLMs): PMCLlama (13B)
================================================================
Preceding Text_FALSE_NOT_FIRST: and BioMistral (7B)
================================================================
Preceding Text_Nested_NOUN_PHRASE: GeneGPT
================================================================
Preceding Text_FALSE_NOUN_PHRASE: et al., 2019), and large language models tailored for    specific domains, such as PMCLlama
================================================================
Preceding Text_FALSE_NOT_FIRST: and Llavamed
================================================================
Preceding Text_FALSE_NOUN_PHRASE: For instance, PGRA
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Later research has aimed to improve these systems by either optimizing the retrieval processes using prior answers
================================================================
Preceding Text_FALSE_NOUN_PHRASE: To achieve this goal, we extract research papers from the global biomedical article database maintained by the National Center for Biotechnology Information1(NCBI)
================================================================
Preceding Text_FALSE_NOUN_PHRASE: This model employs PubMedBERT
================================================================
Preceding Text_FALSE_NOUN_PHRASE: We enhanced this model using the CLIP (Contrastive LanguageImage Pretraining) technique
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Based on this, we constructed a local, highquality biological vector database
================================================================
Preceding Text_FALSE_NOUN_PHRASE: It is especially useful for ob3.1 Datasets We conduct experiments on 6 popularly used biologicalrelated QA datasets to evaluate our proposed BIORAG, i.e., GeneTuring
================================================================
Preceding Text_FALSE_NOT_FIRST: , MedMCQA
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , Medical Genetics
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , College Biology
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , College Medicine
================================================================
Preceding Text_FALSE_NOUN_PHRASE: • BioLLM (Biological LLMs): PMCLlama (13B)
================================================================
Preceding Text_FALSE_NOT_FIRST: and BioMistral (7B)
================================================================
Preceding Text_FALSE_NOUN_PHRASE: • SciRAG (Scientific RAGLLM framework): GeneGPT (175B)
================================================================
Preceding Text_FALSE_NOUN_PHRASE: et al., 2019), and large language models tailored for    specific domains, such as PMCLlama
================================================================
Preceding Text_FALSE_NOT_FIRST: and Llavamed
================================================================
Preceding Text_FALSE_NOUN_PHRASE: For instance, PGRA
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Later research has aimed to improve these systems by either optimizing the retrieval processes using prior answers
================================================================
Preceding Text_FALSE_NOUN_PHRASE: To achieve this goal, we extract research papers from the global biomedical article database maintained by the National Center for Biotechnology Information1(NCBI)
================================================================
Preceding Text_FALSE_NOUN_PHRASE: This model employs PubMedBERT
================================================================
Preceding Text_FALSE_NOUN_PHRASE: We enhanced this model using the CLIP (Contrastive LanguageImage Pretraining) technique
================================================================
Preceding Text_FALSE_NOUN_PHRASE: Based on this, we constructed a local, highquality biological vector database
================================================================
Preceding Text_FALSE_NOUN_PHRASE: It is especially useful for ob3.1 Datasets We conduct experiments on 6 popularly used biologicalrelated QA datasets to evaluate our proposed BIORAG, i.e., GeneTuring
================================================================
Preceding Text_FALSE_NOT_FIRST: , MedMCQA
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , Medical Genetics
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , College Biology
================================================================
Preceding Text_Nested_INDEX_KHAC_0: , College Medicine
================================================================
Preceding Text_FALSE_NOUN_PHRASE: • BioLLM (Biological LLMs): PMCLlama (13B)
================================================================
Preceding Text_FALSE_NOT_FIRST: and BioMistral (7B)
================================================================
Preceding Text_FALSE_NOUN_PHRASE: • SciRAG (Scientific RAGLLM framework): GeneGPT (175B)
================================================================

{
  "text": [
    "This is extracted text content from the PDF document."
  ],
  "citation_candidates": [
    "b0",
    "b1",
    "b2",
    "b3",
    "b4",
    "b5",
    "b6",
    "b7",
    "b8",
    "b9",
    "b10",
    "b11",
    "b12",
    "b13",
    "b14",
    "b15",
    "b16",
    "b17",
    "b18",
    "b19",
    "b20",
    "b21",
    "b22",
    "b23",
    "b24",
    "b25",
    "b26",
    "b27",
    "b28",
    "b29",
    "b30",
    "b31",
    "b32",
    "b33",
    "b34",
    "b35",
    "b36",
    "b37",
    "b38",
    "b39",
    "b40",
    "b41",
    "b42"
  ],
  "bib_entries": {
    "b0": {
      "title": "Extracting funder information from scientific papers-Experiences with question answering",
      "doi": "10.1007/978-3-031-16802-4_24",
      "abstract": "AbstractThis paper is about automatic recognition of entities that funded a research work in economics as being expressed in a publication. While many works apply rules and/or regular expressions to candidate sections within the text, we follow a question answering (QA) based approach to identify those passages that are most likely to inform us about funding. With regard to a digital library scenario, we are dealing with three more challenges: confirming that our approach at least outperforms manual indexing, disambiguation of funding organizations by linking their names to authority data, and integrating the generated metadata into a digital library application. Our computational results by means of machine learning techniques show that our QA performs similar to a previous work (AckNER), although we operated on rather small sets of training and test data. While manual indexing is still needed for a gold standard of reliable metadata, the identification of funding entities only worked for a subset of funder names."
    },
    "b1": {
      "title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling",
      "doi": "10.48550/ARXIV.1312.3005",
      "abstract": "We propose a new benchmark corpus to be used for measuring progress in\nstatistical language modeling. With almost one billion words of training data,\nwe hope this benchmark will be useful to quickly evaluate novel language\nmodeling techniques, and to compare their contribution when combined with other\nadvanced techniques. We show performance of several well-known types of\nlanguage models, with the best results achieved with a recurrent neural network\nbased language model. The baseline unpruned Kneser-Ney 5-gram model achieves\nperplexity 67.6; a combination of techniques leads to 35% reduction in\nperplexity, or 10% reduction in cross-entropy (bits), over that baseline.\n  The benchmark is available as a code.google.com project; besides the scripts\nneeded to rebuild the training/held-out data, it also makes available\nlog-probability values for each word in each of ten held-out data sets, for\neach of the baseline n-gram models."
    },
    "b2": {
      "title": "Network dynamics in university-industry collaboration: A collaboration-knowledge dual-layer network perspective",
      "doi": "10.1007/s11192-022-04330-9",
      "abstract": "Collaborations between universities and firms provide a key pathway for innovation. In thebig data era, however, the interactions between these two communities are being reshapedby information of much higher complexity and knowledge exchanges with more volume andpace. With this research, we put forward a methodology for comprehensively measuringboth actor collaboration and produced knowledge in shaping network dynamics of university-industry collaboration. Using dual-layer networks consisting of organizations and topics, we …"
    },
    "b3": {
      "title": "The Scholar's courtesy: The role of acknowledgement in the primary communication process",
      "doi": "10.1002/(sici)1097-4571(199703)48:3<279::aid-asi11>3.0.co;2-y",
      "abstract": null
    },
    "b4": {
      "title": "The praxis of acknowledgement: From bibliometrics to influmetrics",
      "doi": "10.3989/redc.1995.v18.i2.654",
      "abstract": "El uso del agradecimiento está muy extendido en la comunicación académica y tiende a aumentar. El agradecimiento sirve para definir una serie de relaciones sociales y de conocimiento entre investigadores de una disciplina, así como vinculación con otras áreas del saber y, por eso, puede utilizarse para hacer un diseño de redes de influencia. En este artículo se explora la relación entre autores, agradecimientos y citas -el «triángulo del reconocimiento»- en el contexto de la evaluación académica. Los autores también consideran la posible utilización de datos procedentes del agradecimiento conjuntamente con otros indicadores bibliométricos, tales como las citas, en las evaluaciones."
    },
    "b5": {
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "doi": "10.48550/ARXIV.1810.04805",
      "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement)."
    },
    "b6": {
      "title": "Making visible the invisible through the analysis of acknowledgements in the humanities",
      "doi": "10.1108/AJIM-01-2017-0008",
      "abstract": "PurposeScience is subject to a normative structure that includes how the contributions and interactions between scientists are rewarded. Authorship and citations have been the key elements within the reward system of science, whereas acknowledgements, despite being a well-established element in scholarly communication, have not received the same attention. The purpose of this paper is to put forward the bearing of acknowledgements in the humanities to bring to the foreground contributions and interactions that, otherwise, would remain invisible through traditional indicators of research performance.Design/methodology/approachThe study provides a comprehensive framework to understanding acknowledgements as part of the reward system with a special focus on their value in the humanities as a reflection of intellectual indebtedness. The distinctive features of research in the humanities are outlined and the role of acknowledgements as a source of contributorship information is reviewed to support these assumptions.Findings“Peer interactive communication” is the prevailing support thanked in the acknowledgements of humanities, so the notion of acknowledgements as “super-citations” can make special sense in this area. Since single-authored papers still predominate as publishing pattern in this domain, the study of acknowledgements might help to understand social interactions and intellectual influences that lie behind a piece of research and are not visible through authorship.Originality/valuePrevious works have proposed and explored the prevailing acknowledgement types by domain. This paper focusses on the humanities to show the role of acknowledgements within the reward system and highlight publication patterns and inherent research features which make acknowledgements particularly interesting in the area as a reflection of the socio-cognitive structure of research."
    },
    "b7": {
      "title": "Effectiveness of research grants funded by European research council and polish national science centre",
      "doi": "10.1016/j.joi.2021.101243",
      "abstract": "We propose WEIG – Wroclaw Effectiveness Indicator for Grants. This new scientometric measure is an aggregated quality measure of scientific papers published with the grant support divided by its budget. Several WEIG variations have been considered with respect to journal quality indicators like Impact Factor (IF), Article Influence Score (AIS), and Average Journal Impact Factor Percentile (IF%). Projects from two public agencies were analysed utilising the WEIG measures: European Research Council (ERC) and National Science Centre, Poland (NSC). The studies revealed that NSC grants are overall more effective than ERC ones, constantly 2–3 times more for Physical Sciences and Engineering (PE). There are four NSC panels distinctively more efficient than their counterparts in ERC: Mathematics (PE1), Fundamental Constituents of Matter (PE2), Computer Science and Informatics (PE6) and Universe Sciences (PE9). The most efficient NSC funding schemes are Etiuda, Preludium, and Harmonia. The higher average effectiveness of programmes aimed at young scientists has been observed: the ERC Starting Grants have greater effectiveness than Advanced Grants. Both agencies manage to keep overall efficiency regardless of increasing their budget over the years. Limitations of the proposed approach to assess project effectiveness, especially for Social Sciences and Humanities, are also discussed."
    },
    "b8": {
      "title": "A rule-based named-entity recognition method for knowledge extraction of evidence-based dietary recommendations",
      "doi": "10.1371/journal.pone.0179488",
      "abstract": "Evidence-based dietary information represented as unstructured text is a crucial information that needs to be accessed in order to help dietitians follow the new knowledge arrives daily with newly published scientific reports. Different named-entity recognition (NER) methods have been introduced previously to extract useful information from the biomedical literature. They are focused on, for example extracting gene mentions, proteins mentions, relationships between genes and proteins, chemical concepts and relationships between drugs and diseases. In this paper, we present a novel NER method, called drNER, for knowledge extraction of evidence-based dietary information. To the best of our knowledge this is the first attempt at extracting dietary concepts. DrNER is a rule-based NER that consists of two phases. The first one involves the detection and determination of the entities mention, and the second one involves the selection and extraction of the entities. We evaluate the method by using text corpora from heterogeneous sources, including text from several scientifically validated web sites and text from scientific publications. Evaluation of the method showed that drNER gives good results and can be used for knowledge extraction of evidence-based dietary recommendations."
    },
    "b9": {
      "title": "Unsupervised named-entity extraction from the web: An experimental study",
      "doi": "10.1016/j.artint.2005.03.001",
      "abstract": "The KnowItAll system aims to automate the tedious process of extracting large collections offacts (eg, names of scientists or politicians) from the Web in an unsupervised, domain-independent, and scalable manner. The paper presents an overview of KnowItAll's novelarchitecture and design principles, emphasizing its distinctive ability to extract informationwithout any hand-labeled training examples. In its first major run, KnowItAll extracted over50,000 class instances, but suggested a challenge: How can we improve KnowItAll's recall …"
    },
    "b10": {
      "title": "Incorporating non-local information into information extraction systems by Gibbs sampling",
      "doi": "10.3115/1219840.1219885",
      "abstract": "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks."
    },
    "b11": {
      "title": "Who gets acknowledged: Measuring scientific contributions through automatic acknowledgment indexing",
      "doi": "10.1073/pnas.0407743101",
      "abstract": "Acknowledgments in research publications, like citations, indicate influential contributions to scientific work. However, acknowledgments are different from citations; whereas citations are formal expressions of debt, acknowledgments are arguably more personal, singular, or private expressions of appreciation and contribution. Furthermore, many sources of research funding expect researchers to acknowledge any support that contributed to the published work. Just as citation indexing proved to be an important tool for evaluating research contributions, we argue that acknowledgments can be considered as a metric parallel to citations in the academic audit process. We have developed automated methods for acknowledgment extraction and analysis and show that combining acknowledgment analysis with citation indexing yields a measurable impact of the efficacy of various individuals as well as government, corporate, and university sponsors of scientific work."
    },
    "b12": {
      "title": "Task-Aware Representation of Sentences for Generic Text Classification",
      "doi": "10.18653/v1/2020.coling-main.285",
      "abstract": "State-of-the-art approaches for text classification leverage a transformer architecture with a linear layer on top that outputs a class distribution for a given prediction problem. While effective, this approach suffers from conceptual limitations that affect its utility in few-shot or zero-shot transfer learning scenarios. First, the number of classes to predict needs to be pre-defined. In a transfer learning setting, in which new classes are added to an already trained classifier, all information contained in a linear layer is therefore discarded, and a new layer is trained from scratch. Second, this approach only learns the semantics of classes implicitly from training examples, as opposed to leveraging the explicit semantic information provided by the natural language names of the classes. For instance, a classifier trained to predict the topics of news articles might have classes like \"business\" or \"sports\" that themselves carry semantic information. Extending a classifier to predict a new class named \"politics\" with only a handful of training examples would benefit from both leveraging the semantic information in the name of a new class and using the information contained in the already trained linear layer. This paper presents a novel formulation of text classification that addresses these limitations. It imbues the notion of the task at hand into the transformer model itself by factorizing arbitrary classification problems into a generic binary classification problem. We present experiments in few-shot and zero-shot transfer learning that show that our approach significantly outperforms previous approaches on small training data and can even learn to predict new classes with no training examples at all. The implementation of our model is publicly available at: https://github.com/flairNLP/flair."
    },
    "b13": {
      "title": "Analysis of acknowledgments of libraries in the journal literature using machine learning",
      "doi": "10.1002/pra2.698",
      "abstract": "ABSTRACTIncreasing emphasis is being placed on research impact and it has prompted scholars to explore contributions beyond traditional research impact metrics. Acknowledgments, which are formal statements of indebtedness and contribution, within the journal literature provide an additional means to assess impact. This study examines contributions of libraries to the scholarly literature within acknowledgments using a combination of machine learning and manual methods to quantify and characterize acknowledgments."
    },
    "b14": {
      "title": "CycleNER: An unsupervised training approach for named entity recognition",
      "doi": "10.1145/3485447.3512012",
      "abstract": "Named Entity Recognition (NER) is a crucial natural language understanding task for many down-stream tasks such as question answering and retrieval. Despite significant progress in developing NER models for multiple languages and domains, scaling to emerging and/or low-resource domains still remains challenging, due to the costly nature of acquiring training data. We propose CycleNER, an unsupervised approach based on cycle-consistency training that uses two functions: (i) sentence-to-entity – S2E and (ii) entity-to-sentence – E2S, to carry out the NER task. CycleNER does not require annotations but a set of sentences with no entity labels and another independent set of entity examples. Through cycle-consistency training, the output from one function is used as input for the other (e.g. S2E → E2S) to align the representation spaces of both functions and therefore enable unsupervised training. Evaluation on several domains comparing CycleNER against supervised and unsupervised competitors shows that CycleNER achieves highly competitive performance with only a few thousand input sentences. We demonstrate competitive performance against supervised models, achieving 73% of supervised performance without any annotations on CoNLL03, while significantly outperforming unsupervised approaches."
    },
    "b15": {
      "title": "A refinement strategy for identification of scientific software from bioinformatics publications",
      "doi": "10.1007/s11192-022-04381-y",
      "abstract": "In the field of bioinformatics, a large number of classical software becomes a necessaryresearch tool. To measure the influence of scientific software as one kind of importantintellectual products, a few strategies have been proposed to identify the software namesfrom full texts of papers to collect the usage data of packages in bioinformatics research.However, the performance of these strategies is limited because of the highly imbalance ofdata in the full texts. This study proposes EnsembleSVMs-CRF, a two-step refinement …"
    },
    "b16": {
      "title": "On authorship and acknowledgments",
      "doi": "10.1056/NEJM199111213252112",
      "abstract": "LARGE clinical trials from multiple institutions now involve dozens and sometimes hundreds of people in their conception, design, implementation, analysis, and preparation of reports for publication. Increasingly, we have become concerned about two interrelated aspects of many reports of multicenter studies: ambiguous authorship and lengthy acknowledgments.Clear specification of authors is essential so that any substantive questions about a submitted or published study can be resolved. Authorship has been defined in many ways, but most of the definitions have in common a requirement that authors have sufficient intellectual involvement with the overall study to be able to take responsibility for . . ."
    },
    "b17": {
      "title": "Tagging funding agencies and grants in scientific articles using sequential learning models",
      "doi": "10.18653/v1/w17-2327",
      "abstract": "In this paper we present a solution for tagging funding bodies and grants in scientific articles using a combination of trained sequential learning models, namely conditional random fields (CRF), hidden markov models (HMM) and maximum entropy models (MaxEnt), on a benchmark set created in-house. We apply the trained models to address the BioASQ challenge 5c, which is a newly introduced task that aims to solve the problem of funding information extraction from scientific articles. Results in the dry-run data set of BioASQ task 5c show that the suggested approach can achieve a micro-recall of more than 85% in tagging both funding bodies and grants."
    },
    "b18": {
      "title": "Identifying named entities in academic biographies with supervised learning",
      "doi": "10.1007/s11192-018-2797-4",
      "abstract": "Personal webpages of researchers or faculty members make up a percentage of theacademic web. These webpages contain semi-structured or plain text information, andresearch has shown the importance of combining information extracted from multipleacademic websites to create a unified database that can help in expert finding, and thusimprove information retrieval for end users. This research identifies the kind of namedentities that could be present in academic biographies by manually examining the …"
    },
    "b19": {
      "title": "Adam: A method for stochastic optimization",
      "doi": "10.48550/ARXIV.1412.6980",
      "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm."
    },
    "b20": {
      "title": "Dataset of identified scholars mentioned in acknowledgement statements",
      "doi": "10.1038/s41597-022-01585-y",
      "abstract": "AbstractAcknowledgements represent scholars’ relationships as part of the research contribution. While co-authors and citations are often provided as a well-formatted bibliometric database, acknowledged individuals are difficult to identify because they appear as part of the statements in the paper. We identify acknowledged scholars who appeared in papers published in open-access journals by referring to the co-author and citation relationships stored in the Microsoft Academic Graph (MAG). Therefore, the constructed dataset is compatible with MAG, which accelerates and expands the acknowledgements as a data source of scholarly relationships similar to collaboration and citation analysis. Moreover, the implemented code is publicly available; thus, it can be applied in other studies."
    },
    "b21": {
      "title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "doi": null,
      "abstract": "Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code."
    },
    "b22": {
      "title": "Acknowledgements patterns in sociology",
      "doi": null,
      "abstract": null
    },
    "b23": {
      "title": "Beyond Garfield's citation index: An assessment of some issues in building a personal name acknowledgments index",
      "doi": "10.1007/s11192-017-2598-1",
      "abstract": "To study patterns of personal acknowledgments in life sciences research and assess thefeasibility of a formal Personal Acknowledgments Index, two successive 5-year (1995–1999,2000–2004) sets of original research articles on zebrafish (Danio rerio) were scanned foracknowledgment statements thanking individuals for various “gifts” of research materials,services, and interpersonal communication. Text areas mined included “Materials andMethods”(M&M) and various text locations of “Acknowledgments”(ACK). Acknowledgment …"
    },
    "b24": {
      "title": "Communication, competition, and secrecy: The production and dissemination of research-related information in genetics",
      "doi": "10.1177/016224399101600404",
      "abstract": "The dissemination of experimental materials, instruments, and methods is central to the progress of research in genetics. In recent years, competition for research funding and intellectual property issues have increasingly presented barriers to the dissemination of this \"research-related information. \"Information gathered in interviews with experimental geneticists and analysis of acknowledgment patterns in published genetics research are used to construct a series of basic scenarios for the exchange of genetic materials and research methods. The discussion focuses on factors affecting individuals' behavior and expectations as information requesters and information providers."
    },
    "b25": {
      "title": "Using acknowledgement data to characterize funding organizations by the types of research sponsored: the case of robotics research",
      "doi": "10.1007/s11192-017-2617-2",
      "abstract": "Funded research has been linked to academic production and performance. While thepresence of funding acknowledgements may serve as an indicator of quality to some extent,we still lack tools to evaluate whether funding agencies allocate resources to novel andinnovative research rather than mature fields. We address this issue in the present study byusing bibliometrics. In particular, we exploit the citation network properties of academicarticles to classify specific research fields into four categories: change maker, breakthrough …"
    },
    "b26": {
      "title": "PyTorch: An imperative style, highperformance deep learning library",
      "doi": null,
      "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both.PyTorch is a machine learning library that shows that these two goals are in fact compatible:it was designed from first principles to support an imperative and Pythonic programmingstyle that supports code as a model, makes debugging easy and is consistent with otherpopular scientific computing libraries, while remaining efficient and supporting hardwareaccelerators such as GPUs. In this paper, we detail the principles that drove the …"
    },
    "b27": {
      "title": "Acknowledgements are not just thank you notes: A qualitative analysis of acknowledgements content in scientific articles and reviews published in 2015",
      "doi": "10.1371/journal.pone.0226727",
      "abstract": "Acknowledgements in scientific articles can be described as miscellaneous, their content ranging from pre-formulated financial disclosure statements to personal testimonies of gratitude. To improve understanding of the context and various uses of expressions found in acknowledgements, this study analyses their content qualitatively. The most frequent noun phrases from a Web of Science acknowledgements corpus were analysed to generate 13 categories. When 3,754 acknowledgement sentences were manually coded into the categories, three distinct axes emerged: the contributions, the disclaimers, and the authorial voice. Acknowledgements constitute a space where authors can detail the division of labour within collaborators of a research project. Results also show the importance of disclaimers as part of the current scholarly communication apparatus, an aspect which was not highlighted by previous analyses and typologies of acknowledgements. Alongside formal disclaimers and acknowledgements of various contributions, there seems to remain a need for a more personal space where the authors can speak for themselves, in their own name, on matters they judge worth mentioning."
    },
    "b28": {
      "title": "Beyond funding: Acknowledgement patterns in biomedical, natural and social sciences",
      "doi": "10.1371/journal.pone.0185578",
      "abstract": "For the past 50 years, acknowledgments have been studied as important paratextual traces of research practices, collaboration, and infrastructure in science. Since 2008, funding acknowledgments have been indexed by Web of Science, supporting large-scale analyses of research funding. Applying advanced linguistic methods as well as Correspondence Analysis to more than one million acknowledgments from research articles and reviews published in 2015, this paper aims to go beyond funding disclosure and study the main types of contributions found in acknowledgments on a large scale and through disciplinary comparisons. Our analysis shows that technical support is more frequently acknowledged by scholars in Chemistry, Physics and Engineering. Earth and Space, Professional Fields, and Social Sciences are more likely to acknowledge contributions from colleagues, editors, and reviewers, while Biology acknowledgments put more emphasis on logistics and fieldwork-related tasks. Conflicts of interest disclosures (or lack of thereof) are more frequently found in acknowledgments from Clinical Medicine, Health and, to a lesser extent, Psychology. These results demonstrate that acknowledgment practices truly do vary across disciplines and that this can lead to important further research beyond the sole interest in funding."
    },
    "b29": {
      "title": "GloVe: Global Vectors for Word Representation",
      "doi": "10.3115/v1/d14-1162",
      "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition."
    },
    "b30": {
      "title": "Natural language annotation for machine learning",
      "doi": null,
      "abstract": "Create your own natural language training corpus for machine learning. Whether youreworking with English, Chinese, or any other natural language, this hands-on book guidesyou through a proven annotation development cyclethe process of adding metadata to yourtraining corpus to help ML algorithms work more efficiently. You dont need any programmingor linguistics experience to get started. Using detailed examples at every step, youll learnhow the MATTER Annotation Development Process helps you Model, Annotate, Train, Test …"
    },
    "b31": {
      "title": "What 5,000 acknowledgements tell us about informal collaboration in financial economics",
      "doi": "10.1016/j.respol.2021.104236",
      "abstract": "We present and discuss a novel dataset on informal collaboration in financial economics,manually collected from more than 5,000 acknowledgement sections of published papers.We find that informal collaboration is the norm in financial economics, while generationaldifferences in informal collaboration exist and reciprocity among collaborators prevails.Female researchers appear less often in acknowledgements than comparable maleresearchers. Information derived from networks of informal collaboration allows us to predict …"
    },
    "b32": {
      "title": "Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition",
      "doi": null,
      "abstract": "We describe the CoNLL-2003 shared task: language-independent named entity recognition.We give background information on the data sets (English and German) and the evaluationmethod, present a general overview of the systems that have taken part in the task anddiscuss their performance."
    },
    "b33": {
      "title": "FLERT: Document-level features for named entity recognition",
      "doi": "10.48550/arXiv.2011.06993",
      "abstract": "Current state-of-the-art approaches for named entity recognition (NER)\ntypically consider text at the sentence-level and thus do not model information\nthat crosses sentence boundaries. However, the use of transformer-based models\nfor NER offers natural options for capturing document-level features. In this\npaper, we perform a comparative evaluation of document-level features in the\ntwo standard NER architectures commonly considered in the literature, namely\n\"fine-tuning\" and \"feature-based LSTM-CRF\". We evaluate different\nhyperparameters for document-level features such as context window size and\nenforcing document-locality. We present experiments from which we derive\nrecommendations for how to model document context and present new\nstate-of-the-art scores on several CoNLL-03 benchmark datasets. Our approach is\nintegrated into the Flair framework to facilitate reproduction of our\nexperiments."
    },
    "b34": {
      "title": "SsciBERT: A pre-trained language model for social science texts",
      "doi": "10.1007/s11192-022-04602-4",
      "abstract": "The academic literature of social sciences records human civilization and studies human social problems. With its large-scale growth, the ways to quickly find existing research on relevant issues have become an urgent demand for researchers. Previous studies, such as SciBERT, have shown that pre-training using domain-specific texts can improve the performance of natural language processing tasks. However, the pre-trained language model for social sciences is not available so far. In light of this, the present research proposes a pre-trained model based on the abstracts published in the Social Science Citation Index (SSCI) journals. The models, which are available on GitHub ( https://github.com/S-T-Full-Text-Knowledge-Mining/SSCI-BERT ), show excellent performance on discipline classification, abstract structure–function recognition, and named entity recognition tasks with the social sciences literature."
    },
    "b35": {
      "title": "The journal coverage of web of science, scopus and dimensions: A comparative analysis",
      "doi": "10.1007/s11192-021-03948-5",
      "abstract": "Traditionally, Web of Science and Scopus have been the two most widely used\ndatabases for bibliometric analyses. However, during the last few years some\nnew scholarly databases, such as Dimensions, have come up. Several previous\nstudies have compared different databases, either through a direct comparison\nof article coverage or by comparing the citations across the databases. This\narticle attempts to compare the journal coverage of the three databases: Web of\nScience, Scopus and Dimensions. The most recent master journal lists of the\nthree databases have been used for the purpose of identifying the overlapping\nand unique journals covered in the databases. The results indicate that the\ndatabases have significantly different journal coverage, with the Web of\nScience being most selective and Dimensions being the most exhaustive. About\n99.11% and 96.61% of the journals indexed in Web of Science are also indexed in\nScopus and Dimensions, respectively. Scopus has 96.42% of its indexed journals\nalso covered by Dimensions. Dimensions database has the most exhaustive\ncoverage, with 82.22% more journals covered as compared to Web of Science and\n48.17% more journals covered as compared to Scopus. We also analysed the\nresearch outputs for 20 highly productive countries for the 2010-2019 period,\nas indexed in the three databases, and identified database-induced variations\nin research output volume, rank and global share of different countries. In\naddition to variations in overall coverage of research output from different\ncountries, the three databases appear to have differential coverage of\ndifferent disciplines."
    },
    "b36": {
      "title": "Evaluation of embedding models for automatic extraction and classification of acknowledged entities in scientific documents",
      "doi": "10.1007/s11192-022-04554-9",
      "abstract": "AbstractAnalysis of acknowledgments is particularly interesting as acknowledgments may give information not only about funding, but they are also able to reveal hidden contributions to authorship and the researcher’s collaboration patterns, context in which research was conducted, and specific aspects of the academic work. The focus of the present research is the analysis of a large sample of acknowledgement texts indexed in the Web of Science (WoS) Core Collection. Record types “article” and “review” from four different scientific domains, namely social sciences, economics, oceanography and computer science, published from 2014 to 2019 in a scientific journal in English were considered. Six types of acknowledged entities, i.e., funding agency, grant number, individuals, university, corporation and miscellaneous, were extracted from the acknowledgement texts using a named entity recognition tagger and subsequently examined. A general analysis of the acknowledgement texts showed that indexing of funding information in WoS is incomplete. The analysis of the automatically extracted entities revealed differences and distinct patterns in the distribution of acknowledged entities of different types between different scientific domains. A strong association was found between acknowledged entity and scientific domain, and acknowledged entity and entity type. Only negligible correlation was found between the number of citations and the number of acknowledged entities. Generally, the number of words in the acknowledgement texts positively correlates with the number of acknowledged funding organizations, universities, individuals and miscellaneous entities. At the same time, acknowledgement texts with the larger number of sentences have more acknowledged individuals and miscellaneous categories."
    },
    "b37": {
      "title": "Examining influential factors for acknowledgements classification using supervised learning",
      "doi": "10.1371/journal.pone.0228928",
      "abstract": "Acknowledgements have been examined as important elements in measuring the contributions to and intellectual debts of a scientific publication. Unlike previous studies that were limited in the scope of analysis and manual examination. The present study aimed to conduct the automatic classification of acknowledgements on a large scale of data. To this end, we first created a training dataset for acknowledgements classification by sampling the acknowledgements sections from the entire PubMed Central database. Second, we adopted various supervised learning algorithms to examine which algorithm performed best in what condition. In addition, we observed the factors affecting classification performance. We investigated the effects of the following three main aspects: classification algorithms, categories, and text representations. The CNN+Doc2Vec algorithm achieved the highest performance of 93.58% accuracy in the original dataset and 87.93% in the converted dataset. The experimental results indicated that the characteristics of categories and sentence patterns influenced the performance of classification. Most of the classifiers performed better on the categories of financial, peer interactive communication, and technical support compared to other classes."
    },
    "b38": {
      "title": "Using named entity recognition as a classification heuristic",
      "doi": "10.9776/14401",
      "abstract": "This poster proposes the use of Named Entity Recognition as a heuristic tool for improving manual document classification. This technique was developed as part of a project studying collaborative work via the acknowledgment statements found in a corpus of formally published journal articles. We demonstrate how uncertainty in our initial text mining results were ‘ground-truthed’ using Natural Language Processing tools in a quick-and-dirty fashion. To verify this technique’s validity, we offer some initial results from our larger study."
    },
    "b39": {
      "title": "Funding acknowledgement analysis: An enhanced tool to investigate research sponsorship impacts: The case of nanotechnology",
      "doi": "10.1007/s11192-011-0362-5",
      "abstract": "There is increasing interest in assessing how sponsored research funding influences thedevelopment and trajectory of science and technology. Traditionally, linkages betweenresearch funding and subsequent results are hard to track, often requiring access toseparate funding or performance reports released by researchers or sponsors. Tracingresearch sponsorship and output linkages is even more challenging when researchersreceive multiple funding awards and collaborate with a variety of differentially-sponsored …"
    },
    "b40": {
      "title": "LUKE: Deep contextualized entity representations with entity-aware self-attention",
      "doi": "10.18653/v1/2020.emnlp-main.523",
      "abstract": "Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke."
    },
    "b41": {
      "title": "Named entity recognition as dependency parsing",
      "doi": "10.48550/ARXIV.2005.07150",
      "abstract": "Named Entity Recognition (NER) is a fundamental task in Natural Language\nProcessing, concerned with identifying spans of text expressing references to\nentities. NER research is often focused on flat entities only (flat NER),\nignoring the fact that entity references can be nested, as in [Bank of [China]]\n(Finkel and Manning, 2009). In this paper, we use ideas from graph-based\ndependency parsing to provide our model a global view on the input via a\nbiaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of\nstart and end tokens in a sentence which we use to explore all spans, so that\nthe model is able to predict named entities accurately. We show that the model\nworks well for both nested and flat NER through evaluation on 8 corpora and\nachieving SoTA performance on all of them, with accuracy gains of up to 2.2\npercentage points."
    },
    "b42": {
      "title": "Guest editorial: Extraction and evaluation of knowledge entities in the age of artificial intelligence",
      "doi": "10.1108/AJIM-05-2023-507",
      "abstract": "Scientific documents serve as an essential mediator for research achievements andscientific knowledge. With the increasing availability of full-text data and advanced dataanalytical techniques, bibliometric researchers have started to focus on granular contentwithin scientific documents, transitioning their foci from the external metadata of thesedocuments to the internal knowledge they encompass. In scientific documents, knowledgeconsists of many interconnected units, known as knowledge entities (Ding et al., 2013). Ding …"
    }
  }
}
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Pipeline:
  1. Grobid -> TEI XML
  2. Parse XML -> title (level="a" ∆∞u ti√™n, fallback "m") + DOI
  3. N·∫øu thi·∫øu DOI: tra Crossref theo title
  4. V·ªõi DOI -> l·∫•y abstract (Crossref, arXiv)
  5. N·∫øu abstract v·∫´n null -> th·ª≠ multiple sources (Semantic Scholar, ResearchGate, etc.)
  6. Xu·∫•t JSON
"""

import requests, xml.etree.ElementTree as ET, json, time, re, unicodedata
from urllib.parse import quote, urljoin
from bs4 import BeautifulSoup
import random

UA_LIST = [
    "Academic Research Bot - Reference Pipeline v1.1 - Graduate Thesis Project (mailto:your-real-email@university.edu)",
    "Mozilla/5.0 (compatible; Academic Research; +mailto:your-real-email@university.edu) Reference Analysis",
    "Graduate Student Research Bot v1.1 - Thesis Project - Contact: your-real-email@university.edu"
]

API_UA = "Academic Research Pipeline/1.1 (Graduate Thesis Project; mailto:your-real-email@university.edu)"   # THAY EMAIL TH·∫¨T

def get_random_ua():
    return random.choice(UA_LIST)

# -------------------- GROBID -------------------- #
def extract_references_grobid(pdf_path, output_xml_path):
    url = "http://localhost:8070/api/processReferences"
    with open(pdf_path, 'rb') as f:
        r = requests.post(url, files={'input': (pdf_path, f, 'application/pdf')})
    if r.ok:
        with open(output_xml_path, 'w', encoding='utf-8') as out:
            out.write(r.text)
        print(f"‚úî ƒê√£ ghi file XML: {output_xml_path}")
        return True
    print(f"‚ùå L·ªói Grobid {r.status_code}")
    return False


# -------------------- TI·ªÜN √çCH DOI & TITLE -------------------- #
def _crossref_title(doi):
    try:
        r = requests.get(f"https://api.crossref.org/works/{quote(doi)}",
                         headers={'User-Agent': API_UA}, timeout=10)
        if r.ok:
            return r.json()['message']['title'][0]
    except Exception:
        pass
    return ''


def _slug(txt: str) -> str:
    txt = unicodedata.normalize("NFKD", txt).lower()
    return re.sub(r'\W+', '', txt)[:120]


def _fuzzy_match(gold, candidate, threshold=0.6):  # Gi·∫£m threshold t·ª´ 0.8 xu·ªëng 0.6
    """Ki·ªÉm tra ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa 2 title ƒë√£ ƒë∆∞·ª£c slug"""
    if not gold or not candidate:
        return False
    
    gold_words = set(gold.split())
    candidate_words = set(candidate.split())
    
    if not gold_words or not candidate_words:
        return False
    
    intersection = len(gold_words & candidate_words)
    union = len(gold_words | candidate_words)
    
    return (intersection / union) >= threshold if union > 0 else False


def _search_doi_by_title(title):
    """Tr·∫£ v·ªÅ DOI n·∫øu Crossref c√≥ b·∫£n ghi c√≥ title kh·ªõp ho√†n to√†n"""
    try:
        r = requests.get("https://api.crossref.org/works",
                         params={'query.title': title, 'rows': 5},
                         headers={'User-Agent': API_UA}, timeout=10)
        if not r.ok:
            return None
        gold = _slug(title)
        for item in r.json()['message']['items']:
            if item.get('title'):
                if _slug(item['title'][0]) == gold:
                    return item['DOI']
    except Exception:
        pass
    return None


# --------------- ARXIV ------------------ #
def _arxiv_title(arxiv_id):
    try:
        r = requests.get(f"https://export.arxiv.org/api/query?id_list={arxiv_id}",
                         timeout=10)
        if r.ok:
            root = ET.fromstring(r.text)
            t = root.find('.//{http://www.w3.org/2005/Atom}title')
            return (t.text or '').strip() if t is not None else ''
    except Exception:
        pass
    return ''


def _arxiv_abs(arxiv_id):
    try:
        r = requests.get(f"https://export.arxiv.org/api/query?id_list={arxiv_id}",
                         timeout=10)
        if r.ok:
            root = ET.fromstring(r.text)
            s = root.find('.//{http://www.w3.org/2005/Atom}summary')
            return (s.text or '').strip() if s is not None else None
    except Exception:
        pass
    return None


def _arxiv_abs_by_title(title):
    """T√¨m abstract t·ª´ arXiv b·∫±ng title v·ªõi fuzzy matching"""
    try:
        # Th·ª≠ search v·ªõi title ƒë·∫ßy ƒë·ªß
        r = requests.get("https://export.arxiv.org/api/query",
                         params={'search_query': f'ti:"{title}"', 'max_results': 5},
                         timeout=15)
        if r.ok:
            root = ET.fromstring(r.text)
            gold = _slug(title)
            
            for entry in root.findall('.//{http://www.w3.org/2005/Atom}entry'):
                entry_title = entry.find('.//{http://www.w3.org/2005/Atom}title')
                if entry_title is not None:
                    entry_slug = _slug(entry_title.text)
                    if entry_slug == gold or _fuzzy_match(gold, entry_slug):
                        summary = entry.find('.//{http://www.w3.org/2005/Atom}summary')
                        if summary is not None:
                            return summary.text.strip()
        
        # N·∫øu kh√¥ng t√¨m th·∫•y, th·ª≠ search v·ªõi keywords ch√≠nh
        title_words = title.lower().split()
        key_words = [w for w in title_words if len(w) > 3][:4]
        
        if key_words:
            search_query = ' AND '.join([f'ti:{word}' for word in key_words])
            r = requests.get("https://export.arxiv.org/api/query",
                             params={'search_query': search_query, 'max_results': 3},
                             timeout=15)
            if r.ok:
                root = ET.fromstring(r.text)
                gold = _slug(title)
                
                for entry in root.findall('.//{http://www.w3.org/2005/Atom}entry'):
                    entry_title = entry.find('.//{http://www.w3.org/2005/Atom}title')
                    if entry_title is not None and _fuzzy_match(gold, _slug(entry_title.text)):
                        summary = entry.find('.//{http://www.w3.org/2005/Atom}summary')
                        if summary is not None:
                            return summary.text.strip()
                            
    except Exception:
        pass
    return None


# --------------- CROSSREF ABSTRACT ---------------- #
def get_abstract_from_crossref(doi):
    try:
        r = requests.get(f"https://api.crossref.org/works/{quote(doi)}",
                         headers={'User-Agent': API_UA}, timeout=10)
        if r.ok:
            abs_raw = r.json()['message'].get('abstract')
            if abs_raw and 'withheld' not in abs_raw.lower():
                return re.sub(r'<[^>]+>', '', abs_raw).strip()
    except Exception:
        pass
    return None


def _search_abstract_by_title_crossref(title):
    """T√¨m abstract tr·ª±c ti·∫øp b·∫±ng title t·ª´ Crossref v·ªõi fuzzy matching"""
    try:
        r = requests.get("https://api.crossref.org/works",
                         params={'query.title': title, 'rows': 5},
                         headers={'User-Agent': API_UA}, timeout=10)
        if not r.ok:
            return None
        
        gold = _slug(title)
        
        # Th·ª≠ kh·ªõp ch√≠nh x√°c tr∆∞·ªõc
        for item in r.json()['message']['items']:
            if item.get('title') and _slug(item['title'][0]) == gold:
                abs_raw = item.get('abstract')
                if abs_raw and 'withheld' not in abs_raw.lower():
                    return re.sub(r'<[^>]+>', '', abs_raw).strip()
        
        # N·∫øu kh√¥ng kh·ªõp ch√≠nh x√°c, th·ª≠ fuzzy matching
        for item in r.json()['message']['items']:
            if item.get('title'):
                item_slug = _slug(item['title'][0])
                if _fuzzy_match(gold, item_slug):
                    abs_raw = item.get('abstract')
                    if abs_raw and 'withheld' not in abs_raw.lower():
                        return re.sub(r'<[^>]+>', '', abs_raw).strip()
                        
    except Exception:
        pass
    return None


# --------------- SEMANTIC SCHOLAR API ---------------- #
def get_abstract_from_semantic_scholar(title, doi=None):
    """L·∫•y abstract t·ª´ Semantic Scholar API v·ªõi enhanced debugging"""
    try:
        # Th·ª≠ search b·∫±ng DOI tr∆∞·ªõc n·∫øu c√≥
        if doi:
            print(f"        üîó Semantic Scholar DOI: {doi}")
            r = requests.get(f"https://api.semanticscholar.org/graph/v1/paper/DOI:{doi}",
                           params={'fields': 'abstract'},
                           headers={'User-Agent': API_UA}, timeout=15)
            
            print(f"        üì° Semantic Scholar DOI search: {r.status_code}")
            
            if r.ok:
                data = r.json()
                if data.get('abstract'):
                    print(f"        üí° DOI abstract length: {len(data['abstract'])}")
                    return data['abstract'].strip()
                else:
                    print(f"        ‚ÑπÔ∏è  DOI found but no abstract field - trying title search")
            elif r.status_code == 404:
                print(f"        ‚ÑπÔ∏è  DOI not found in Semantic Scholar")
            elif r.status_code == 429:
                print(f"        ‚è∞ Rate limited by Semantic Scholar - skipping title search")
                return None  # Skip title search if already rate limited
            elif r.status_code == 403:
                print(f"        üö´ BLOCKED by Semantic Scholar - IP may be banned")
                return None
            elif r.status_code >= 500:
                print(f"        üîß Semantic Scholar server error: {r.status_code}")
            else:
                print(f"        ‚ùå Semantic Scholar error: {r.status_code}")
        
        # Search b·∫±ng title - only if DOI search wasn't rate limited
        print(f"        üîç Semantic Scholar title: {title[:30]}...")
        r = requests.get("https://api.semanticscholar.org/graph/v1/paper/search",
                         params={'query': title, 'fields': 'abstract,title', 'limit': 5},
                         headers={'User-Agent': API_UA}, timeout=15)
        
        print(f"        üì° Semantic Scholar title search: {r.status_code}")
        
        if r.ok:
            data = r.json()
            results_count = len(data.get('data', []))
            print(f"        üìä Found {results_count} results")
            
            gold = _slug(title)
            
            for i, paper in enumerate(data.get('data', [])):
                if paper.get('title') and paper.get('abstract'):
                    paper_slug = _slug(paper['title'])
                    similarity = _fuzzy_match(gold, paper_slug)
                    print(f"        üéØ Result {i+1}: {similarity} similarity")
                    
                    if paper_slug == gold or similarity:
                        print(f"        ‚úÖ Title match found!")
                        return paper['abstract'].strip()
            
            print(f"        ‚ùå No matching titles found")
        elif r.status_code == 429:
            print(f"        ‚è∞ Rate limited by Semantic Scholar (title search)")
        elif r.status_code == 403:
            print(f"        üö´ BLOCKED by Semantic Scholar - IP may be banned")
        elif r.status_code >= 500:
            print(f"        üîß Semantic Scholar server error: {r.status_code}")
        else:
            print(f"        ‚ùå Semantic Scholar title search error: {r.status_code}")
                        
    except requests.RequestException as e:
        print(f"        üåê Network error: {str(e)[:100]}")
    except Exception as e:
        print(f"        üí• Unexpected error: {str(e)[:100]}")
    
    return None


# --------------- RESEARCHGATE SCRAPING ---------------- #
def get_abstract_from_researchgate(title):
    """Scrape abstract t·ª´ ResearchGate"""
    try:
        search_url = f"https://www.researchgate.net/search.Search.html?type=publication&query={quote(title)}"
        headers = {'User-Agent': get_random_ua()}
        
        r = requests.get(search_url, headers=headers, timeout=15)
        if not r.ok:
            return None
            
        soup = BeautifulSoup(r.text, 'html.parser')
        gold = _slug(title)
        
        # T√¨m c√°c k·∫øt qu·∫£ search
        for result in soup.find_all('div', class_=['nova-legacy-c-card__body', 'nova-legacy-v-publication-item__body']):
            title_elem = result.find('a', href=re.compile(r'/publication/'))
            if title_elem:
                result_title = title_elem.get_text(strip=True)
                if result_title and (_slug(result_title) == gold or _fuzzy_match(gold, _slug(result_title))):
                    # T√¨m abstract trong result n√†y
                    abstract_elem = result.find('div', class_='nova-legacy-v-publication-item__description')
                    if abstract_elem:
                        abstract_text = abstract_elem.get_text(strip=True)
                        if len(abstract_text) > 50:  # ƒê·∫£m b·∫£o kh√¥ng ph·∫£i snippet ng·∫Øn
                            return abstract_text
                            
    except Exception:
        pass
    return None


# --------------- GOOGLE SCHOLAR SCRAPING (FIXED) ---------------- #
def get_abstract_from_google_scholar(title):
    """Scrape abstract t·ª´ Google Scholar - c·∫©n th·∫≠n v·ªõi rate limiting"""
    try:
        search_url = f"https://scholar.google.com/scholar?q={quote(title)}"
        headers = {'User-Agent': get_random_ua()}
        
        r = requests.get(search_url, headers=headers, timeout=20)
        if not r.ok:
            print(f"        ‚ùå Google Scholar HTTP {r.status_code}")
            return None
            
        soup = BeautifulSoup(r.text, 'html.parser')
        gold = _slug(title)
        
        # T√¨m k·∫øt qu·∫£ trong Google Scholar
        for result in soup.find_all('div', class_='gs_ri'):
            title_elem = result.find('h3')
            if title_elem:
                result_title = title_elem.get_text(strip=True)
                if result_title and (_slug(result_title) == gold or _fuzzy_match(gold, _slug(result_title))):
                    # T√¨m abstract/snippet
                    abstract_elem = result.find('div', class_='gs_rs')
                    if abstract_elem:
                        abstract_text = abstract_elem.get_text(strip=True)
                        if len(abstract_text) > 50:
                            return abstract_text
                            
    except Exception as e:
        print(f"        ‚ùå Google Scholar error: {str(e)[:50]}")
    return None


# --------------- OPENALEX API ---------------- #
def get_abstract_from_openalex(title, doi=None):
    """L·∫•y abstract t·ª´ OpenAlex API - free academic database"""
    try:
        # Th·ª≠ search b·∫±ng DOI tr∆∞·ªõc n·∫øu c√≥
        if doi:
            print(f"        üîó OpenAlex DOI: {doi}")
            # OpenAlex format: https://doi.org/10.xxxx
            if not doi.startswith('http'):
                doi_url = f"https://doi.org/{doi}"
            else:
                doi_url = doi
                
            r = requests.get(f"https://api.openalex.org/works/{doi_url}",
                           headers={'User-Agent': API_UA}, timeout=15)
            
            print(f"        üì° OpenAlex DOI search: {r.status_code}")
            
            if r.ok:
                data = r.json()
                abstract = data.get('abstract_inverted_index')
                if abstract:
                    # Convert inverted index to text
                    words = [''] * (max(max(positions) for positions in abstract.values()) + 1)
                    for word, positions in abstract.items():
                        for pos in positions:
                            words[pos] = word
                    abstract_text = ' '.join(words).strip()
                    if len(abstract_text) > 50:
                        print(f"        üí° OpenAlex DOI abstract length: {len(abstract_text)}")
                        return abstract_text
                else:
                    print(f"        ‚ÑπÔ∏è  OpenAlex DOI found but no abstract")
            elif r.status_code == 404:
                print(f"        ‚ÑπÔ∏è  DOI not found in OpenAlex")
            else:
                print(f"        ‚ùå OpenAlex DOI error: {r.status_code}")
        
        # Search b·∫±ng title
        print(f"        üîç OpenAlex title search: {title[:30]}...")
        r = requests.get("https://api.openalex.org/works",
                         params={'search': title, 'limit': 5},
                         headers={'User-Agent': API_UA}, timeout=15)
        
        print(f"        üì° OpenAlex title search: {r.status_code}")
        
        if r.ok:
            data = r.json()
            results_count = len(data.get('results', []))
            print(f"        üìä Found {results_count} OpenAlex results")
            
            gold = _slug(title)
            
            for i, work in enumerate(data.get('results', [])):
                work_title = work.get('title', '')
                if work_title:
                    work_slug = _slug(work_title)
                    similarity = _fuzzy_match(gold, work_slug)
                    print(f"        üéØ OpenAlex Result {i+1}: {similarity} similarity")
                    
                    if work_slug == gold or similarity:
                        abstract = work.get('abstract_inverted_index')
                        if abstract:
                            # Convert inverted index to text
                            words = [''] * (max(max(positions) for positions in abstract.values()) + 1)
                            for word, positions in abstract.items():
                                for pos in positions:
                                    words[pos] = word
                            abstract_text = ' '.join(words).strip()
                            if len(abstract_text) > 50:
                                print(f"        ‚úÖ OpenAlex title match found!")
                                return abstract_text
            
            print(f"        ‚ùå No matching OpenAlex titles found")
        else:
            print(f"        ‚ùå OpenAlex title search error: {r.status_code}")
                        
    except Exception as e:
        print(f"        üí• OpenAlex error: {str(e)[:100]}")
    
    return None


def get_abstract_from_core(title):
    """L·∫•y abstract t·ª´ CORE API - open access repository"""
    try:
        print(f"        üîç CORE search: {title[:30]}...")
        r = requests.get("https://api.core.ac.uk/v3/search/works",
                         params={'q': title, 'limit': 5},
                         headers={'User-Agent': API_UA}, timeout=15)
        
        print(f"        üì° CORE search: {r.status_code}")
        
        if r.ok:
            data = r.json()
            results_count = len(data.get('results', []))
            print(f"        üìä Found {results_count} CORE results")
            
            gold = _slug(title)
            
            for i, work in enumerate(data.get('results', [])):
                work_title = work.get('title', '')
                if work_title:
                    work_slug = _slug(work_title)
                    similarity = _fuzzy_match(gold, work_slug)
                    print(f"        üéØ CORE Result {i+1}: {similarity} similarity")
                    
                    if work_slug == gold or similarity:
                        abstract = work.get('abstract')
                        if abstract and len(abstract.strip()) > 50:
                            print(f"        ‚úÖ CORE match found!")
                            return abstract.strip()
            
            print(f"        ‚ùå No matching CORE results")
        else:
            print(f"        ‚ùå CORE error: {r.status_code}")
                        
    except Exception as e:
        print(f"        üí• CORE error: {str(e)[:100]}")
    
    return None


# --------------- UNIFIED ABSTRACT RETRIEVAL (UPDATED) ---------------- #
def get_abstract_by_title(title, doi=None):
    """L·∫•y abstract b·∫±ng title - optimized order v√† better error handling"""
    if not title or title == "No title":
        return None
    
    print(f"      üîç Searching abstract for: {title[:50]}...")
    
    # Reordered sources - fast and reliable first
    sources = [
        ("OpenAlex", lambda: get_abstract_from_openalex(title, doi), 0.5),
        ("Crossref", lambda: _search_abstract_by_title_crossref(title), 0.3),
        ("arXiv", lambda: _arxiv_abs_by_title(title), 1),
        ("CORE", lambda: get_abstract_from_core(title), 1),
        ("Semantic Scholar", lambda: get_abstract_from_semantic_scholar(title, doi), 2),
        ("Google Scholar", lambda: get_abstract_from_google_scholar(title), 8),
        ("ResearchGate", lambda: get_abstract_from_researchgate(title), 4)
    ]
    
    for source_name, source_func, delay in sources:
        try:
            print(f"        üì° Trying {source_name}...")
            abstract = source_func()
            if abstract and len(abstract.strip()) > 30:
                print(f"        ‚úÖ Found from {source_name}")
                return abstract.strip()
            
            print(f"        ‚è±Ô∏è  Waiting {delay}s...")
            time.sleep(delay)
            
        except Exception as e:
            print(f"        ‚ùå {source_name} failed: {str(e)[:50]}")
            # Still wait on network errors to avoid hammering
            time.sleep(delay * 0.5)
            continue
    
    return None


# --------------- PARSER ------------------ #
def parse_grobid_xml(xml_path):
    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}
    root = ET.parse(xml_path).getroot()

    bib_entries, citation_candidates = {}, []

    for bibl in root.findall('.//tei:biblStruct', ns):
        bid = bibl.get('{http://www.w3.org/XML/1998/namespace}id')
        if not bid:
            continue

        # --- title ---
        t_a = bibl.find('.//tei:title[@level="a"]', ns)
        if t_a is not None:
            title = ''.join(t_a.itertext()).strip()
        else:
            t_m = bibl.find('.//tei:title[@level="m"]', ns)
            title = ''.join(t_m.itertext()).strip() if t_m is not None else "No title"

        # --- doi ---
        doi_elem = bibl.find('.//tei:idno[@type="DOI"]', ns)
        doi = None
        if doi_elem is not None and doi_elem.text:
            m = re.search(r'10\.\d+/.+', doi_elem.text.strip())
            if m:
                doi = m.group(0)

        # fallback title (arXiv / Crossref)
        if not title and doi and 'arxiv' in doi.lower():
            arxiv_match = re.search(r'arxiv[\.:/]?(\d+\.\d+)', doi.lower())
            if arxiv_match:
                title = _arxiv_title(arxiv_match.group(1))
        if not title and doi:
            title = _crossref_title(doi)
        if not title:
            title = "No title"

        bib_entries[bid] = {"title": title, "doi": doi, "abstract": None}
        citation_candidates.append(bid)

    return bib_entries, citation_candidates


# --------------- ENRICH ------------------ #
def enrich_entries(bib_entries):
    print("üîç B·ªï sung DOI v√† abstract t·ª´ multiple sources...")
    
    total_entries = len(bib_entries)
    doi_added = 0
    abs_from_doi = 0
    abs_from_title = 0
    failed_entries = []
    
    for i, (bid, entry) in enumerate(bib_entries.items(), 1):
        doi, title = entry['doi'], entry['title']
        print(f"  [{i}/{total_entries}] Processing: {bid}")

        # --- (1) N·∫øu CH∆ØA c√≥ DOI nh∆∞ng c√≥ title ‚Üí tra Crossref ---
        if not doi and title != "No title":
            doi_found = _search_doi_by_title(title)
            if doi_found:
                entry['doi'] = doi = doi_found
                doi_added += 1
                print(f"    ‚úî Found DOI by title: {doi}")

        # --- (2) L·∫•y abstract b·∫±ng DOI ---
        if doi:
            abs_text = get_abstract_from_crossref(doi)
            if not abs_text and 'arxiv' in doi.lower():
                arxiv_match = re.search(r'arxiv[\.:/]?(\d+\.\d+)', doi.lower())
                if arxiv_match:
                    abs_text = _arxiv_abs(arxiv_match.group(1))
            
            if abs_text:
                entry['abstract'] = abs_text
                abs_from_doi += 1
                print(f"    ‚úî Got abstract from DOI")

        # --- (3) N·∫øu v·∫´n ch∆∞a c√≥ abstract ‚Üí th·ª≠ multiple sources ---
        if not entry['abstract'] and title != "No title":
            abs_text = get_abstract_by_title(title, doi)
            if abs_text:
                entry['abstract'] = abs_text
                abs_from_title += 1
                print(f"    ‚úî Got abstract from title search")
            else:
                failed_entries.append((bid, title[:50] + "..." if len(title) > 50 else title))
                print(f"    ‚ùå No abstract found")

        # Progressive delay - tƒÉng d·∫ßn ƒë·ªÉ tr√°nh rate limit
        delay = min(1.5 + (i * 0.1), 3.0)  # Start 1.5s, max 3s
        time.sleep(delay)

    print(f"\nüìà Final Summary:")
    print(f"   DOI added by title search: {doi_added}")
    print(f"   Abstracts from DOI: {abs_from_doi}")
    print(f"   Abstracts from title search: {abs_from_title}")
    print(f"   Total abstracts: {abs_from_doi + abs_from_title}")
    print(f"   Success rate: {(abs_from_doi + abs_from_title)/total_entries*100:.1f}%")
    print(f"   Failed entries: {len(failed_entries)}")
    
    if failed_entries:
        print(f"\n‚ùå Entries without abstract:")
        for bid, title in failed_entries[:5]:
            print(f"   {bid}: {title}")
        if len(failed_entries) > 5:
            print(f"   ... and {len(failed_entries) - 5} more")


# --------------- OUTPUT ------------------ #
def create_output_structure(text_stub, bib_entries, citation_candidates):
    return {
        "text": text_stub or ["Sample text content"],
        "citation_candidates": citation_candidates,
        "bib_entries": bib_entries
    }


def save_json_output(data, path):
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"‚úî JSON saved ‚Üí {path}")


# --------------- MAIN -------------------- #
def main():
    pdf_path = "paper.pdf"
    xml_path = "references_grobid.xml"
    output = "references_with_abstracts.json"
    
    # B∆∞·ªõc 1: Grobid extraction
    print("üîÑ Step 1: Extracting references with Grobid...")
    if not extract_references_grobid(pdf_path, xml_path):
        print("‚ùå Grobid extraction failed. Exiting.")
        return
    
    # B∆∞·ªõc 2: Parse XML
    print("üîÑ Step 2: Parsing Grobid XML...")
    try:
        bib_entries, citation_candidates = parse_grobid_xml(xml_path)
        print(f"‚úî Parsed {len(bib_entries)} bibliography entries")
        print(f"‚úî Found {len(citation_candidates)} citation candidates")
    except Exception as e:
        print(f"‚ùå XML parsing failed: {e}")
        return
    
    if not bib_entries:
        print("‚ùå No bibliography entries found. Exiting.")
        return
    
    # B∆∞·ªõc 3: Enrich v·ªõi DOI v√† abstracts
    print("üîÑ Step 3: Enriching entries with DOI and abstracts...")
    try:
        enrich_entries(bib_entries)
    except Exception as e:
        print(f"‚ùå Enrichment failed: {e}")
        return
    
    # B∆∞·ªõc 4: T·∫°o output structure
    print("üîÑ Step 4: Creating output structure...")
    text_stub = ["This is extracted text content from the PDF document."]
    output_data = create_output_structure(text_stub, bib_entries, citation_candidates)
    
    # B∆∞·ªõc 5: Save JSON
    print("üîÑ Step 5: Saving results...")
    try:
        save_json_output(output_data, output)
        print(f"‚úÖ Pipeline completed successfully!")
        print(f"üìÑ Output file: {output}")
        
        # Statistics summary
        total_entries = len(bib_entries)
        entries_with_doi = sum(1 for e in bib_entries.values() if e['doi'])
        entries_with_abstract = sum(1 for e in bib_entries.values() if e['abstract'])
        
        print(f"\nüìä Final Statistics:")
        print(f"   Total entries: {total_entries}")
        print(f"   Entries with DOI: {entries_with_doi} ({entries_with_doi/total_entries*100:.1f}%)")
        print(f"   Entries with abstract: {entries_with_abstract} ({entries_with_abstract/total_entries*100:.1f}%)")
        
    except Exception as e:
        print(f"‚ùå Failed to save output: {e}")
        return


if __name__ == "__main__":
    print("üöÄ Academic Reference Pipeline v1.1")
    print("=" * 50)
    main()
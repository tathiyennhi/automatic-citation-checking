[
    {
        "original_sentence": "A survey by [8] details the progress of LLMs on IE tasks.",
        "citation_number": "[8]",
        "page_number": null,
        "citation_content": "",
        "reference": "Large language models for generative information extraction: A survey",
        "pdf_path": null
    },
    {
        "original_sentence": "Related work A common practice in many specialized IE tasks is that welltrained experts review what was extracted, and provide ground truth [4].",
        "citation_number": "[4]",
        "page_number": null,
        "citation_content": "",
        "reference": "Evaluating ChatGPT in Information Extraction: A Case Study of Extracting Cognitive Exam Dates and Scores",
        "pdf_path": null
    },
    {
        "original_sentence": "In [3] they suggest summary score without reference (SUSWIR), a score to evaluate the quality of text summaries without the need for human annotations.",
        "citation_number": "[3]",
        "page_number": null,
        "citation_content": "",
        "reference": "Who Needs External References?—Text Summarization Evaluation Using Original Documents",
        "pdf_path": null
    },
    {
        "original_sentence": "These prove themselves as highly costeffective data creators, either by labeling unlabeled data or generating data given the labels, see [6].",
        "citation_number": "[6]",
        "page_number": null,
        "citation_content": "",
        "reference": "Making large language models better data creators",
        "pdf_path": null
    },
    {
        "original_sentence": "Capturing the structure In this section, we explain how structured information can be obtained from a text document by an LLM. 3.1 Schema To impose a structure on the data, we adopt the idea of schema markup [2] which is used to communicate the content of a web page to the search tool.",
        "citation_number": "[2]",
        "page_number": null,
        "citation_content": "",
        "reference": "Schema and structured data markup",
        "pdf_path": null
    },
    {
        "original_sentence": "[8].",
        "citation_number": "[8]",
        "page_number": null,
        "citation_content": "",
        "reference": "Large language models for generative information extraction: A survey",
        "pdf_path": null
    },
    {
        "original_sentence": "When trying to use the maximal possible input another issue may appear – the Lost in the middle phenomenon [7] says that the ability of LLMs to retrieve information from a long context declines and that the attention focuses on the beginning and the end of the context while it tends to attenuate information in the middle.",
        "citation_number": "[7]",
        "page_number": null,
        "citation_content": "",
        "reference": "Lost in the middle: How language models use long contexts",
        "pdf_path": null
    },
    {
        "original_sentence": "Lost in the middle In the case of long documents, whose extraction consumes almost the whole context window, LLMs are giving more inconsistent results and we can observe a presence of the Lost in the middle phenomenon, see [7].",
        "citation_number": "[7]",
        "page_number": null,
        "citation_content": "",
        "reference": "Lost in the middle: How language models use long contexts",
        "pdf_path": null
    },
    {
        "original_sentence": "Therefore we adopt methods from [3].",
        "citation_number": "[3]",
        "page_number": null,
        "citation_content": "",
        "reference": "Who Needs External References?—Text Summarization Evaluation Using Original Documents",
        "pdf_path": null
    },
    {
        "original_sentence": "[5].",
        "citation_number": "[5]",
        "page_number": null,
        "citation_content": "",
        "reference": "In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss",
        "pdf_path": null
    },
    {
        "original_sentence": "To measure the quality of the summary we adopt the scores from [3] (a convex combination of these scores creates the overall SUSWIR metric), namely semantic similarity, relevance, and redundancy avoidance.",
        "citation_number": "[3]",
        "page_number": null,
        "citation_content": "",
        "reference": "Who Needs External References?—Text Summarization Evaluation Using Original Documents",
        "pdf_path": null
    },
    {
        "original_sentence": "We use a modified bias avoidance score from [3] and add two new scores, relevance spread, and incompleteness score.",
        "citation_number": "[3]",
        "page_number": null,
        "citation_content": "",
        "reference": "Who Needs External References?—Text Summarization Evaluation Using Original Documents",
        "pdf_path": null
    },
    {
        "original_sentence": "References [1] Satanjeev Banerjee and Alon Lavie.",
        "citation_number": "[1]",
        "page_number": null,
        "citation_content": "",
        "reference": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
        "pdf_path": null
    },
    {
        "original_sentence": "[2] Matthew Edgar.",
        "citation_number": "[2]",
        "page_number": null,
        "citation_content": "",
        "reference": "Schema and structured data markup",
        "pdf_path": null
    },
    {
        "original_sentence": "[3] Abdullah Al Foysal and Ronald B¨ock.",
        "citation_number": "[3]",
        "page_number": null,
        "citation_content": "",
        "reference": "Who Needs External References?—Text Summarization Evaluation Using Original Documents",
        "pdf_path": null
    },
    {
        "original_sentence": "12 [4] Neil Jethani, Simon Jones, Nicholas Genes, Vincent J Major, Ian S Jaffe, Anthony B Cardillo, Noah Heilenbach, Nadia Fazal Ali, Luke J Bonanni, Andrew J Clayburn, et al.",
        "citation_number": "[4]",
        "page_number": null,
        "citation_content": "",
        "reference": "Evaluating ChatGPT in Information Extraction: A Case Study of Extracting Cognitive Exam Dates and Scores",
        "pdf_path": null
    },
    {
        "original_sentence": "[5] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev.",
        "citation_number": "[5]",
        "page_number": null,
        "citation_content": "",
        "reference": "In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs Miss",
        "pdf_path": null
    },
    {
        "original_sentence": "[6] DongHo Lee, Jay Pujara, Mohit Sewak, Ryen W White, and Sujay Kumar Jauhar.",
        "citation_number": "[6]",
        "page_number": null,
        "citation_content": "",
        "reference": "Making large language models better data creators",
        "pdf_path": null
    },
    {
        "original_sentence": "[7] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.",
        "citation_number": "[7]",
        "page_number": null,
        "citation_content": "",
        "reference": "Lost in the middle: How language models use long contexts",
        "pdf_path": null
    },
    {
        "original_sentence": "[8] Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, and Enhong Chen.",
        "citation_number": "[8]",
        "page_number": null,
        "citation_content": "",
        "reference": "Large language models for generative information extraction: A survey",
        "pdf_path": null
    },
    {
        "original_sentence": "13 Appendix A To measure the quality of the summary we adopt the methods from [3]: semantic similarity combines latent semantic similarity and cosine similarity; relevance is measured using METEOR score, see [1], without chunk penalty; redundancy avoidance compares extracted entities among themselves using a threshold parameter – entities with a higher cosine similarity are assumed to be redundant; redundancy avoidance can be focused on a single particular property of entities (we use ’name’ as this pivotal property).",
        "citation_number": "[3]",
        "page_number": null,
        "citation_content": "",
        "reference": "Who Needs External References?—Text Summarization Evaluation Using Original Documents",
        "pdf_path": null
    },
    {
        "original_sentence": "13 Appendix A To measure the quality of the summary we adopt the methods from [3]: semantic similarity combines latent semantic similarity and cosine similarity; relevance is measured using METEOR score, see [1], without chunk penalty; redundancy avoidance compares extracted entities among themselves using a threshold parameter – entities with a higher cosine similarity are assumed to be redundant; redundancy avoidance can be focused on a single particular property of entities (we use ’name’ as this pivotal property).",
        "citation_number": "[1]",
        "page_number": null,
        "citation_content": "",
        "reference": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
        "pdf_path": null
    },
    {
        "original_sentence": "We modify the bias avoidance score from [3] to be where A represents the entities in the original text document and we normalize by a number of entities that were extracted, |B|.",
        "citation_number": "[3]",
        "page_number": null,
        "citation_content": "",
        "reference": "Who Needs External References?—Text Summarization Evaluation Using Original Documents",
        "pdf_path": null
    }
]
An Integrated, Conditional Model of Information Extraction and Coreference with Application to Citation Matching Ben Wellner∗†, Andrew McCallum∗, Fuchun Peng∗, Michael Hay∗  ∗University of Massachusetts Amherst  Amherst, MA 01003 USA  {wellner, mccallum, fuchun, mhay}@cs.umass.edu  †The MITRE Corporation 202 Burlington Road  Bedford, MA 01730 USA  ate hypotheses from both extraction and data mining  Although information extraction and coreference resolution appear together in many applications, most current systems perform them as independent steps.

This paper describes an approach to integrated inference for extraction and coreference based on conditionallytrained undirected graphical models.

We discuss the advantages of conditional probability training, and of a coreference model structure based on graph partitioning.

On a data set of research paper citations, we show significant reduction in error by using extraction uncertainty to improve coreference citation matching accuracy, and using coreference to improve the accuracy of the extracted fields.

can be easily communicated between extraction and data mining in a closed loop system, (b) mutuallyreinforcing evidence from multiple sources will have the opportunity to be properly marshaled, (c) and accuracy and confidence assessment should improve.

In particular, we advocate creating these joint models as conditional random fields (CRFs) (Lafferty et al., 2001) that have been configured to represent relational data by using parameter tying in repeated patterns based on the structure of the data—also known as relational Markov networks (Taskar et al., 2002).

In natural language processing, conditionallytrained rather than generativelytrained models almost always perform better because they allow more freedom to include a large collection of arbitrary, overlapping and nonindependent features of the input without the need to explicitly represent their dependencies, e.g., (Lafferty et al., 2001; Carreras et al., 2002;    Although information extraction (IE) and data mining appear together in many applications, their interface in most current systems would be better described as loose serial juxtaposition than as tight integration.

Information extraction populates slots in a database by identifying relevant subsequences of text, but is usually unaware of the emerging patterns and regularities in the database.

Data mining begins from a populated database, and is often unaware of where the data came from, or their inherent uncertainties.

The result is that the accuracy of both suffers, and significant mining of complex text sources is beyond reach.

To address this problem we have previously advocated (McCallum & Jensen, 2003) the use of joint probabilistic models that perform extraction and data mining in an integrated inference procedure—the evidence for an outcome being the result of simultaneously making inferences both “bottom up” from extraction, and  autocorrelation and other relations without concern for avoiding circularities (Taskar et al., 2002; Nevillle et al., 2004).

Both these modeling choices are in contrast to other related work in using directed, generativelytrained probabilistic models for information extraction (Marthi et al., 2003).

This paper presents a model, inference and learning procedure for a preliminary case of this extraction and data mining integration—namely information extraction and coreference on research paper citations.1Extraction in this context consists of segmenting and labeling the various fields of a citation, including title, author, journal, year, etc.

Coreference (also known as identity uncertainty, record linkage or object consol1We currently avoid calling this work a joint model of extraction and coreference, because we have not yet “closed the loop” by repeatedly cycling between extraction and coreference inference.

idation) is a key problem in creating databases created from noisy data.

For example, without properly resolving “Stuart Russell,” “S.

Russell,” and “Stuart Russel” to the same entity in a database, relational connections will be missing, and subsequent data mining will not find the patterns it should.

Using a data set of citations from CiteSeer (Lawrence et al., 1999), we present experimental results indicating that the type of integration we advocate does indeed hold promise—we show that modeling uncertainty about extraction of citation fields can improve coreference (in the face of different field orderings, abbreviations and typographic errors), and that leveraging predicted coreference can improve the extraction accuracy of these fields.

Measurements of bestcase scenarios show that there is yet further gain available to be found through this integration.

Certainly further gains are expected from experiments that close the loop between extraction and coreference, rather than the limited, separate bidirectional results prowithin the entity attributes is performed exactly by exhaustive search.

Across the three substructures, approximate inference is accomplished by variants of iterated conditional modes (ICM) (Besag, 1986).

More precisely, approximate inference in the entire model proceeds as follows: (1) for each citation, N segmentations with highest probability (Nbest lists) are found by a variant of Viterbi and provided to coreference; (2) coreference decisions are inferred by approximate graph partitioning, integrating out uncertainty about the sampled N segmentations; (3) these coreference decisions are used to infer the attributes of each paper by searching over all combinations of values suggested by each citation’s segmentations; and finally (4) inference of citation segmentations are revised to make themselves more compatible with their corresponding entity attributes.

Joint parameter estimation in this complex model is intractable, and thus, as in inference, we perform parameter estimation somewhat separately for each of    Building on earlier work in coreference that assumes perfect extraction (McCallum & Wellner, 2003), we cast coreference as a problem in graph partitioning based on Markov random fields (Boykov et al., 1999; Bansal et al., 2002).

The graphical model has cliques for pairs of citations, the logcliquepotential of which is the edge weight in the graph to be partitioned.

These edge weights may be positive or negative, and thus the optimal number of partitions (equivalent to number of cited papers in this case) falls out naturally from the optimization function of the maxflowmincut partitioning.

Later in this paper we provide statistical correlation results indicating that the redundancy in this “fullyconnected graph partitioning” approach to coreference is more robust than a graphical model in which a “prototype yields observations.” In the model introduced in this paper, the graphical model consists of three repeated substructures: (1) a linearchain representing a finitestate segmenter for the sequence of words in each citation (2) a boolean variable in a clique between each pair of segmented citations, representing graphpartitioningstyle citation coreference decisions, (3) a collection of attribute variables once for each paper entity (that is, one for each partition in the coreference graph), noting that the  iterative, consisting of BFGS quasiNewton steps on a maximum a posteriori conditional likelihood, with a zeromean sphericalvariance Gaussian prior on the parameters.

The parameters of the linearchain are set to maximize the conditional likelihood of the correct label sequence, in the traditional fashion for linearchain CRFs.

The parameters for the distance function in graph partitioning are set to maximize the product of independent conditional likelihoods for each pairwise coreference decision.

The parameters for the entity attributes are set by pseudolikelihood to maximize the likelihood of correct placement of edges between highestaccuracy citation segmentations and their true entity attributes.

We present experimental results on the four sections of CiteSeer citationmatching data (Lawrence et al., 1999).

Using our integrated model, both extraction and coreference show significant reductions in error—by 2535% for coreference and by 614% for extraction.

We also provide some encouraging bestcase experiments showing substantial additional potential gain that may come from more integrated joint inference and creation of additional features that leverage the capabilities of conditional probability models.

Model 1 in McCallum and Wellner (2003).

Inference within the linear chain is performed exactly by dynamic programming; inference within the fullyconnected coreference is performed approximately by a simple graph partitioning algorithm, and inference  This paper presents a method for integrated information extraction and coreference based on conditionallytrained, undirected graphical models—also known as conditional random fields (Lafferty et al., 2001).

The model predicts entities and their attributes conditioned on observed text.

UAI 2004 	WELLNER ET AL.

595 The model contains three types of repeated subindicating the membership of a citation word to a field structures with tied parameters.

These three sub-	(such as author, title or year).

As a convenience, we  structures are responsible for (1) information extraction, in the form of segmentation and labeling of word sequences in order to find (the fields of) each mention of an entity, (2) coreference among the mentions to discover when two mentions are referring to the same underlying entity, (3) representing the attributes of each entity and the dependencies among those attribute values.

The attributes of each entity correspond to the canonical values that could be entered into database record fields, and the dependencies allow the model to represent expectations about what combinations of attributes would be expected in the world.

In general, conditional random fields (CRFs) are undirected graphical models that encode a conditional probability distribution using a given set of features.

CRFs are defined as follows.

Let G be an undirected  also define citation fields c = {c1, ...cK}, where ci is a collection of variables containing the complete string value for each of the various fields of xi, deterministically agglomerated from the label sequence si.

Let y = {y1,2, ..yi,j, ...yK−1,K}, i < j be a set of boolean coreference variables indicating whether or not citation xi is referring to the same paper as citation xj; (note y here is more specific than in Eq 1).

Finally, let a = {a1, ...aM} be the set of attributes of each paper (“entities”), where M is the number of underlying research paper entities.

Here, entity attributes are field values, such as title and year, but canonicalized from their noisy appearance in the multiple coreference citation mentions.

As described above, the model consists of three repeated substructures: (1) a linearchain on the eleC = {{yc, xc}} is the set of cliques in G, then CRFs define the conditional probability of an output labeling, y given the observed variables, x as:  quences, for finitestate citation segmentation and labeling (information extraction); (2) a fullyconnected graph on the xi’s, with the binary coreference decitransitivity, but these potentials never actually have to   y 	c∈CΦ(yc, xc) is a normalization factor.

We assume the potentials factorize according to a set of features {fk}, which are given and fixed, so that Φ(yc, xc) = exp( 	kλkfk(yc, xc)).

The model parameters are a set of realvalued weights Λ = {λk}, one  potentials measuring the compatibility between each mention’s segmentation ci and the attributes of its corresponding entity ak.

This model is closely related to a combination of Models 1 and 2 in McCallum and Wellner (2003), where further details and background    CRFs have shown recent success in a number of domains especially in sequence modeling for natural language tasks (Lafferty et al., 2001; Sha & Pereira, 2003; Pinto et al., 2003; McCallum & Li, 2003; Sutton et al., 2004), often outperforming their generative counterthe mode of P(a|x) = c,s,yP(a, y, c, s|x), where the summed term is defined in Equation 2 by a product of potentials on cliques of the graphical model.

Figure 1 shows an example graphical model with two coreferent citations and a singleton.

be a set of observed citations (“mentions”), where each xi = (xi1, xi2, ...) is a sequence of words forming the text of the citation.

Let s = {s1, ...sK} be the corresponding set of label sequences, each label sequence, si  Exact inference in this model is clearly intractable.

We have, however, some clearly defined substructures within the model, and there is considerable previous work on inference by structured approximations—that   tion is integrated into coreference.) Thus, the graphof which are coreferent.

Joint inference over all coreference decisions involves finding the mode of    Here we experiment with a particularly simple form of approximate inference: structured variants on iterated conditional modes (ICM) (Besag, 1986).

In ICM, inference is performed by maximizing posterior probability of a variable, conditioned on all others, and repeatedly iterating over all variables.

In its structured form, (possibly exact) inference may be performed on entire substructures of the model rather than a single variable, e.g.

(Ying et al., 2002).

In this paper we also use a variant we term iterated conditional sampling, in which, rather than selecting the single assignment of variables with maximum probability, several assignments are sampled (although not necessarily randomly) from the posterior and made available to subsequent inference.

We expect that doing so makes the procedure less sensitive to local minima.

These samples can also be understood as a strong compression of the exponentiallysized conditional probability tables that would have been sent as messages in structured belief propagation.

Inference in our model is performed as follows.

First exact inference is performed independently for each label sequence si, conditioned on its corresponding citation word sequence xi using the Viterbi algorithm.

Rather than selecting the single highest probability si (mode), however, we find the Nbest list of label sequences (sample).

The citation fields ci are set deterministically from the sampled label sequences si.

Then coreference is accomplished by approximate inference via a greedy graph partitioning algorithm on the yij’s conditioned on the citation fields ci, as described in McCallum and Wellner (2003), except that the edge weights in the graph are determined by “inThis problem is an instance of correlation clustering, which has sparked recent theoretical interest (Bansal et al., 2002; Demaine & Immorlica, 2003).

Here we use a different approach to graph partitioning that bears more resemblance to agglomerative clustering — suitable for larger graphs with a high ratio of negative to positive edges.

We search through this space of possible partitionings using a stochastic beam search.

Specifically, we begin with each citation in its own cluster and select k pairs of clusters to merge with probability proportional to the edgeweight between the two clusters.

We examine each of these k possible merges until one is found that results in an increase in the objective function.

We repeat this until we reach a stage where none of the k candidate merge pairs would result in an increased objective function value.

The final step to consider in our ICMbased inference is estimation of the attributes on entities, a, and a revisitation of citation segmentation given coreference and these entity attributes.

Although our current experiments do not use entity attributes to affect coreference directly, they could do so by creating entity variables on the fly as coreference decisions are hypothesized.

We seek to maximize The segmentations s and citation fields c are selected only among the Nbest segmentations, and the entity     attributes are selected among the Nbest citation fields of the coreferent citations for a given entity.

Fortunately, there are few enough combinations that exact  common authors, publication venues, publishers, etc.

The size of coreferent citation clusters has a skewed distribution; in three of the four subsets, at least 70%    there are a maximum of 13 citation fields, (although typically much fewer), 221 citations in a coreferent cluster, and N was 5 or less.

Given the attributes of an entity, we compute scores for all (entity, segmentation) pairs.

The segmentation with highest score is selected as the best segmentation for the citation.

These entity attributes are also scored by summing these highest scores for all citations.

In the end, the entity attributes with the highest score are chosen as the canonical citation for this cluster and best segmentations are selected based on these attributes.

cluster consists of 21 citations to the same paper.

We present results on two different sets of experiments.

First, we consider the coreference component of the model, which takes as input a sample of the Nbest segmentations of each observation.

We compare its performance to coreference in which we assume perfect labeling and in which we use no labeling at all.

Second, we consider the segmentation performance of the model, which takes as input the citation clusters produced by the coreference component.

We compare its accuracy to the baseline performance consisting of    merically climbing the gradient of the full, joint likelihood.

This approach is not practical because complete inference in this model is intractable.

In addition, our previous experience with coreference (McCallum & Wellner, 2003) indicates that learning parameters by maximizing a product of local marginals, i<jP(yij|xi, xj), provides equal or superior accuracy to stochastic gradient ascent on an approximation of the full joint likelihood.

Good segmentation of author, title and other fields enables features that are naturally expected to be useful to accurate coreference.

The pairwise coreference potentials are a function of a wide range of rich, overlapping features.

The features largely consider fieldlevel similarity using a number of string and tokenbased comparison metrics.2  Briefly, these metrics include various string edit distance measures, TFIDF over tokens, TFIDF over character ngrams as well hybrid  Following this success, 	here we train each submethods that combine token TFIDF and string edit structure of the model separately, either as strucdistance.

We also used feature conjunctions (e.g.

a tured pseudolikelihood, 	or simply independently.

feature that combines the author and title similarity  The parameters of the linearchain CRF’s potentials,φ3(si(t−1), sit, cixi), are set to maximize the joint  measures).

Some specialized features were developed for matching and normalizing author name fields as    We employ feature induction as part of this training (McCallum, 2003).

The parameters of the coreference potentials on pairs of citations, φ2(ci, cj, yij),  cluded “global” features, based on string and tokenbased distance metrics, that looked at the entire citation.

The features are a mix of realvalued and binaryare set to maximize the product of local likelihoods 	valued functions.

of each pair, 	i<jP(yij|xi, xj).

The parameters of entityattribute/citation potentials, φ1(ai, cj), are set by pseudolikelihood to maximize the likelihood of correct placement of edges between citations and their true entity attributes.

A spherical Gaussian prior with zero mean is used in all cases.

We measure coreference performance at the pairlevel and clusterlevel.

We report pairwise F1, which is the harmonic mean of pairwise precision and pairwise recall.

Pairwise precision is the fraction of pairs in the same cluster that are coreferent; pairwise recall is the fraction of coreferent pairs that were placed in the  the number of true clusters.

Note that cluster recall  To evaluate our model, we apply it to a citation dataset from CiteSeer (Lawrence et al., 1999).

The dataset contains approximately 1500 citations to 900 papers.

The citations have been manually labeled for coreference and manually segmented into fields, such as author, title, etc.

The dataset has four subsets of citations, each one centered around a topic (e.g.

reinforcement learning).

Within a section, many citations share  gives no credit for a cluster that is partially correct.

Table 1 summarizes coreference performance in terms of pairwise F1 and cluster recall respectively.

Results reported are on the indicated test section; the model was trained on the other three sections.

We report the 2We used the Secondstring package, some of the functions of which are described in (Cohen et al., 2003)     Table 1: coreference performance measured by pairwise F1 (upper part) cluster recall (lower part) usfrom (Pasula et al., 2003).

tations (Labeled).

The Optimal result represents an upperbound where the optimal pairwise potential is chosen by an oracle.

performance of our model using the N -best Viterbi segmentations for different values of N.

Overall best  In leveraging coference to improve extraction, we use a combination of local (e.g.

word contains digits), layout, lexicon membership features (e.g.

membership in a database of Bibtex records).

See (Peng & McCallum, 2004) for a description of features.

Segmentation performance is measured by the microaveraged F1 across all fields, which approximates the accuracy    include the coreference performance when we include no segmentation information (NoSeg) and rely solely on the “global” features.

model was trained on a completely separate data set of citations (Peng & McCallum, 2004).

Table 3 shows the improved segmentation performance  Also included in the tables is the coreference perforusing coreference information.

The results reported mance when the handlabeled segmentation is prohere only consider citations that were grouped tovided (Labeled).

Note that the results using the N = 9 	gether with at least one other citation (i.e.

nonViterbi segmentations are comparable to or higher than those using the correctly labeled segmentations—indicating that neither segmentation performance nor our technique for incorporating segmentation uncertainty are the inhibiting factor in improving coreference performance.

As an upperbound experiment, we evaluate coreference performance assuming the model always chooses  singletons), since these are the only citations whose segmentation we might hope to improve by using coreference.

To test the significance of the improvements, we use McNemar’s test on labeling disagreements (Gillick & Cox, 1989).

Table 3 summarizes the significance test results.

At the 95% confidence level (pvalue smaller than 0.05), the improvements on the four datasets are significant.

line, we include the results of their implementation of the Phrase matching algorithm—a greedy agglomerative clustering algorithm where pairwise citation similarity is based on the overlap in words  We also explore the potential for improving segmentation performance by selecting among the Nbest segmentations.

For a given list of N segmentations, we  and phrases (word bigrams).

RPM + MCMC is 	aim to select the segmentation closest to the true segtheir firstorder, generativelytrained graphical model 	mentation.

The results in Table 4 show the optimal (see Related Work section).

These results are not 	segmentation performance for different values of N.

We can see that there is further potential to improve segmentation based on optimally selecting segmentaWe then compute the objective function values of these partitionings according to both the pairwise and    Table 4: Optimal segmentation improvement for different values of N over all citations (including singlefield values from among the citations in the partition.

(A medoid is the item in a cluster that has the mintons).

imum dissimilarity to all other items in the cluster— in this case, the similarity metric is a stringedit disOur model consists of both explicit pairwise coreference variables for each pair of citations, as well as explicit entity attribute variables for each group of coreferent citations.

At inference time in our current experiments, however, coreference is driven solely by the pairwise citation potentials.

An alternative method would ignore the pairwise potentials and consider only entitycitation potentials, creating entities at inference time as necessary.

If given a uniform prior over the number of entities, such a method would always chose to have a separate entity for each citation, and we must include a prior that prefers smaller numbers of entities (or equivalently, a penalty for generating each entity).

Thus, the number of entities induced is a function of the “tension” between the entitycitation potentials (which depend on the observed citation strings, be highly parameterized, and be learned) and the prior (which do not).

Intuitively, we wonder if this imbalance in exFor the randomly generated partitionings, we examine how well the probabilities for both models correlate (r2correlation) with the Pair F1 evaluation metric; see Table 5.

Figure 2 shows scatter plots for the Constraint data set illustrating the correlation between the model objective function values and Pair F1.

We hypothesize that the pairwise model correlates better partly due to the fact that the noise in edge potentials is ameliorated by averaging over n potentials for each citation (one for each other citation) instead of only a single potential as is the case with the entitybased model.

Perhaps even though the edge potentials in the entitybased model are expected to be less noisy (because the entity attributes are more “canonical”), averaging in the pairwise model is still more robust.

Further work is needed here to determine the effect of the quality of the generated entity attributes and learned potentials on the model’s performance.

naturally from the tension between positive and negadependency on the observed citation strings.

Also the  compatibility between a set of hypothesized coreferent mentions is represented by a “mixture” rather than a single “prototype.” To explore these issues, we compare the two models using a randomlygenerated sample of 100 partitionings of our citation dataset.

Here we focus on the robustness of clustering coreferent citations; to remove the issues of tuning a prior over the number of entities, all randomlygenerated partitions were constrained to have the correct number of clusters.

This paper is an example of integrating information extraction and data mining, as discussed in McCallum and Jensen (2003).

Additional previous work in this area includes Nahm and Mooney (2000), in which data mining association rules are applied to imperfectlyextracted job posting data, and then used to improve the recall of subsequent information extraction.

Our work here is most related to the work of Pasula et al. (2003), who describe a firstorder probabilistic model for citation segmentation and coreference.

From   UAI 2004 	WELLNER ET AL.

601 not necessarily reflect those of the sponsor.

We are also 	Milch, B., Marthi, B., & Russell, S.

(2004).

Blog: Regrateful to Charles Sutton and Brian Milch for helpful comlational modeling with unknown objects.

ICML 2004 ments on a previous draft and David Jensen for helpful 	Workshop on Statistical Relational Learning and Its discussions.

Connections to Other Fields.

Nahm, U.

Y., & Mooney, R.

J.

(2000).

A mutually benefi


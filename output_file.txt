BIORAG: A RAGLLM Framework for Biological Question Reasoning Chengrui Wang1,2, Qingqing Long1,2, Meng Xiao1,2, Xunxin Cai1,2, Chengjun Wu1,2, Zhen Meng1,2, Xuezhi Wang1,2, Yuanchun Zhou1,2*  1Computer Network Information Center, Chinese Academy of Sciences.

2University of the Chinese Academy of Sciences.

{crwang,qqlong,shaow,xxcai,cwu,zhenm99,wxz,zyc}cnic.cn   *Yuanchun Zhou is the corresponding author.

et al., 2019), and large language models tailored for    specific domains, such as PMCLlama (Wu et al., 2024) and Llavamed (Li et al., 2024).

These models are trained on domainspecific corpora, thereby embedding deep domain knowledge within their architectures.

However, that embedded knowledge could be incomplete and computationally expensive to update.

RetrievalAgumented Generation methods follow the information indexing and retrieval, information augmentation, and answer generation paradigm.

For instance, PGRA (Guo et al., 2023) adopts a retriever to search and reranking the context, then generate the answer.

Later research has aimed to improve these systems by either optimizing the retrieval processes using prior answers (Wang et al., 2023), enhancing model functionality through iterative feedback cycles (Liu  pora.

BIORAG then addresses the complexity of biological knowledge systems by combining a prebuilt research hierarchy with an embedding model for accurate context retrieval.

To cope with emerging biology knowledge, BIORAG can adaptively select knowledge sources from search engines, existing domainspecific tools, or indexed research articles.

Once the framework determines that it has gathered sufficient information, it will generate the answer based on the reasoned material.

We illustrate the questionreasoning power of BIORAG on 6 popularly used biology QA datasets and compare it against 6 baseline methods.

Extensive case studies show the great potential to apply this framework to general science questionreasoning scenarios.

methods address the issue of updating information, they often oversee the intricate complexities inherent in the domain knowledge of biology.

Based on the aforementioned discussion, we summarize three challenges in building efficient biology questionreasoning systems: (C1) The scarcity of highquality domainspecific corpora.

While biological research publications are abundant, there remains a significant void in the availability of extensive, highquality datasets to build robust information indexing models.

(C2) The inherent complexity of biological knowledge systems.

This complexity is compounded by the interdisciplinary nature of modern biological research.

Consequently, automated questionreasoning systems must be able to understand and process multifaceted and often ambiguous biological query.

(C3) The continual updating of knowledge.

Biology is a dynamic field where discoveries are frequently made, and existing theories are regularly revised or replaced.

This fluidity necessitates that questionreasoning systems adeptly select the knowledge source from databases or contemporary search engines to reflect the correct scientific understanding.

Our Perspective and Contributions: To solve the above challenges, we proposed BIORAG, a novel RetrievalAugmented Generation framework integrated with Large Language Models for biological questionreasoning.

To obtain a robust domainspecific information indexing embedding model, we start by parsing, indexing, and segmenting extensive research articles from the biology domain and constructing highquality training corIn this paper, we propose the Biological RetrievalAugmented Generation LLM Framework, namely BIORAG (as shown in Figure 2).

In the following sections, we first introduce the preliminary step of constructing a highquality local information source and training the biological domainspecific information indexing embedding model.

For questions that require the most current or other domainrelated data, we introduce external information sources.

Then, we demonstrate the knowledge hierarchybased query preprocessing, retriever execution component, and how the model iteratively collects sufficient information.

Finally, the large language model will generate the answer based on the information obtained.

The details of customized prompts are given in Section 2.4.

2.1 Internal Biological Information Source Highquality domainspecific corpora are crucial for enriching the information source and enhancing the embedding model in the context of biological questionreasoning systems.

To achieve this goal, we extract research papers from the global biomedical article database maintained by the National Center for Biotechnology Information1(NCBI) (Schoch et al., 2020).

This extensive repository aggregates over 37 million scientific citations and abstracts spanning from the 1950s to the present, encompassing a broad array of biomedical fields, including clinical medicine, molecular biology, etc.

For the purposes of this study, we utilize the abstracts from 1    Figure 2: The architecture of our proposed BIORAG framework.

The pipeline consists of five iterative components designed to enhance the process of biological questionreasoning: ①Retriever Selection aims to choose the most ideal information source; ②Query Preprocessing aims to rewrite the query and find closed topic tag from predefined knowledge hierarchy; ③Retriever Execution aims to combination retrieve the correlated context from knowledge base; ④SelfEvaluation assess the adequacy of the retrieved information and decides whether to cycle through additional retrieval tools or to move to the next phase; ⑤Inference and Generation uses the information gathered to generate an informed and accurate answer to the biological query.

these PubMed papers as the supporting corpus for the BIORAG framework.

Local Data Preparation: Specifically, we initially downloaded over 37 million original papers from which we subsequently filtered out 14 million entries deemed to be of low quality.

The preprocessing of these texts was conducted using the Unstructured tool2, specifically designed to ingest and preprocess unstructured textual data effectively.

Our filtration process involved the removal of gibberish using regular expression techniques, as well as the exclusion of nonsemantic content such as hyperlinks, charts, tables, and other embedded tags.

This meticulous process yielded a corpus of 22,371,343 highquality, processed PubMed abstracts.

Information Indexing: To further refine the retrieval performance of abstracts tailored to specific biological questions, we developed a specialized biological embedding model within the BIORAG framework.

This model employs PubMedBERT (Gu et al., 2021) as the foundational model.

We enhanced this model using the CLIP (Contrastive LanguageImage Pretraining) technique (Li et al., 2021; Nussbaum et al., 2024), allowing us to finetune the model, denoted as Memb.

Based on this, we constructed a local, highquality biological vector database (Xian et al., 2024) to support efficient and effective query processing and re2  trieval operations.

This database serves as a critical resource in facilitating rapid and accurate access to relevant biomedical information, significantly advancing the capabilities of our BIORAGframework in handling complex biological questions.

2.2 External Information Sources External biology knowledge is crucial to biological reasoning due to the rapidly evolving nature of biological research, which continuously integrates new discoveries.

To address this challenge, we introduce two external information sources.

Biological Data Hub: In BIORAG, we harness several specialized biological Hubs to ensure the accuracy of experimental data and to provide detailed biological insights.

Specifically, BIORAGintegrates the following databases, each serving a unique purpose in the broader context of biological analyses: (1) Gene Database3: This resource provides comprehensive information on the functions, structures, and expressions of specific genes.

It is invaluable for addressing queries related to gene mechanisms, gene actions, and gene expressions, facilitating a deeper understanding of generelated phenomena.

(2) dbSNP Database4: This database houses a vast repository of single nucleotide polymorphisms (SNPs), offering critical insights into genetic variants and their potential as34    sociations with various diseases.

It is instrumental for studies exploring the genetic basis of disease and trait inheritance.

(3) Genome Database5: Providing complete genome sequences, this database is essential for studying the structure, function, and evolution of genomes across different organisms.

It supports comprehensive genomic analyses and comparative studies, enhancing our understanding of genomic architecture and its functional implications.

(4) Protein Database6: This resource offers detailed information about the sequences, structures, and functions of proteins.

It is crucial for exploring proteinrelated biological processes, understanding molecular functions, and investigating the complex interactions within the proteome.

Search Engine: To ensure access to the most current discussions and developments, BIORAG incorporates a variety of search engines, including Google, Bing, arXiv, Wikimedia, and Crossref.

Each platform contributes uniquely to the aggregation of information: (1) Google and Bing: These search engines scour the web for a diverse range of content, including news articles, blogs, and forums, providing insights into public discussions and concerns related to scientific topics.

This breadth of information is crucial for understanding the societal impact and general discourse surrounding scientific issues.

(2) arXiv: As a repository for preprint papers, arXiv offers access to the latest research reports and scholarly articles across multiple scientific disciplines before they undergo peer review.

This source is invaluable for staying abreast of the newest scientific theories and experiments.

(3) Wikimedia: Known for its userfriendly content, Wikimedia offers easily digestible explanations of complex scientific concepts and principles.

This resource helps simplify advanced topics for broader public understanding and educational purposes.

(4) Crossref: This service acts as a comprehensive aggregator of academic citation data, providing links to peerreviewed scholarly publications and their citation networks.

Crossref is essential for accessing highquality research outputs and understanding their impact on the academic community.

external information source, BIORAG is firstly tasked with comprehending the complex disciFollowing the construction of the internal and 56  Figure 3: Training Template for MMeSH.

Figure 4: An example of MeSH filtering SQLs Generation.

plinary framework of the life sciences to retrieve the most relevant information accurately.

Moreover, BIORAG integrates a selfevaluation mechanism to continuously assess the adequacy and relevance of the information it has collected.

Internal Information Retrieve: To effectively navigate the inherent complexity of biological knowledge systems, BIORAG leverages an integrated approach, combining a welldefined hierarchical structure with indexed information to conduct a comprehensive internal information retrieval.

The Medical Subject Headings7(MeSH) thesaurus is popularly used for indexing, cataloging, and searching for biomedicalrelated information and research papers.

Specifically, we first train a model MMeSH to predict MeSH of the input questions.

We then use the templates in Figure 3 for finetuning a Llama38B model to classify given questions.

After that, we construct MeSH filtering SQLs (as shown in Figure 4) to generate the scalar condition retrieval.

A candidate result is considered relevant to the given question because it has one consistent MeSH with the question.

Then, the vector retrieval process is adopted to sort the relative results based on the cosine similarity of the sentence embedding between the input questions and the filtered results.

Selfevaluation Strategy: In order to ensure the accuracy and contemporary of the retrieved information, BIORAG incorporates a selfevaluation strategy that assesses the adequacy of data collected from the internal knowledge base.

In detail, this 7   Nomenclature Genomic location Functional analysis Table 1: Performance of BioRAG compared to other RAGLLMs on the GeneTuring QA dataset.The scores represent accuracy.

Table 2: Performance of BioRAG compared to other RAGLLMs on the biologicalrelated QA benchmarks.The scores represent accuracy.

Bold and underlined results denote the highest and secondhighest performance, respectively.

critical evaluation is driven by the backend large language model which aims to determine whether the information retrieved internally is sufficient to address the posed question substantively.

If the internal content is insufficient, the model will loop back to pertinent external knowledge sources.

Additionally, when the initial assessment indicates that the scientific questions require broader searches or retrieval of entityspecific data, the model tends to deploy external tools.

This methodology supports the framework’s goal of providing precise, uptodate, comprehensive answers, facilitating more informed decisionmaking, and advancing research and applications in the life sciences.

2.4 	Customized Prompts Detail To maximize the effect of the retrieved corpus and knowledge, we design customized prompts in BIORAG.

The prompts in Figure.

2 is detailed defined as follows, • Prompt # 1: To provide the most helpful and 	accurate response to the following Question: 	{Question}.

You have been given descriptions of several RETRIEVAL METHODS: 	{Retrieval}.

Please select the RETRIEVAL  METHODS you consider the most appropriate for addressing this question.

• Prompt # 2: Based on the RETRIEVAL METHODS you selected, and considering the Question and the Input Requirements of the retrieval method, please REWRITE the search query accordingly.

• Prompt # 3: 	Now, using the rewritten 	QUERY and the retrieval FILTER methods, 	perform a logical combination to execute the 	search effectively.

• Prompt # 4: Based on the RETRIEVAL RESULTS from the above steps, please evaluate whether the RESULTS support answering the original Question.

If they do not support it, output "NO".

If they do support it, output "YES".

• Prompt # 5: Based on the RETRIEVAL RESULTS, perform a comprehensive reasoning and provide an answer to the Question.

Furthermore, we designed instruction manuals for specialized biological tools and databases, aim    at exploiting their potentialities.

These instructions are shown as follows, • Manual #Gene: The Gene database search engine is a valuable tool for retrieving comprehensive information about genes, including gene structure, function, and related genetic events.

It is particularly useful for answering detailed questions regarding generelated research and findings.

To utilize this search  • Manual #PubMed: The PubMed local vector database search engine is an advanced tool designed for retrieving biomedical literature and research articles using vectorbased search techniques.

It is particularly useful for answering detailed questions about medical research, clinical studies, and scientific discoveries.

To utilize this search engine effectively, the input should be a specific query or topic of interest.

• Manual #dbSNP: The dbSNP database search engine is an essential tool for retrieving detailed information about single nucleotide polymorphisms (SNPs) and other genetic variations.

It is particularly useful for answering questions related to genetic diversity, allele frequency, and related genetic studies.

To utilize this search engine effectively, the input must be a specific SNP identifier or genetic variant name.

• Manual #Genome: The Genome database search engine is an indispensable tool for accessing comprehensive information about entire genomes, including their sequences, annotations, and functional elements.

It is particularly useful for answering complex questions about genomic structures, variations, and comparative genomics.

To use this search engine effectively, the input must be a specific genome name or identifier.

• Manual #Protein: 	The Protein database search engine is a crucial resource for obtaining detailed information about proteins, including their sequences, structures, functions, and interactions.

It is particularly useful for answering questions related to protein biology, biochemical properties, and molecular function.

To use this search engine effectively, the input must be a specific protein name or identifier.

• Manual #Web Search: The Web Search Engine is a powerful tool designed to help you find information about current events quickly and efficiently.

It is especially useful for ob3.1 Datasets We conduct experiments on 6 popularly used biologicalrelated QA datasets to evaluate our proposed BIORAG, i.e., GeneTuring (Hou and Ji, 2023), MedMCQA (Pal et al., 2022), Medical Genetics (Hendrycks et al., 2020), College Biology (Hendrycks et al., 2020), College Medicine (Hendrycks et al., 2020).

Note that the GeneTuring dataset contains more specialized biological questions.

It contains 12 tasks, and each task has 50 questionanswer pairs.

We use 7 GeneTuring tasks that are related to NCBI resources to evaluate the proposed BIORAG.

The chosen tasks are classified into three modules and briefly described as follows, • Nomenclature: This is about gene names.

The objectives of the gene alias task and name conversion task are finding the official gene symbols for their nonofficial synonyms.

• Genomics location: The tasks are about the locations of genes, singlenucleotide polymorphism (SNP), and their relations.

We include the gene location, SNP location, and gene SNP association tasks.

The first two tasks ask for the chromosome locations of a gene or an SNP, and the last one asks for related genes for a given SNP.

• Functional analysis asks for gene functions.

We use the genedisease association task where the goal is to return related genes for a given disease, and the proteincoding genes task which asks whether a gene is a proteincoding gene or not.

this search engine effectively, simply enter a relevant search query.

We compare BIORAGwith various baselines, which can be classified into three categories,   Nomenclature Genomic location Functional analysis Table 3: Ablation study on the GeneTuring dataset.The scores represent accuracy.

• LLM (General LLMs): We select GPT3.5Turbo (175B), Llama38B (8B), Llama70B (70B) as representative baselines.

• BioLLM (Biological LLMs): PMCLlama (13B) (Wu et al., 2024) and BioMistral (7B) (Labrak et al., 2024) are two medical LLMs.

They are pretrained on opensource biomedical texts.

• SciRAG (Scientific RAGLLM framework): GeneGPT (175B) (Jin et al., 2024) is a biological RAGLLM framework that integrates the NCBI databases, i.e., Gene, dbSNP, OIMI, and Blast.

NewBing8(>400B) is a retrievalaugmented LLM that has access to relevant web pages retrieved by Bing.

3.3 	Experimental Settings We take the Llama370B as the basic language model of BIORAG.

For our embedding model Memb, we take AdamW as the optimizer and finetune 2 epochs.

The number of retrieved results by biological databases, search engines, and local PubMed databases are set to 10, 10, and 4, respectively.

The max iteration of selfevaluation is set to 15.

If the model does not output the final answer within 15 times, BIORAG stops the iteration and outputs the current wrong answer.

We use the accuracy to verify the overall performance.

For the GeneTuring dataset, we only consider exact matches between model predictions and the ground truth as correct predictions for all nomenclature and genomics location tasks.

For the genedisease association task, we measure the recall as in the 8  original dataset but based on exact individual gene matches.

For the proteincoding genes task, we consider exact matches as correct after applying a simple vocabulary mapping that converts modelpredicted "yes" / "no" to "TRUE" / "NA" and Latin species names to their informal names, respectively.

The final answer of other datasets is "yes" / "no".

3.4 Results on Biologicalrelated Tasks To verify the effectiveness of the proposed model, we first conduct biological QA tasks.

Results are shown in Table 2.

We conclude with the following findings: (1) Based on the results of BioLLMs and GPT3.5, we conclude that finetuning domainspecific data is helpful for domainspecific tasks.

As the size of BioLLMs is much smaller than GPT3.5, their performance is on par with GPT3.5.

(2) BIORAG performs better than BioLLMs and GPT3.5, it indicates the effectiveness of local and external data sources.

(3) Though the size of BIORAG is much smaller than SciRAG (NewBing), it has better performance.

The gain comes from two aspects.

The first one is our customized prompts.

The second aspect lies in the local and external information sources.

NewBing has no access to specialized databases and lacks technical biological descriptions for reasoning.

(4) GeneGPT scores 0% accuracy in this task, because it is a customized model for the GeneTuring dataset, resulting in poor generalization capabilities.

3.5 Specialized Biological Reasoning Results The GeneTuring dataset contains more specialized biological questions, and the corresponding reasoning process highly relies on technical biological corpus and descriptions.

Results are shown in TaOption A: Moderate sexual size dimorphism   Query Preprocessing  Small rodent species with small litter sizes Retriever Execution And Result  Most rodents are small animals with robust bodies...

The largest litter I have heard of was 32 babies...

SelfEvaluation： The necessary information has been obtained to form an answer.

Inference and Generation： The official gene symbol for SGEF is ARHGEF26.

OUTPUT ARHGEF26    Retriever Execution And Result The average litter sizes and reproductive performance...

Previous analysis of the rules regarding how much more...

SelfEvaluation： Small litter sizes in rodents may be an adaptation to their specialized diet, requiring more parental investment.

Inference and Generation： Since the rodent species has a small litter size, it's possible that they invest more in each offspring.

This could lead to a higher parental investment, which might be related to option B: High parental investment.

Fewshot： Hello.

Your task is to use NCBI Web APIs to answer genomic questions.

Here are some examples (Entrez API) ...

Request : https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?

db=gene&term=SGEF Request : https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?

db=gene&id=1956 Inference and Generation： The official gene symbol for SGEF is SGEF    Figure 5: A case study selected from the College Biology dataset.

ble 1.

As this dataset does not contain the train data, BioLLMs are performed directly without finetuning.

Their bad results indicate their poor generalization.

In this dataset, we focus on the analyses of GeneGPT, NewBing, and BIORAG (1) For the nomenclature tasks, the performance of BIORAG and GeneGPT rank first and second respectively, as both of them have access to the Gene database.

BIORAG integrates the results of search engines while GeneGPT does not, and this brings the gap.

(2) The reasoning behind genomic location tasks relies on the highly specialized Gene and dbSNP database.

BIORAG and GeneGPT achieve 100% accuracy in the gene SNP association subtask, as  Figure 6: A case study conducted on the gene alias task in the GeneTuring dataset.

both of them have access to the dbSNP database.

However, NewBing has no access to the dbSNP database, thus it gets 0% accuracy in this task.

For the gene location subtask, the challenge is the variants of gene names.

The interface of GeneGPT does not support advanced search, thus the retrieved names are not comprehensive.

In contrast, general search engines, such as NewBing, have better retrieved results when the query entity has variants or ambiguities.

Thus NewBing has a better performance in this task than GeneGPT.

BIORAG supports the above two kinds of interfaces, and achieves the best results in this task.

(3) Functional analysis tasks rely on both the Gene database and   Thought:   Query Preprocessing  Bcell immunodeficiency.

Retriever Execution And Result  Official Symbol: TOP2B  Name: DNA topoisomerase II beta Other Aliases: BILU, TOPIIB, top2beta SelfEvaluation： Use the PubMed tool to conduct further searches on genes and  Xlinked agammaglobulinemia (XLA):  Gene: Bruton tyrosine kinase (BTK)  Function: Crucial for B cell development; mutations lead to a lack of mature B cells and antibody production1.

Common variable immunodeficiency (CVID):  Genes: Multiple genes (e.g., CTLA4, IKZF1, PIK3CD, STAT3)  Features: Preserved B cell numbers but impaired antibody production and low immunoglobulin levels2.

TOP2B Bcell immunodeficiency Retriever Execution And Result 	GeneGPT  Sequencing of patients with B cellrestricted...

The role of TOP2B in the immune system was revealed ...

SelfEvaluation： Use the PubMed tool to conduct further searches on genes and diseases, specifically focusing on TOP2B Bcell  immunodeficiency syndrome.

Fewshot： Hello.

Your task is to use NCBI Web APIs to answer genomic questions.

Here are some examples (Entrez API) ...

1.

Gene ： https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?

db=gene&retmax=5&retmode=json&sort=relevance&term=LMP10  Result:     HTML + CSS + JavaScript    Code  SelfEvaluation： TOP2B is associated with Bcell immunodeficiency syndrome Inference and Generation： TOP2B is associated with Bcell immunodeficiency syndrome  Inference and Generation： Bcell immunodeficiency is the gene associated with Bcell immunodeficiency  Figure 7: A case study conducted on the gene disease association task in GeneTuring dataset.

relative PubMed papers.

The PubMed corpus provides detailed genedisease relationships.

Although NewBing retrieves the metadata, BIORAG combines the local PubMed database with other specialized databases to achieve the best results.

nents within the BIORAG framework to achieve optimal performance in biological question reasoning tasks.

By understanding the contribution of each component, we can better optimize BIORAG for different tasks and datasets.

To evaluate the contribution of each component of BIORAG, we performed an extensive ablation study using the GeneTuring dataset, systematically removing individual components to assess their impact on performance across various tasks.

This study was designed to isolate the effects of different databases, components, and base models, with the experiments categorized as follows: (1) Databases: We consider three variations to evaluate the effectiveness of each data sources of our database: D1: BIORAGwithout the Gene database; D2: BIORAGwithout general search engines.

D3: BIORAGwithout the local PubMed database.

(2) Model Components: We investigate the impact of specific components of our proposed framework: C1: BIORAGwithout the MeSH Filter; C2: BIORAGwithout the Query Rewrite component; C3: BIORAGwithout the SelfEvaluation mechanism.

(3) Base Models: We compare the performance when using two different base LLM models: M1: take Llama38B as the basic LLM, and M2: take Llama370B as the basic LLM of BioRAG.

Based on the results of ablation study, we highlights the following key findings: (1) Impact of Databases: The results indicate that the Gene database (D1) plays a crucial role in performance.

For instance, the accuracy significantly drops in tasks such as Gene_location when this component is removed.

The general search engines (D2) and local PubMed database (D3) also contribute positively, but their impact is less pronounced compared to the Gene database.

(2) Component Contributions: Among the components, the SelfEvaluation mechanism (C3) is vital for maintaining high accuracy across most tasks.

The MeSH Filter (C1) and Query Rewrite (C2) also enhance performance, but their absence does not degrade the results as severely as the removal of SelfEvaluation.

(3) Effects of Basic Language Models: Comparing the two base models, Llama370B (M2) generally outperforms Llama38B (M1) across all tasks, indicating that the larger model size contributes to better handling of complex biological queries.

These findings underscore the importance of integrating diverse data sources and advanced compoTo compare reasoning differences among BIORAG and the baselines in a more intuitive manner, we select three typical case studies in this section.

We first provide a case study to show the workflow of BIORAG (Figure 5).

It is selected from the College Biology dataset.

BIORAG performs selfevaluation twice: the first time it starts with a web search for general information, but the results are insufficient to support answering the question.

Thus BIORAG conducts the second selfevaluation and calls for the more specialized PubMed database.

The results this time are accurate and sufficient to support answering the question, thus BIORAG gives the final answer based on the results.

The second case study is conducted on the gene alias task in the GeneTuring dataset (Figure 6).

The challenge of this task is the variants of gene names.

NewBing gets the response from the Wikimedia.

However, Wikimedia is not specialized enough to provide the alias for the input gene, which leads to the wrong answer.

The prompts of GeneGPT are too complicated, none of the prompts is relevant to this task.

In addition, its NCBI API returns the gene IDs, instead of the gene names.

The LLM is unable to understand these IDs, and finally arrives at a wrong answer.

BIORAG employs fuzzy queries, yielding a larger number of related responses with a higher error tolerance.

Furthermore, each result contains detailed generelated information and descriptions, such as the aliases.

Thus BIORAG gets the correct answer.

The third case study is conducted on the genedisease association task in the GeneTuring dataset, shown in Figure 7.

Reasoning behind this task relies on both the Gene database and relative PubMed papers.

The PubMed abstracts provide detailed genedisease relationships.

NewBing gets the response from the Geekymedics website.

Although the Geekymedics website provides general medical information, it does not offer the correct or specific details required for genedisease associations.

Consequently, NewBing’s response is inaccurate due to the reliance on a nonspecialized source.

GeneGPT chose the wrong NCBI API.

The API’s    feedback is a complicated and interminable HTML page, with massive irrelevant information or descriptions.

Based on the ambiguous backgrounds, GeneGPT outputs the wrong answer.

In the reasoning process of BIORAG, BioRAG uses multiple tools, i.e., Gene database, local PubMed database, and Web search, to gather and conduct mutual confirmation on the information of genes associated with Bcell immunodeficiency.

The process involves preprocessing queries, executing searches, and conducting selfevaluations at each step to ensure comprehensive and accurate results.

The reasoning process is thorough, incorporating various data sources to confirm the association of specific genes with Bcell immunodeficiency.

4 Conclusion This paper introduces BIORAG, an innovative framework that integrates RetrievalAugmented Generation with Large Language Models to enhance biological questionreasoning.

The framework’s ability to obtain relevant and current information from a blend of traditional databases, toolkits, and modern search engines ensures the accuracy of the generated answers.

Through extensive validation, including rigorous testing on widely recognized biology QA datasets and extensive case studies, BIORAG has demonstrated its superior ability to handle complex biological queries.

These results underscore the framework’s potential as a valuable tool for the scientific community, facilitating more accurate and efficient information processing.


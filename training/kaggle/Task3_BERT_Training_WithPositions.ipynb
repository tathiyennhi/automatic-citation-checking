{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 14648895,
     "sourceType": "datasetVersion",
     "datasetId": 9357890
    }
   ],
   "dockerImageVersionId": 31259,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Task 3: Citation Span Extraction with BERT-QA (WITH POSITIONS + METRICS)\n\n**Model:** bert-base-uncased (Question Answering)\n\n**Task:** Extract text span that citation supports\n\n**KEY IMPROVEMENTS:**\n- âœ… Uses pre-computed s_span/e_span positions (NO MORE text.find()!)\n- âœ… F1 + Exact Match metrics for proper evaluation\n- âœ… Early stopping based on F1 score\n- âœ… No data loss from failed text.find()\n- âœ… Memory efficient (streaming data)\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import transformers, datasets, accelerate\nprint(f\"âœ… transformers: {transformers.__version__}\")\nprint(f\"âœ… datasets: {datasets.__version__}\")\nprint(f\"âœ… accelerate: {accelerate.__version__}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-28T12:29:09.611200Z",
     "iopub.execute_input": "2026-01-28T12:29:09.611485Z",
     "iopub.status.idle": "2026-01-28T12:29:18.992795Z",
     "shell.execute_reply.started": "2026-01-28T12:29:09.611453Z",
     "shell.execute_reply": "2026-01-28T12:29:18.992170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "âœ… transformers: 4.57.1\nâœ… datasets: 4.4.2\nâœ… accelerate: 1.11.0\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "import os\n\ntrain_path = '/kaggle/input/thesis-data-task3-with-positions/train'\nval_path = '/kaggle/input/thesis-data-task3-with-positions/val'\n\ntrain_count = len([f for f in os.listdir(train_path) if f.endswith('.label')])\nval_count = len([f for f in os.listdir(val_path) if f.endswith('.label')])\n\nprint(f\"âœ… Train: {train_count:,} files\")\nprint(f\"âœ… Val: {val_count:,} files\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-28T12:29:18.994184Z",
     "iopub.execute_input": "2026-01-28T12:29:18.994758Z",
     "iopub.status.idle": "2026-01-28T12:29:19.858675Z",
     "shell.execute_reply.started": "2026-01-28T12:29:18.994730Z",
     "shell.execute_reply": "2026-01-28T12:29:19.857911Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Load data with positions\nimport json\nfrom pathlib import Path\nfrom datasets import IterableDataset\n\ndef generate_task3_examples(data_dir):\n    data_path = Path(data_dir)\n    label_files = sorted(data_path.glob(\"*.label\"))\n    total_files = len(label_files)\n    print(f\"ðŸ“Š Found {total_files:,} .label files\")\n    \n    skipped = 0\n    successful = 0\n\n    for i, label_file in enumerate(label_files):\n        if (i+1) % 5000 == 0:\n            print(f\"â³ {i+1:,}/{total_files:,} | Success: {successful:,} | Skipped: {skipped}\")\n\n        try:\n            with open(label_file) as f:\n                label_data = json.load(f)\n        except:\n            skipped += 1\n            continue\n\n        text = label_data.get('text', '')\n        if not text:\n            skipped += 1\n            continue\n            \n        citation_spans = label_data.get('citation_spans', [])\n\n        for span_info in citation_spans:\n            citation_id = span_info.get('citation_id', '')\n            span_text = span_info.get('span_text', '')\n            s_span = span_info.get('s_span', -1)\n            e_span = span_info.get('e_span', -1)\n            \n            if s_span == -1 or e_span == -1 or s_span >= e_span:\n                skipped += 1\n                continue\n\n            question = f\"What does citation {citation_id} support?\"\n            successful += 1\n            \n            yield {\n                'question': question,\n                'context': text,\n                'answer': span_text,\n                'start_char': s_span,\n                'end_char': e_span\n            }\n\n    print(f\"âœ… {successful:,} examples | Skipped: {skipped}\")\n\nprint(\"=\" * 60)\nprint(\"Creating datasets...\")\ntrain_dataset = IterableDataset.from_generator(\n    generate_task3_examples,\n    gen_kwargs={'data_dir': '/kaggle/input/thesis-data-task3-with-positions/train'}\n)\nval_dataset = IterableDataset.from_generator(\n    generate_task3_examples,\n    gen_kwargs={'data_dir': '/kaggle/input/thesis-data-task3-with-positions/val'}\n)\nprint(\"âœ… Datasets ready\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-28T12:29:19.859581Z",
     "iopub.execute_input": "2026-01-28T12:29:19.859803Z",
     "iopub.status.idle": "2026-01-28T12:29:20.319708Z",
     "shell.execute_reply.started": "2026-01-28T12:29:19.859783Z",
     "shell.execute_reply": "2026-01-28T12:29:20.318667Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Tokenize\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\ndef prepare_train_features(examples):\n    tokenized = tokenizer(\n        examples['question'],\n        examples['context'],\n        max_length=512,\n        truncation='only_second',\n        padding=False,\n        return_offsets_mapping=True\n    )\n\n    offset_mapping = tokenized['offset_mapping']\n    start_positions = []\n    end_positions = []\n\n    for i in range(len(examples['question'])):\n        start_char = examples['start_char'][i]\n        end_char = examples['end_char'][i]\n        offsets = offset_mapping[i]\n\n        start_token = 0\n        for idx, (offset_start, offset_end) in enumerate(offsets):\n            if offset_start <= start_char < offset_end:\n                start_token = idx\n                break\n\n        end_token = 0\n        for idx, (offset_start, offset_end) in enumerate(offsets):\n            if offset_start < end_char <= offset_end:\n                end_token = idx\n                break\n\n        start_positions.append(start_token)\n        end_positions.append(end_token)\n\n    tokenized['start_positions'] = start_positions\n    tokenized['end_positions'] = end_positions\n    return tokenized\n\ntrain_dataset = train_dataset.map(prepare_train_features, batched=True, remove_columns=['question', 'context', 'answer', 'start_char', 'end_char'])\nval_dataset = val_dataset.map(prepare_train_features, batched=True, remove_columns=['question', 'context', 'answer', 'start_char', 'end_char'])\ntrain_dataset = train_dataset.map(lambda x: {k: v for k, v in x.items() if k != 'offset_mapping'}, batched=True)\nval_dataset = val_dataset.map(lambda x: {k: v for k, v in x.items() if k != 'offset_mapping'}, batched=True)\nprint(\"âœ… Tokenization complete\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-28T12:29:20.320794Z",
     "iopub.execute_input": "2026-01-28T12:29:20.322172Z",
     "iopub.status.idle": "2026-01-28T12:29:27.385445Z",
     "shell.execute_reply.started": "2026-01-28T12:29:20.322142Z",
     "shell.execute_reply": "2026-01-28T12:29:27.384720Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2cef9452e314596b008637ef703392e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47994bb1321e49988671d3cc8b515414"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "904d1c5fab5f40fca46ffb4445954a8d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed36d79235e44c2ca635fc81af4bc427"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "âœ… Tokenization complete\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "# Load model\nfrom transformers import AutoModelForQuestionAnswering\n\nmodel = AutoModelForQuestionAnswering.from_pretrained('bert-base-uncased')\nprint(f\"âœ… BERT loaded: {model.num_parameters():,} parameters\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-28T12:29:27.386194Z",
     "iopub.execute_input": "2026-01-28T12:29:27.386666Z",
     "iopub.status.idle": "2026-01-28T12:29:47.118200Z",
     "shell.execute_reply.started": "2026-01-28T12:29:27.386641Z",
     "shell.execute_reply": "2026-01-28T12:29:47.117510Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "2026-01-28 12:29:30.166986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769603370.346665      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769603370.398244      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769603370.861880      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769603370.861924      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769603370.861927      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769603370.861930      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bce465b35d941b28c808d02d5d95897"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "âœ… BERT loaded: 108,893,186 parameters\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "# Define F1 + EM metrics\nimport numpy as np\n\ndef compute_metrics(pred):\n    \"\"\"\n    Compute Exact Match (EM) and Token-level F1 for span extraction.\n    \n    For QA models:\n    - pred.predictions[0]: start_logits\n    - pred.predictions[1]: end_logits\n    - pred.label_ids: tuple of (start_positions, end_positions)\n    \"\"\"\n    start_logits, end_logits = pred.predictions\n    start_predictions = np.argmax(start_logits, axis=1)\n    end_predictions = np.argmax(end_logits, axis=1)\n    \n    start_labels = pred.label_ids[0] if isinstance(pred.label_ids, tuple) else pred.label_ids[:, 0]\n    end_labels = pred.label_ids[1] if isinstance(pred.label_ids, tuple) else pred.label_ids[:, 1]\n    \n    exact_match = 0\n    f1_total = 0.0\n    total = len(start_labels)\n    \n    for i in range(total):\n        pred_start = start_predictions[i]\n        pred_end = end_predictions[i]\n        true_start = start_labels[i]\n        true_end = end_labels[i]\n        \n        # Exact Match: both start and end must match\n        if pred_start == true_start and pred_end == true_end:\n            exact_match += 1\n            f1_total += 1.0\n        else:\n            # F1: Calculate token overlap\n            if pred_end < pred_start:\n                pred_end = pred_start\n            \n            pred_tokens = set(range(pred_start, pred_end + 1))\n            true_tokens = set(range(true_start, true_end + 1))\n            \n            if len(pred_tokens) > 0 and len(true_tokens) > 0:\n                overlap = pred_tokens & true_tokens\n                \n                if len(overlap) > 0:\n                    precision = len(overlap) / len(pred_tokens)\n                    recall = len(overlap) / len(true_tokens)\n                    f1 = 2 * precision * recall / (precision + recall)\n                    f1_total += f1\n    \n    return {\n        'exact_match': exact_match / total,\n        'f1': f1_total / total\n    }\n\nprint(\"âœ… Metrics function defined: EM + F1\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-28T12:29:47.119052Z",
     "iopub.execute_input": "2026-01-28T12:29:47.119698Z",
     "iopub.status.idle": "2026-01-28T12:29:47.128404Z",
     "shell.execute_reply.started": "2026-01-28T12:29:47.119670Z",
     "shell.execute_reply": "2026-01-28T12:29:47.127702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "âœ… Metrics function defined: EM + F1\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "# Training setup with metrics\nfrom transformers import TrainingArguments, Trainer, DataCollatorWithPadding, EarlyStoppingCallback\nfrom pathlib import Path\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/checkpoints/task3_bert_with_positions',\n    max_steps=10000,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    gradient_accumulation_steps=4,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    warmup_steps=500,\n    eval_strategy='steps',\n    eval_steps=500,\n    logging_steps=100,\n    save_strategy='steps',\n    save_steps=500,\n    save_total_limit=3,\n    load_best_model_at_end=True,\n    metric_for_best_model='f1',\n    greater_is_better=True,\n    fp16=True,\n    report_to='none',\n    seed=42\n)\n\nearly_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n\ncheckpoint_dir = Path(training_args.output_dir)\ncheckpoints = sorted(checkpoint_dir.glob('checkpoint-*')) if checkpoint_dir.exists() else []\nresume_checkpoint = str(checkpoints[-1]) if checkpoints else None\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[early_stopping]\n)\n\nprint(f\"ðŸ’¡ Training config:\")\nprint(f\"   - Model: BERT\")\nprint(f\"   - Positions: s_span/e_span âœ…\")\nprint(f\"   - Metrics: F1 + EM âœ…\")\nprint(f\"   - Best model selection: F1 score\")\nprint(f\"   - Early stopping: patience=3\")\nprint(f\"   - Batch size: {8 * 4}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-28T12:29:47.131013Z",
     "iopub.execute_input": "2026-01-28T12:29:47.131691Z",
     "iopub.status.idle": "2026-01-28T12:29:49.284453Z",
     "shell.execute_reply.started": "2026-01-28T12:29:47.131654Z",
     "shell.execute_reply": "2026-01-28T12:29:49.283587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "ðŸ’¡ Training config:\n   - Model: BERT\n   - Positions: s_span/e_span âœ…\n   - Metrics: F1 + EM âœ…\n   - Best model selection: F1 score\n   - Early stopping: patience=3\n   - Batch size: 32\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/tmp/ipykernel_55/3371120942.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "# Train\nprint(\"=\"*60)\nprint(\"ðŸš€ TRAINING SCIBERT WITH POSITIONS + METRICS\")\nprint(\"=\"*60)\ntrainer.train(resume_from_checkpoint=resume_checkpoint)\nprint(\"\\nâœ… Training complete!\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-28T12:29:49.285366Z",
     "iopub.execute_input": "2026-01-28T12:29:49.285627Z",
     "iopub.status.idle": "2026-01-28T12:29:49.710206Z",
     "shell.execute_reply.started": "2026-01-28T12:29:49.285604Z",
     "shell.execute_reply": "2026-01-28T12:29:49.709211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "============================================================\nðŸš€ TRAINING SCIBERT WITH POSITIONS + METRICS\n============================================================\nðŸ“Š Found 0 .label files\nâœ… 0 examples | Skipped: 0\n",
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/4102536826.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ðŸš€ TRAINING SCIBERT WITH POSITIONS + METRICS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nâœ… Training complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2616\u001b[0m                 \u001b[0mupdate_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2617\u001b[0m                 \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mupdate_step\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_updates\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mremainder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2618\u001b[0;31m                 \u001b[0mbatch_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2619\u001b[0m                 \u001b[0;31m# Store the number of batches for current gradient accumulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m                 \u001b[0;31m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[1;32m   5652\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5653\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5654\u001b[0;31m                 \u001b[0mbatch_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5655\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5656\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    889\u001b[0m                     \u001b[0;34mf\"Batch does not contain any data (`{batch}`). At the end of all iterable data available before expected stop iteration.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                 )\n",
      "\u001b[0;31mValueError\u001b[0m: Batch does not contain any data (`None`). At the end of all iterable data available before expected stop iteration."
     ],
     "ename": "ValueError",
     "evalue": "Batch does not contain any data (`None`). At the end of all iterable data available before expected stop iteration.",
     "output_type": "error"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "# Evaluate with metrics\nprint(\"ðŸ“Š VALIDATION RESULTS\")\neval_results = trainer.evaluate()\nprint(\"=\"*60)\nfor key, value in eval_results.items():\n    print(f\"{key}: {value:.4f}\")\nprint(\"=\"*60)\nprint(f\"\\nâœ… F1 Score: {eval_results.get('eval_f1', 0):.2%}\")\nprint(f\"âœ… Exact Match: {eval_results.get('eval_exact_match', 0):.2%}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-28T12:29:49.710947Z",
     "iopub.status.idle": "2026-01-28T12:29:49.711290Z",
     "shell.execute_reply.started": "2026-01-28T12:29:49.711134Z",
     "shell.execute_reply": "2026-01-28T12:29:49.711151Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Save\nfinal_model_path = '/kaggle/working/models/task3_bert_with_positions_final'\ntrainer.save_model(final_model_path)\ntokenizer.save_pretrained(final_model_path)\nprint(f\"âœ… Model saved to: {final_model_path}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-28T12:29:49.712486Z",
     "iopub.status.idle": "2026-01-28T12:29:49.712749Z",
     "shell.execute_reply.started": "2026-01-28T12:29:49.712630Z",
     "shell.execute_reply": "2026-01-28T12:29:49.712646Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Test\nimport torch\nfrom transformers import pipeline\n\nqa_pipeline = pipeline(\n    'question-answering',\n    model=final_model_path,\n    tokenizer=final_model_path,\n    device=0 if torch.cuda.is_available() else -1\n)\n\nresult = qa_pipeline(\n    question=\"What does citation [CITATION_1] support?\",\n    context=\"Previous studies demonstrated significant improvements in model performance. These findings support our hypothesis.\"\n)\n\nprint(\"\\nðŸ“‹ Test Inference:\")\nprint(f\"Answer: {result['answer']}\")\nprint(f\"Confidence: {result['score']:.4f}\")\nprint(\"\\nâœ… SCIBERT TRAINING COMPLETE (WITH METRICS)!\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-28T12:29:49.713977Z",
     "iopub.status.idle": "2026-01-28T12:29:49.714307Z",
     "shell.execute_reply.started": "2026-01-28T12:29:49.714136Z",
     "shell.execute_reply": "2026-01-28T12:29:49.714152Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
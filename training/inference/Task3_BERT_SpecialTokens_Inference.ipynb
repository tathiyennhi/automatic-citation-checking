{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Task 3: Citation Span Extraction - BERT + Special Tokens Inference\n",
    "\n",
    "**Model:** bert-base-uncased (Question Answering) + Citation Special Tokens\n",
    "\n",
    "**Task:** Extract text span that citation supports\n",
    "\n",
    "**Dataset:** thesis-data-task3-test-gold-500\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Load Model - WITH SPECIAL TOKENS\n# Path to saved model from training notebook output\nimport os\nfrom pathlib import Path\n\n# First, let's explore the dataset structure\nbase_path = '/kaggle/input/task3-bert-training-specialtokens'\nprint(f\"üìÇ Exploring {base_path}...\")\n\nif os.path.exists(base_path):\n    for root, dirs, files in os.walk(base_path):\n        level = root.replace(base_path, '').count(os.sep)\n        indent = ' ' * 2 * level\n        print(f\"{indent}{os.path.basename(root)}/\")\n        subindent = ' ' * 2 * (level + 1)\n        for file in files[:5]:  # Show first 5 files\n            print(f\"{subindent}{file}\")\n        if len(files) > 5:\n            print(f\"{subindent}... and {len(files)-5} more files\")\n        if level > 2:  # Stop after 3 levels deep\n            break\nelse:\n    print(f\"‚ùå Base path not found: {base_path}\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# Now find the model directory\nmodel_dir = None\nmodels_path = os.path.join(base_path, 'models')\n\nif os.path.exists(models_path):\n    # List all subdirectories in models/\n    subdirs = [d for d in os.listdir(models_path) if os.path.isdir(os.path.join(models_path, d))]\n    print(f\"Found subdirectories in models/: {subdirs}\")\n    \n    if subdirs:\n        # Use the first one (should be task3_bert_special_tokens_final)\n        model_dir = os.path.join(models_path, subdirs[0])\n        print(f\"‚úÖ Using model directory: {model_dir}\")\n\n# Also check working/models path\nworking_models_path = os.path.join(base_path, 'working', 'models')\nif os.path.exists(working_models_path):\n    subdirs = [d for d in os.listdir(working_models_path) if os.path.isdir(os.path.join(working_models_path, d))]\n    if subdirs:\n        model_dir = os.path.join(working_models_path, subdirs[0])\n        print(f\"‚úÖ Using model directory: {model_dir}\")\n\nif model_dir is None:\n    raise FileNotFoundError(f\"‚ùå Could not find model directory in {base_path}\")\n\nprint(f\"\\nüì¶ Loading model from: {model_dir}\")\n\n# Load tokenizer (includes special tokens)\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_dir)\n\nprint(f\"‚úÖ Tokenizer vocab size: {len(tokenizer)}\")\n\n# Verify special tokens\ntest_text = \"This research [CITATION_1] shows that [CITATION_2] improves performance.\"\ntest_tokens = tokenizer.tokenize(test_text)\nprint(f\"\\nüìã Test tokenization:\")\nprint(f\"Text: {test_text}\")\nprint(f\"Tokens: {test_tokens}\")\n\n# Move to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.eval()\n\nprint(f\"\\n‚úÖ Model loaded on {device}\")\nprint(f\"üìä Model parameters: {model.num_parameters():,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Data\n",
    "test_path = Path('/kaggle/input/thesis-data-task3-test-gold-500/test_gold_500')\n",
    "test_files = sorted(test_path.glob(\"*.in\"))\n",
    "\n",
    "print(f\"üìÇ Test path: {test_path}\")\n",
    "print(f\"üìä Found {len(test_files)} test files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Function\n",
    "def extract_citation_span(text, citation, model, tokenizer, device, max_length=512):\n",
    "    \"\"\"\n",
    "    Extract span for a given citation using Question Answering approach\n",
    "    \"\"\"\n",
    "    # Create question\n",
    "    question = f\"What does citation {citation} support?\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        truncation='only_second',\n",
    "        return_tensors='pt',\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get start and end positions\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "    \n",
    "    start_idx = torch.argmax(start_logits, dim=1).item()\n",
    "    end_idx = torch.argmax(end_logits, dim=1).item()\n",
    "    \n",
    "    # Decode answer\n",
    "    if start_idx <= end_idx and start_idx > 0:\n",
    "        answer_tokens = inputs['input_ids'][0][start_idx:end_idx+1]\n",
    "        predicted_span = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "    else:\n",
    "        predicted_span = \"\"\n",
    "    \n",
    "    # Get confidence scores\n",
    "    start_prob = torch.softmax(start_logits, dim=1)[0][start_idx].item()\n",
    "    end_prob = torch.softmax(end_logits, dim=1)[0][end_idx].item()\n",
    "    confidence = (start_prob + end_prob) / 2\n",
    "    \n",
    "    return {\n",
    "        'predicted_span': predicted_span,\n",
    "        'start_idx': start_idx,\n",
    "        'end_idx': end_idx,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Inference function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Inference on Test Set\n",
    "results = []\n",
    "errors = []\n",
    "\n",
    "print(\"üöÄ Starting inference...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for test_file in tqdm(test_files, desc=\"Processing files\"):\n",
    "    try:\n",
    "        # Load input file\n",
    "        with open(test_file) as f:\n",
    "            in_data = json.load(f)\n",
    "        \n",
    "        # Load label file (for comparison)\n",
    "        label_file = test_file.with_suffix('.label')\n",
    "        with open(label_file) as f:\n",
    "            label_data = json.load(f)\n",
    "        \n",
    "        text = in_data['text']\n",
    "        citation_spans = label_data.get('citation_spans', [])\n",
    "        \n",
    "        # Process each citation\n",
    "        for span_info in citation_spans:\n",
    "            citation_id = span_info['citation_id']\n",
    "            gold_span = span_info['span_text']\n",
    "            \n",
    "            # Run inference\n",
    "            prediction = extract_citation_span(\n",
    "                text=text,\n",
    "                citation=citation_id,\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'file': test_file.stem,\n",
    "                'citation_id': citation_id,\n",
    "                'gold_span': gold_span,\n",
    "                'predicted_span': prediction['predicted_span'],\n",
    "                'confidence': prediction['confidence'],\n",
    "                'start_idx': prediction['start_idx'],\n",
    "                'end_idx': prediction['end_idx']\n",
    "            })\n",
    "    \n",
    "    except Exception as e:\n",
    "        errors.append({\n",
    "            'file': test_file.stem,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úÖ Inference complete!\")\n",
    "print(f\"üìä Processed: {len(results)} predictions\")\n",
    "print(f\"‚ùå Errors: {len(errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Metrics\n",
    "def calculate_exact_match(gold, pred):\n",
    "    \"\"\"Exact match: predicted == gold (after normalization)\"\"\"\n",
    "    return gold.strip().lower() == pred.strip().lower()\n",
    "\n",
    "def calculate_f1(gold, pred):\n",
    "    \"\"\"F1 score based on token overlap\"\"\"\n",
    "    gold_tokens = gold.strip().lower().split()\n",
    "    pred_tokens = pred.strip().lower().split()\n",
    "    \n",
    "    if len(gold_tokens) == 0 or len(pred_tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    common = set(gold_tokens) & set(pred_tokens)\n",
    "    \n",
    "    if len(common) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gold_tokens)\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# Calculate metrics\n",
    "exact_matches = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for result in results:\n",
    "    if calculate_exact_match(result['gold_span'], result['predicted_span']):\n",
    "        exact_matches += 1\n",
    "    \n",
    "    total_f1 += calculate_f1(result['gold_span'], result['predicted_span'])\n",
    "\n",
    "exact_match_score = exact_matches / len(results) if results else 0\n",
    "avg_f1_score = total_f1 / len(results) if results else 0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä EVALUATION METRICS - SPECIAL TOKENS MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total predictions: {len(results)}\")\n",
    "print(f\"Exact Match: {exact_match_score:.4f} ({exact_matches}/{len(results)})\")\n",
    "print(f\"F1 Score: {avg_f1_score:.4f}\")\n",
    "print(f\"Average Confidence: {sum(r['confidence'] for r in results)/len(results):.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Examples - FULL TEXT (no truncation)\n",
    "print(\"\\nüìã SAMPLE PREDICTIONS:\\n\")\n",
    "\n",
    "# Show first 10 predictions\n",
    "for i, result in enumerate(results[:10]):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"File: {result['file']}\")\n",
    "    print(f\"Citation: {result['citation_id']}\")\n",
    "    print(f\"\\nüìå Gold Span (FULL):\")\n",
    "    print(f\"{result['gold_span']}\")\n",
    "    print(f\"\\nüîÆ Predicted Span (FULL):\")\n",
    "    print(f\"{result['predicted_span']}\")\n",
    "    print(f\"\\nConfidence: {result['confidence']:.4f}\")\n",
    "    match = \"‚úÖ EXACT MATCH\" if calculate_exact_match(result['gold_span'], result['predicted_span']) else \"‚ùå NO MATCH\"\n",
    "    f1 = calculate_f1(result['gold_span'], result['predicted_span'])\n",
    "    print(f\"Result: {match} (F1: {f1:.4f})\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'task3_bert_special_tokens_predictions.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {output_file}\")\n",
    "print(f\"üìä Total rows: {len(df)}\")\n",
    "\n",
    "# Show errors if any\n",
    "if errors:\n",
    "    print(f\"\\n‚ö†Ô∏è Errors encountered: {len(errors)}\")\n",
    "    for error in errors[:5]:\n",
    "        print(f\"  - {error['file']}: {error['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ INFERENCE COMPLETE - SPECIAL TOKENS MODEL!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
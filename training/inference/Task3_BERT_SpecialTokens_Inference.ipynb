{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Task 3: Citation Span Extraction - BERT + Special Tokens Inference\n",
    "\n",
    "**Model:** bert-base-uncased (Question Answering) + Citation Special Tokens\n",
    "\n",
    "**Task:** Extract text span that citation supports\n",
    "\n",
    "**Dataset:** thesis-data-task3-test-gold-500\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model - WITH SPECIAL TOKENS\n",
    "# Path to saved model from training notebook output\n",
    "model_path = '/kaggle/input/task3-bert-special-tokens-training/models/task3_bert_special_tokens_final'\n",
    "\n",
    "print(f\"üì¶ Loading model from: {model_path}\")\n",
    "\n",
    "# Load tokenizer (includes special tokens)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "print(f\"‚úÖ Tokenizer vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# Verify special tokens\n",
    "test_text = \"This research [CITATION_1] shows that [CITATION_2] improves performance.\"\n",
    "test_tokens = tokenizer.tokenize(test_text)\n",
    "print(f\"\\nüìã Test tokenization:\")\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Tokens: {test_tokens}\")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded on {device}\")\n",
    "print(f\"üìä Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Data\n",
    "test_path = Path('/kaggle/input/thesis-data-task3-test-gold-500/test_gold_500')\n",
    "test_files = sorted(test_path.glob(\"*.in\"))\n",
    "\n",
    "print(f\"üìÇ Test path: {test_path}\")\n",
    "print(f\"üìä Found {len(test_files)} test files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Function\n",
    "def extract_citation_span(text, citation, model, tokenizer, device, max_length=512):\n",
    "    \"\"\"\n",
    "    Extract span for a given citation using Question Answering approach\n",
    "    \"\"\"\n",
    "    # Create question\n",
    "    question = f\"What does citation {citation} support?\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        truncation='only_second',\n",
    "        return_tensors='pt',\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get start and end positions\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "    \n",
    "    start_idx = torch.argmax(start_logits, dim=1).item()\n",
    "    end_idx = torch.argmax(end_logits, dim=1).item()\n",
    "    \n",
    "    # Decode answer\n",
    "    if start_idx <= end_idx and start_idx > 0:\n",
    "        answer_tokens = inputs['input_ids'][0][start_idx:end_idx+1]\n",
    "        predicted_span = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "    else:\n",
    "        predicted_span = \"\"\n",
    "    \n",
    "    # Get confidence scores\n",
    "    start_prob = torch.softmax(start_logits, dim=1)[0][start_idx].item()\n",
    "    end_prob = torch.softmax(end_logits, dim=1)[0][end_idx].item()\n",
    "    confidence = (start_prob + end_prob) / 2\n",
    "    \n",
    "    return {\n",
    "        'predicted_span': predicted_span,\n",
    "        'start_idx': start_idx,\n",
    "        'end_idx': end_idx,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Inference function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Inference on Test Set\n",
    "results = []\n",
    "errors = []\n",
    "\n",
    "print(\"üöÄ Starting inference...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for test_file in tqdm(test_files, desc=\"Processing files\"):\n",
    "    try:\n",
    "        # Load input file\n",
    "        with open(test_file) as f:\n",
    "            in_data = json.load(f)\n",
    "        \n",
    "        # Load label file (for comparison)\n",
    "        label_file = test_file.with_suffix('.label')\n",
    "        with open(label_file) as f:\n",
    "            label_data = json.load(f)\n",
    "        \n",
    "        text = in_data['text']\n",
    "        citation_spans = label_data.get('citation_spans', [])\n",
    "        \n",
    "        # Process each citation\n",
    "        for span_info in citation_spans:\n",
    "            citation_id = span_info['citation_id']\n",
    "            gold_span = span_info['span_text']\n",
    "            \n",
    "            # Run inference\n",
    "            prediction = extract_citation_span(\n",
    "                text=text,\n",
    "                citation=citation_id,\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'file': test_file.stem,\n",
    "                'citation_id': citation_id,\n",
    "                'gold_span': gold_span,\n",
    "                'predicted_span': prediction['predicted_span'],\n",
    "                'confidence': prediction['confidence'],\n",
    "                'start_idx': prediction['start_idx'],\n",
    "                'end_idx': prediction['end_idx']\n",
    "            })\n",
    "    \n",
    "    except Exception as e:\n",
    "        errors.append({\n",
    "            'file': test_file.stem,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úÖ Inference complete!\")\n",
    "print(f\"üìä Processed: {len(results)} predictions\")\n",
    "print(f\"‚ùå Errors: {len(errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Metrics\n",
    "def calculate_exact_match(gold, pred):\n",
    "    \"\"\"Exact match: predicted == gold (after normalization)\"\"\"\n",
    "    return gold.strip().lower() == pred.strip().lower()\n",
    "\n",
    "def calculate_f1(gold, pred):\n",
    "    \"\"\"F1 score based on token overlap\"\"\"\n",
    "    gold_tokens = gold.strip().lower().split()\n",
    "    pred_tokens = pred.strip().lower().split()\n",
    "    \n",
    "    if len(gold_tokens) == 0 or len(pred_tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    common = set(gold_tokens) & set(pred_tokens)\n",
    "    \n",
    "    if len(common) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gold_tokens)\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# Calculate metrics\n",
    "exact_matches = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for result in results:\n",
    "    if calculate_exact_match(result['gold_span'], result['predicted_span']):\n",
    "        exact_matches += 1\n",
    "    \n",
    "    total_f1 += calculate_f1(result['gold_span'], result['predicted_span'])\n",
    "\n",
    "exact_match_score = exact_matches / len(results) if results else 0\n",
    "avg_f1_score = total_f1 / len(results) if results else 0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä EVALUATION METRICS - SPECIAL TOKENS MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total predictions: {len(results)}\")\n",
    "print(f\"Exact Match: {exact_match_score:.4f} ({exact_matches}/{len(results)})\")\n",
    "print(f\"F1 Score: {avg_f1_score:.4f}\")\n",
    "print(f\"Average Confidence: {sum(r['confidence'] for r in results)/len(results):.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Examples - FULL TEXT (no truncation)\n",
    "print(\"\\nüìã SAMPLE PREDICTIONS:\\n\")\n",
    "\n",
    "# Show first 10 predictions\n",
    "for i, result in enumerate(results[:10]):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"File: {result['file']}\")\n",
    "    print(f\"Citation: {result['citation_id']}\")\n",
    "    print(f\"\\nüìå Gold Span (FULL):\")\n",
    "    print(f\"{result['gold_span']}\")\n",
    "    print(f\"\\nüîÆ Predicted Span (FULL):\")\n",
    "    print(f\"{result['predicted_span']}\")\n",
    "    print(f\"\\nConfidence: {result['confidence']:.4f}\")\n",
    "    match = \"‚úÖ EXACT MATCH\" if calculate_exact_match(result['gold_span'], result['predicted_span']) else \"‚ùå NO MATCH\"\n",
    "    f1 = calculate_f1(result['gold_span'], result['predicted_span'])\n",
    "    print(f\"Result: {match} (F1: {f1:.4f})\")\n",
    "    print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'task3_bert_special_tokens_predictions.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {output_file}\")\n",
    "print(f\"üìä Total rows: {len(df)}\")\n",
    "\n",
    "# Show errors if any\n",
    "if errors:\n",
    "    print(f\"\\n‚ö†Ô∏è Errors encountered: {len(errors)}\")\n",
    "    for error in errors[:5]:\n",
    "        print(f\"  - {error['file']}: {error['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ INFERENCE COMPLETE - SPECIAL TOKENS MODEL!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

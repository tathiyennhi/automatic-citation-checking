[
  {
    "id": "doc_019_sent_01",
    "text": "(2019) performed an optimization of the BERT model and introduced \nRoBERTa (Robustly Optimized BERT Pretraining Approach)."
  },
  {
    "id": "doc_019_sent_02",
    "text": "RoBERTa was evaluated \non three benchmarks and demonstrated massive improvements over the reported BERT \nperformance."
  },
  {
    "id": "doc_019_sent_03",
    "text": "Currently, several domain-specific models have been developed."
  },
  {
    "id": "doc_019_sent_04",
    "text": "Thus, Beltagy etÂ al.. \n(2019) released SciBERT a BERT-based language model pre-trained on a large number \nof unlabeled scientific articles from the computer science and biomedical domains."
  },
  {
    "id": "doc_019_sent_05",
    "text": "SciB-\nERT showed improvements over BERT on several downstream NLP tasks, including NER."
  }
]
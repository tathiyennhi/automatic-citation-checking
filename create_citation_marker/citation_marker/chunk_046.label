[
  {
    "id": "doc_046_sent_01",
    "with_marker": "1), as well as the performance of two default FLAIR models (Flair Embeddings \nand Transformers) with corpus No.2.",
    "citations": []
  },
  {
    "id": "doc_046_sent_02",
    "with_marker": "Additionally, the performance of Flair Embeddings \nand Transformers models was tested with the corpus No.1 The training was conducted with \nthe recommended parameters for all algorithms, as Flair developers specifically ran vari-\nous tests to find the best hyperparameters for the default models.",
    "citations": []
  },
  {
    "id": "doc_046_sent_03",
    "with_marker": "For the few-shot TARS, \nthe training was conducted with the small dataset (corpus No.1), and for Transformers and \nFlair Embeddings with a larger dataset (corpus No.2).",
    "citations": []
  },
  {
    "id": "doc_046_sent_04",
    "with_marker": "13  https://​github.​com/​flair​NLP/​flair\n\nScientometrics\t\n1 3\nThe Flair Embeddings model was initiated as a combination of static and contextual \nstring embeddings.",
    "citations": []
  },
  {
    "id": "doc_046_sent_05",
    "with_marker": "We applied GloVe [CITATION_44] as a static word-level \nembedding model.",
    "citations": [
      {
        "marker": "[CITATION_44]",
        "original": "Pennington et  al., 2014"
      }
    ]
  }
]
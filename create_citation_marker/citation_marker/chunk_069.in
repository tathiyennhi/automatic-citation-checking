[
  {
    "id": "doc_069_sent_01",
    "text": "RoBERTa was pre-trained \non the combination of five datasets containing news articles, blog entries, books, and Wikipe-\ndia articles."
  },
  {
    "id": "doc_069_sent_02",
    "text": "TARS was mainly pre-trained on datasets for text classification."
  },
  {
    "id": "doc_069_sent_03",
    "text": "Thus, the models \nused were not trained on domain-specific data."
  },
  {
    "id": "doc_069_sent_04",
    "text": "This can also explain the pure Transformers \nand TARS performance."
  },
  {
    "id": "doc_069_sent_05",
    "text": "The higher accuracy for the individuals category in the training with \nTARS can be explained by the fact, that the word ’person’ is semantically more straightforward \nthan other categories."
  }
]
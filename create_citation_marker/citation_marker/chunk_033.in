[
  {
    "id": "doc_033_sent_01",
    "text": "1   An example of acknowledged entities."
  },
  {
    "id": "doc_033_sent_02",
    "text": "Each entity type is marked with a distinct color\n8  New transformer models as SciBERT or SsciBERT were not evaluated in this study, as the objective of \nthe study is to evaluate the performance of the Flair default models."
  },
  {
    "id": "doc_033_sent_03",
    "text": "Scientometrics\t\n1 3\nLSTM-CRF with the multilingual XML-RoBERTa transformer model (Schweter & Akbik, \n2020)."
  },
  {
    "id": "doc_033_sent_04",
    "text": "The TARS (task-aware representation of sentences) is a transformer-based model, \nwhich allows performing training without any training data (zero-shot learning) or with a \nsmall dataset (few-short learning) (Halder et al., 2020)."
  },
  {
    "id": "doc_033_sent_05",
    "text": "The TARS approach differs from \nthe traditional transfer learning approach in the way that the TARS model also considers \nsemantic information captured in the class labels themselves."
  }
]
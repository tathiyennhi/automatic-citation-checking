[
  {
    "id": "doc_047_sent_01",
    "with_marker": "Thus, in our case, stacked embeddings comprise GloVe embeddings, \nforward contextual string embeddings, and backward contextual string embeddings.",
    "citations": []
  },
  {
    "id": "doc_047_sent_02",
    "with_marker": "The \nmodel was trained with the recommended parameters: the size of mini-batches was set to \n32 and the maximum number of epochs was set to 150.",
    "citations": []
  },
  {
    "id": "doc_047_sent_03",
    "with_marker": "For Transformers, training was initiated with the RoBERTa model [CITATION_45].",
    "citations": [
      {
        "marker": "[CITATION_45]",
        "original": "Liu etÂ al., 2019"
      }
    ]
  },
  {
    "id": "doc_047_sent_04",
    "with_marker": "For \nthe present paper, a fine-tuning approach was used.",
    "citations": []
  },
  {
    "id": "doc_047_sent_05",
    "with_marker": "The fine-tuning procedure consisted of \nadding a linear layer to a transformer and retraining the entire network with a small learn-\ning rate.",
    "citations": []
  }
]
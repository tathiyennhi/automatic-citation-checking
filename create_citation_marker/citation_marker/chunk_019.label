[
  {
    "id": "doc_019_sent_01",
    "with_marker": "(2019) performed an optimization of the BERT model and introduced \nRoBERTa (Robustly Optimized BERT Pretraining Approach).",
    "citations": []
  },
  {
    "id": "doc_019_sent_02",
    "with_marker": "RoBERTa was evaluated \non three benchmarks and demonstrated massive improvements over the reported BERT \nperformance.",
    "citations": []
  },
  {
    "id": "doc_019_sent_03",
    "with_marker": "Currently, several domain-specific models have been developed.",
    "citations": []
  },
  {
    "id": "doc_019_sent_04",
    "with_marker": "Thus, Beltagy etÂ al.. \n(2019) released SciBERT a BERT-based language model pre-trained on a large number \nof unlabeled scientific articles from the computer science and biomedical domains.",
    "citations": []
  },
  {
    "id": "doc_019_sent_05",
    "with_marker": "SciB-\nERT showed improvements over BERT on several downstream NLP tasks, including NER.",
    "citations": []
  }
]
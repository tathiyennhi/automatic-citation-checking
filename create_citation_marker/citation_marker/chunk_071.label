[
  {
    "id": "doc_071_sent_01",
    "with_marker": "Previous works showed improvements in downstream tasks using embedding models \nfine-tuned for the domain used [CITATION_52][CITATION_53].",
    "citations": [
      {
        "marker": "[CITATION_52]",
        "original": "Shen et al., 2022"
      },
      {
        "marker": "[CITATION_53]",
        "original": "Beltagy et al.., 2019"
      }
    ]
  },
  {
    "id": "doc_071_sent_02",
    "with_marker": "Therefore, fine-\ntuning the general language model on the sample of acknowledgment texts could improve \nthe performance of the NER model for acknowledgment texts.",
    "citations": []
  },
  {
    "id": "doc_071_sent_03",
    "with_marker": "We are planning to fine-tune \nBERT and Flair Embeddings (contextual string embeddings) on a sample of approx.",
    "citations": []
  },
  {
    "id": "doc_071_sent_04",
    "with_marker": "5 mil-\nlion acknowledgment texts from WoS and evaluate the performance of the NER models.",
    "citations": []
  },
  {
    "id": "doc_071_sent_05",
    "with_marker": "The results of Experiment 2 generally did not show an improvement in accuracy.",
    "citations": []
  }
]
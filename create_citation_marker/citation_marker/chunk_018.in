[
  {
    "id": "doc_018_sent_01",
    "text": "This approach generates dif-\nferent embeddings for the same word depending on its context and showed good results \non downstream tasks such as NER."
  },
  {
    "id": "doc_018_sent_02",
    "text": "Devlin et al."
  },
  {
    "id": "doc_018_sent_03",
    "text": "(2018) presented BERT (Bidirectional \nEncoder Representations Transformers), a transformer-based language representa-\ntion model that models the representation of contextualized word embeddings."
  },
  {
    "id": "doc_018_sent_04",
    "text": "BERT \nshowed superior results on downstream tasks using different benchmarking datasets."
  },
  {
    "id": "doc_018_sent_05",
    "text": "Later, Liu et al."
  }
]
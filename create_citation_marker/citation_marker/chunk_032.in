[
  {
    "id": "doc_032_sent_01",
    "text": "Flair has three default training algorithms for \nNER which were used for the first experiment in the present research: a) NER Model with \nFlair Embeddings (later on Flair Embeddings) (Akbik et al., 2018), b) NER Model with \nTransformers (later on Transformers) (Schweter & Akbik, 2020), and c) Zero-shot NER \nwith TARS (later on TARS) (Halder et al., 2020) 8."
  },
  {
    "id": "doc_032_sent_02",
    "text": "The Flair Embeddings model uses stacked embeddings, i.e., a combination of contex-\ntual string embeddings (Akbik et al., 2018) with a static embeddings model."
  },
  {
    "id": "doc_032_sent_03",
    "text": "This approach \nwill generate different embeddings for the same word depending on its context."
  },
  {
    "id": "doc_032_sent_04",
    "text": "Stacked \nembedding is an important Flair feature, as a combination of different embeddings might \nbring better results than their separate uses (Akbik et al., 2019)."
  },
  {
    "id": "doc_032_sent_05",
    "text": "The Transformers model or FLERT-extension (document-level features for NER) is a \nset of settings to perform a NER on the document level using fine-tuning and feature-based \nFig."
  }
]
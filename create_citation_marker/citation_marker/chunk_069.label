[
  {
    "id": "doc_069_sent_01",
    "with_marker": "RoBERTa was pre-trained \non the combination of five datasets containing news articles, blog entries, books, and Wikipe-\ndia articles.",
    "citations": []
  },
  {
    "id": "doc_069_sent_02",
    "with_marker": "TARS was mainly pre-trained on datasets for text classification.",
    "citations": []
  },
  {
    "id": "doc_069_sent_03",
    "with_marker": "Thus, the models \nused were not trained on domain-specific data.",
    "citations": []
  },
  {
    "id": "doc_069_sent_04",
    "with_marker": "This can also explain the pure Transformers \nand TARS performance.",
    "citations": []
  },
  {
    "id": "doc_069_sent_05",
    "with_marker": "The higher accuracy for the individuals category in the training with \nTARS can be explained by the fact, that the word ’person’ is semantically more straightforward \nthan other categories.",
    "citations": []
  }
]
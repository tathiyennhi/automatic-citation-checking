[
  {
    "id": "doc_047_sent_01",
    "text": "Thus, in our case, stacked embeddings comprise GloVe embeddings, \nforward contextual string embeddings, and backward contextual string embeddings."
  },
  {
    "id": "doc_047_sent_02",
    "text": "The \nmodel was trained with the recommended parameters: the size of mini-batches was set to \n32 and the maximum number of epochs was set to 150."
  },
  {
    "id": "doc_047_sent_03",
    "text": "For Transformers, training was initiated with the RoBERTa model (Liu etÂ al., 2019)."
  },
  {
    "id": "doc_047_sent_04",
    "text": "For \nthe present paper, a fine-tuning approach was used."
  },
  {
    "id": "doc_047_sent_05",
    "text": "The fine-tuning procedure consisted of \nadding a linear layer to a transformer and retraining the entire network with a small learn-\ning rate."
  }
]
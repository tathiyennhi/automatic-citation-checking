[
  {
    "id": "doc_018_sent_01",
    "with_marker": "This approach generates dif-\nferent embeddings for the same word depending on its context and showed good results \non downstream tasks such as NER.",
    "citations": []
  },
  {
    "id": "doc_018_sent_02",
    "with_marker": "Devlin et al.",
    "citations": []
  },
  {
    "id": "doc_018_sent_03",
    "with_marker": "(2018) presented BERT (Bidirectional \nEncoder Representations Transformers), a transformer-based language representa-\ntion model that models the representation of contextualized word embeddings.",
    "citations": []
  },
  {
    "id": "doc_018_sent_04",
    "with_marker": "BERT \nshowed superior results on downstream tasks using different benchmarking datasets.",
    "citations": []
  },
  {
    "id": "doc_018_sent_05",
    "with_marker": "Later, Liu et al.",
    "citations": []
  }
]
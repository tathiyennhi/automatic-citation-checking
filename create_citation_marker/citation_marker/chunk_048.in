[
  {
    "id": "doc_048_sent_01",
    "text": "We used a standard approach, where only a linear classifier layer was added on the \ntop of the transformer, as adding the additional CRF decoder between the transformer and \nlinear classifier did not increase accuracy compared with this standard approach (Schweter \n& Akbik, 2020)."
  },
  {
    "id": "doc_048_sent_02",
    "text": "The chosen transformer model uses subword tokenization."
  },
  {
    "id": "doc_048_sent_03",
    "text": "We used the \nmean of embeddings of all subtokens and concatenation of all transformer layers to pro-\nduce embeddings."
  },
  {
    "id": "doc_048_sent_04",
    "text": "The context around the sentence was considered."
  },
  {
    "id": "doc_048_sent_05",
    "text": "The training was initi-\nated with a small learning rate using the Adam Optimisation Algorithm (Kingma & Ba, \n2014)."
  }
]
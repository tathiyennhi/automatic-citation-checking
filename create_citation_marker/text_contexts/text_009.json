{
  "text": "This approach will generate different embeddings for the same word depending on its context. Stacked embedding is an important Flair feature, as a combination of different embeddings might bring better results than their separate uses [CITATION_1]. The Transformers model or FLERT-extension (document-level features for NER) is a set of settings to perform a NER on the document level using fine-tuning and feature-based 8 New transformer models as SciBERT or SsciBERT were not evaluated in this study, as the objective of the study is to evaluate the performance of the Flair default models. 1 3 Scientometrics Table 2 Number of sentences/texts in the training corpora Corpus No. Training set (train) Test set (test) Validation set (dev) Total 1 29/27 10/10 10/10 49/47 2 339/282 165/150 150/136 654/441 3 784/657 165/150 150/136 1099/816 4 1148/885 165/150 150/136 1463/1044 Table 3 Number of sentences/texts from each scientific domain in the training corpora Corpus No. Oceanography Economics Social Sciences Computer Science 1 13/13 3/3 20/20 16/14 2 127/75 92/58 351/234 173/129 3 175/112 128/89 590/434 333/269 LSTM-CRF with the multilingual XML-RoBERTa transformer model [CITATION_2].",
  "citation_candidates": [],
  "bib_entries": {}
}
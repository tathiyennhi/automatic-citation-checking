The TARS (task-aware representation of sentences) is a transformer-based model, which allows performing training without any training data (zero-shot learning) or with a small dataset (few-short learning) (Halder et al., 2020). The TARS approach differs from the traditional transfer learning approach in the way that the TARS model also considers semantic information captured in the class labels themselves. For example, for analyzing acknowledgments, class labels like funding organization or university already carry semantic information. Training data The Web of Science (WoS) database was used to harvest the training data (funding acknowledgments).9 From 2008 on, WoS started indexing information about funders and grants. WoS uses information from different funding reporting systems such as Researchfish,10 Medline11 and others. As WoS contains millions of metadata records (Singh et al., 2021), the data chosen for the present study was restricted by year and scientific domain (for the corpora Nos. 1, 2, and 3) or additionally by the affiliation country (for corpus No.4). To construct corpora Nos. 1-3 records from four different scientific domains published from 2014 to 2019 were considered: two domains from the social sciences (sociology and economics) and oceanography and computer science. Different scientific domains were 9 The present research was conducted in scopes of two projects: MinAck (https:// kalaw inka. github. io/ minack/) and SEASON (https:// github. com/ kalaw inka/ season). Corpora Nos.1, 2, and 3 were created for the MinAck project and serve the purpose of a general evaluation of the impact of the size of the training corpus on the model performance. Corpus No.4 was designed specifically for the SEASON project in the hope of improving the recognition of Indian funding information. The project SEASON aims to get insight into German-Indian scientific collaboration.
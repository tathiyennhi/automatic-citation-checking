Experiment 1 In the first experiment, we tested the TARS model zero-shot and few-shot scenarios (with corpus No. 1), as well as the performance of two default FLAIR models (Flair Embeddings and Transformers) with corpus No.2. Additionally, the performance of Flair Embeddings and Transformers models was tested with the corpus No.1 The training was conducted with the recommended parameters for all algorithms, as Flair developers specifically ran various tests to find the best hyperparameters for the default models. For the few-shot TARS, the training was conducted with the small dataset (corpus No.1), and for Transformers and Flair Embeddings with a larger dataset (corpus No.2). 13 https:// github. com/ flair NLP/ flair 1 3 Scientometrics Fig. 5 The training results with the training corpus No.2. A Comprises diagrams with the F1-scores of the training with three algorithms for each label class. B depicts the total accuracy of training algorithms The Flair Embeddings model was initiated as a combination of static and contextual string embeddings. We applied GloVe [CITATION_1] as a static word-level embedding model. Thus, in our case, stacked embeddings comprise GloVe embeddings, forward contextual string embeddings, and backward contextual string embeddings. The model was trained with the recommended parameters: the size of mini-batches was set to 32 and the maximum number of epochs was set to 150. For Transformers, training was initiated with the RoBERTa model [CITATION_2]. For the present paper, a fine-tuning approach was used. The fine-tuning procedure consisted of adding a linear layer to a transformer and retraining the entire network with a small learning rate.
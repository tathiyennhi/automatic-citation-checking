Other works on acknowledgment analysis were focused on the classification of acknowledgment texts. The Flair NLP framework Flair is an open-sourced NLP framework built on PyTorch [CITATION_1], which is an open-source machine learning library. “The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings” [CITATION_2]. Flair has three default training algorithms for NER which were used for the first experiment in the present research: a) NER Model with Flair Embeddings (later on Flair Embeddings) [CITATION_3], b) NER Model with Transformers (later on Transformers) [CITATION_4], and c) Zero-shot NER with TARS (later on TARS) [CITATION_5] 8. The Flair Embeddings model uses stacked embeddings, i.e., a combination of contextual string embeddings [CITATION_6] with a static embeddings model. This approach will generate different embeddings for the same word depending on its context. Stacked embedding is an important Flair feature, as a combination of different embeddings might bring better results than their separate uses [CITATION_7]. The Transformers model or FLERT-extension (document-level features for NER) is a set of settings to perform a NER on the document level using fine-tuning and feature-based 8 New transformer models as SciBERT or SsciBERT were not evaluated in this study, as the objective of the study is to evaluate the performance of the Flair default models. 1 3 Scientometrics Table 2 Number of sentences/texts in the training corpora Corpus No. Training set (train) Test set (test) Validation set (dev) Total 1 29/27 10/10 10/10 49/47 2 339/282 165/150 150/136 654/441 3 784/657 165/150 150/136 1099/816 4 1148/885 165/150 150/136 1463/1044 Table 3 Number of sentences/texts from each scientific domain in the training corpora Corpus No. Oceanography Economics Social Sciences Computer Science 1 13/13 3/3 20/20 16/14 2 127/75 92/58 351/234 173/129 3 175/112 128/89 590/434 333/269 LSTM-CRF with the multilingual XML-RoBERTa transformer model [CITATION_8]. The TARS (task-aware representation of sentences) is a transformer-based model, which allows performing training without any training data (zero-shot learning) or with a small dataset (few-short learning) [CITATION_9]. The TARS approach differs from the traditional transfer learning approach in the way that the TARS model also considers semantic information captured in the class labels themselves.